0<CODESPLIT>https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/native/cpu/arm.py#L197-L203<CODESPLIT>EVMWorld.start_transaction<CODESPLIT>Manually compute the carry bit produced by expanding an immediate operand ( see ARMExpandImm_C )<CODESPLIT>def start_transaction ( self , sort , address , price = None , data = None , caller = None , value = 0 , gas = 2300 ) : assert self . _pending_transaction is None , "Already started tx" self . _pending_transaction = PendingTransaction ( sort , address , price , data , caller , value , gas )

0<CODESPLIT>https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/core.py#L745-L758<CODESPLIT>Log.write_log<CODESPLIT>Load all the files in a given directory selecting only files with the given extension if specified . The given kwargs are passed through to the normal constructor .<CODESPLIT>def write_log ( log_path , data , allow_append = True ) : append = os . path . isfile ( log_path ) islist = isinstance ( data , list ) if append and not allow_append : raise Exception ( 'Appending has been disabled' ' and file %s exists' % log_path ) if not ( islist or isinstance ( data , Args ) ) : raise Exception ( 'Can only write Args objects or dictionary' ' lists to log file.' ) specs = data if islist else data . specs if not all ( isinstance ( el , dict ) for el in specs ) : raise Exception ( 'List elements must be dictionaries.' ) log_file = open ( log_path , 'r+' ) if append else open ( log_path , 'w' ) start = int ( log_file . readlines ( ) [ - 1 ] . split ( ) [ 0 ] ) + 1 if append else 0 ascending_indices = range ( start , start + len ( data ) ) log_str = '\n' . join ( [ '%d %s' % ( tid , json . dumps ( el ) ) for ( tid , el ) in zip ( ascending_indices , specs ) ] ) log_file . write ( "\n" + log_str if append else log_str ) log_file . close ( )

1<CODESPLIT>https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/scripts/profiling/tm_profile.py#L31-L54<CODESPLIT>profileTM<CODESPLIT>profiling performance of TemporalMemory ( TM ) using the python cProfile module and ordered by cumulative time see how to run on command - line above .<CODESPLIT>def profileTM ( tmClass , tmDim , nRuns ) : # create TM instance to measure tm = tmClass ( numberOfCols = tmDim ) # generate input data data = numpy . random . randint ( 0 , 2 , [ tmDim , nRuns ] ) . astype ( 'float32' ) for i in xrange ( nRuns ) : # new data every time, this is the worst case performance # real performance would be better, as the input data would not be completely random d = data [ : , i ] # the actual function to profile! tm . compute ( d , True )

1<CODESPLIT>https://github.com/ARMmbed/yotta/blob/56bc1e56c602fa20307b23fe27518e9cd6c11af1/yotta/lib/registry_access.py#L139-L159<CODESPLIT>_handleAuth<CODESPLIT>Decorator to re - try API calls after asking the user for authentication .<CODESPLIT>def _handleAuth ( fn ) : @ functools . wraps ( fn ) def wrapped ( * args , * * kwargs ) : # auth, , authenticate users, internal from yotta . lib import auth # if yotta is being run noninteractively, then we never retry, but we # do call auth.authorizeUser, so that a login URL can be displayed: interactive = globalconf . get ( 'interactive' ) try : return fn ( * args , * * kwargs ) except requests . exceptions . HTTPError as e : if e . response . status_code == requests . codes . unauthorized : #pylint: disable=no-member logger . debug ( '%s unauthorised' , fn ) # any provider is sufficient for registry auth auth . authorizeUser ( provider = None , interactive = interactive ) if interactive : logger . debug ( 'retrying after authentication...' ) return fn ( * args , * * kwargs ) raise return wrapped

0<CODESPLIT>https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/graph/__init__.py#L298-L324<CODESPLIT>graph.remove_node<CODESPLIT>Add weighted ( 0 . 0 - 1 . 0 ) edge between nodes creating them if necessary . The weight represents the importance of the connection ( not the cost ) .<CODESPLIT>def remove_node ( self , id ) : if self . has_key ( id ) : n = self [ id ] self . nodes . remove ( n ) del self [ id ] # Remove all edges involving id and all links to it. for e in list ( self . edges ) : if n in ( e . node1 , e . node2 ) : if n in e . node1 . links : e . node1 . links . remove ( n ) if n in e . node2 . links : e . node2 . links . remove ( n ) self . edges . remove ( e )

0<CODESPLIT>https://github.com/Devoxin/Lavalink.py/blob/63f55c3d726d24c4cfd3674d3cd6aab6f5be110d/lavalink/PlayerManager.py#L132-L135<CODESPLIT>DefaultPlayer.play_now<CODESPLIT>Play the queue from a specific point . Disregards tracks before the index .<CODESPLIT>async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )

1<CODESPLIT>https://github.com/opencobra/cobrapy/blob/9d1987cdb3a395cf4125a3439c3b002ff2be2009/cobra/flux_analysis/variability.py#L268-L301<CODESPLIT>find_essential_genes<CODESPLIT>Return a set of essential genes .<CODESPLIT>def find_essential_genes ( model , threshold = None , processes = None ) : if threshold is None : threshold = model . slim_optimize ( error_value = None ) * 1E-02 deletions = single_gene_deletion ( model , method = 'fba' , processes = processes ) essential = deletions . loc [ deletions [ 'growth' ] . isna ( ) | ( deletions [ 'growth' ] < threshold ) , : ] . index return { model . genes . get_by_id ( g ) for ids in essential for g in ids }

1<CODESPLIT>https://github.com/calebsmith/django-template-debug/blob/f3d52638da571164d63e5c8331d409b0743c628f/template_debug/templatetags/debug_tags.py#L18-L23<CODESPLIT>require_template_debug<CODESPLIT>Decorated function is a no - op if TEMPLATE_DEBUG is False<CODESPLIT>def require_template_debug ( f ) : def _ ( * args , * * kwargs ) : TEMPLATE_DEBUG = getattr ( settings , 'TEMPLATE_DEBUG' , False ) return f ( * args , * * kwargs ) if TEMPLATE_DEBUG else '' return _

0<CODESPLIT>https://github.com/apache/incubator-heron/blob/ad10325a0febe89ad337e561ebcbe37ec5d9a5ac/heron/tools/admin/src/python/standalone.py#L349-L375<CODESPLIT>template_heron_tools_hcl<CODESPLIT>get cluster info for standalone cluster<CODESPLIT>def template_heron_tools_hcl ( cl_args , masters , zookeepers ) : heron_tools_hcl_template = "%s/standalone/templates/heron_tools.template.hcl" % cl_args [ "config_path" ] heron_tools_hcl_actual = "%s/standalone/resources/heron_tools.hcl" % cl_args [ "config_path" ] single_master = masters [ 0 ] template_file ( heron_tools_hcl_template , heron_tools_hcl_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<heron_tracker_executable>" : '"%s/heron-tracker"' % config . get_heron_bin_dir ( ) , "<heron_tools_hostname>" : '"%s"' % get_hostname ( single_master , cl_args ) , "<heron_ui_executable>" : '"%s/heron-ui"' % config . get_heron_bin_dir ( ) } )

0<CODESPLIT>https://github.com/CitrineInformatics/python-citrination-client/blob/409984fc65ce101a620f069263f155303492465c/citrination_client/models/columns/base.py#L33-L49<CODESPLIT>DataViewBuilder.add_descriptor<CODESPLIT>Converts the column to a dictionary representation accepted by the Citrination server .<CODESPLIT>def add_descriptor ( self , descriptor , role = 'ignore' , group_by_key = False ) : descriptor . validate ( ) if descriptor . key in self . configuration [ "roles" ] : raise ValueError ( "Cannot add a descriptor with the same name twice" ) self . configuration [ 'descriptors' ] . append ( descriptor . as_dict ( ) ) self . configuration [ "roles" ] [ descriptor . key ] = role if group_by_key : self . configuration [ "group_by" ] . append ( descriptor . key )

0<CODESPLIT>https://github.com/google/grumpy/blob/3ec87959189cfcdeae82eb68a47648ac25ceb10b/third_party/pypy/_struct.py#L144-L159<CODESPLIT>float_unpack<CODESPLIT>Python 3 style round : round a float x to the nearest int but unlike the builtin Python 2 . x round function :<CODESPLIT>def float_unpack ( Q , size , le ) : if size == 8 : MIN_EXP = - 1021 # = sys.float_info.min_exp MAX_EXP = 1024 # = sys.float_info.max_exp MANT_DIG = 53 # = sys.float_info.mant_dig BITS = 64 elif size == 4 : MIN_EXP = - 125 # C's FLT_MIN_EXP MAX_EXP = 128 # FLT_MAX_EXP MANT_DIG = 24 # FLT_MANT_DIG BITS = 32 else : raise ValueError ( "invalid size value" ) if Q >> BITS : raise ValueError ( "input out of range" ) # extract pieces sign = Q >> BITS - 1 exp = ( Q & ( ( 1 << BITS - 1 ) - ( 1 << MANT_DIG - 1 ) ) ) >> MANT_DIG - 1 mant = Q & ( ( 1 << MANT_DIG - 1 ) - 1 ) if exp == MAX_EXP - MIN_EXP + 2 : # nan or infinity result = float ( 'nan' ) if mant else float ( 'inf' ) elif exp == 0 : # subnormal or zero result = math . ldexp ( float ( mant ) , MIN_EXP - MANT_DIG ) else : # normal mant += 1 << MANT_DIG - 1 result = math . ldexp ( float ( mant ) , exp + MIN_EXP - MANT_DIG - 1 ) return - result if sign else result

1<CODESPLIT>https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/convert/SVGBuilder.py#L121-L127<CODESPLIT>SVGBuilder.place_svg_use<CODESPLIT>Same as : meth : place_svg_use_coords .<CODESPLIT>def place_svg_use ( self , symbol_id , layer_id , group = None ) : self . place_svg_use_coords ( 0 , 0 , symbol_id , layer_id , group )

1<CODESPLIT>https://github.com/boakley/robotframework-lint/blob/3e3578f4e39af9af9961aa0a715f146b74474091/rflint/parser/tables.py#L96-L134<CODESPLIT>AbstractContainerTable.append<CODESPLIT>The idea is we recognize when we have a new testcase by checking the first cell . If it s not empty and not a comment we have a new test case .<CODESPLIT>def append ( self , row ) : if len ( row ) == 0 : # blank line. Should we throw it away, or append a BlankLine object? return if ( row [ 0 ] != "" and ( not row [ 0 ] . lstrip ( ) . startswith ( "#" ) ) ) : # we have a new child table self . _children . append ( self . _childClass ( self . parent , row . linenumber , row [ 0 ] ) ) if len ( row . cells ) > 1 : # It appears the first row -- which contains the test case or # keyword name -- also has the first logical row of cells. # We'll create a Row, but we'll make the first cell empty instead # of leaving the name in it, since other code always assumes the # first cell is empty.  # # To be honest, I'm not sure this is the Right Thing To Do, but  # I'm too lazy to audit the code to see if it matters if we keep  # the first cell intact. Sorry if this ends up causing you grief # some day... row [ 0 ] = "" self . _children [ - 1 ] . append ( row . linenumber , row . raw_text , row . cells ) elif len ( self . _children ) == 0 : # something before the first test case # For now, append it to self.comments; eventually we should flag # an error if it's NOT a comment self . comments . append ( row ) else : # another row for the testcase if len ( row . cells ) > 0 : self . _children [ - 1 ] . append ( row . linenumber , row . raw_text , row . cells )

1<CODESPLIT>https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/phonetic/_norphone.py#L73-L155<CODESPLIT>Norphone.encode<CODESPLIT>Return the Norphone code .<CODESPLIT>def encode ( self , word ) : word = word . upper ( ) code = '' skip = 0 if word [ 0 : 2 ] == 'AA' : code = 'Å' skip = 2 elif word [ 0 : 2 ] == 'GI' : code = 'J' skip = 2 elif word [ 0 : 3 ] == 'SKY' : code = 'X' skip = 3 elif word [ 0 : 2 ] == 'EI' : code = 'Æ' skip = 2 elif word [ 0 : 2 ] == 'KY' : code = 'X' skip = 2 elif word [ : 1 ] == 'C' : code = 'K' skip = 1 elif word [ : 1 ] == 'Ä':  code = 'Æ' skip = 1 elif word [ : 1 ] == 'Ö':  code = 'Ø' skip = 1 if word [ - 2 : ] == 'DT' : word = word [ : - 2 ] + 'T' # Though the rules indicate this rule applies in all positions, the # reference implementation indicates it applies only in final position. elif word [ - 2 : - 1 ] in self . _uc_v_set and word [ - 1 : ] == 'D' : word = word [ : - 2 ] for pos , char in enumerate ( word ) : if skip : skip -= 1 else : for length in sorted ( self . _replacements , reverse = True ) : if word [ pos : pos + length ] in self . _replacements [ length ] : code += self . _replacements [ length ] [ word [ pos : pos + length ] ] skip = length - 1 break else : if not pos or char not in self . _uc_v_set : code += char code = self . _delete_consecutive_repeats ( code ) return code

1<CODESPLIT>https://github.com/cgrok/clashroyale/blob/2618f4da22a84ad3e36d2446e23436d87c423163/clashroyale/official_api/client.py#L402-L414<CODESPLIT>Client.get_tournament<CODESPLIT>Get a tournament information<CODESPLIT>def get_tournament ( self , tag : crtag , timeout = 0 ) : url = self . api . TOURNAMENT + '/' + tag return self . _get_model ( url , PartialTournament , timeout = timeout )

0<CODESPLIT>https://github.com/swharden/SWHLab/blob/a86c3c65323cec809a4bd4f81919644927094bf5/doc/oldcode/swhlab/core/common.py#L831-L835<CODESPLIT>version_upload<CODESPLIT>upload everything from localFolder into the current FTP folder .<CODESPLIT>def version_upload ( fname , username = "nibjb" ) : print ( "popping up pasword window..." ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not password : return print ( "username:" , username ) print ( "password:" , "*" * ( len ( password ) ) ) print ( "connecting..." ) ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) print ( "successful login!" ) ftp . cwd ( "/software/swhlab/versions" ) #IMMEDIATELY GO HERE!!! print ( "uploading" , os . path . basename ( fname ) ) ftp . storbinary ( "STOR " + os . path . basename ( fname ) , open ( fname , "rb" ) , 1024 ) #for binary files print ( "disconnecting..." ) ftp . quit ( )

1<CODESPLIT>https://github.com/pbrisk/businessdate/blob/79a0c5a4e557cbacca82a430403b18413404a9bc/businessdate/businessdate.py#L260-L267<CODESPLIT>BusinessDate.to_date<CODESPLIT>construct datetime . date instance represented calendar date of BusinessDate instance<CODESPLIT>def to_date ( self ) : y , m , d = self . to_ymd ( ) return date ( y , m , d )

0<CODESPLIT>https://github.com/cokelaer/spectrum/blob/bad6c32e3f10e185098748f67bb421b378b06afe/src/spectrum/levinson.py#L140-L239<CODESPLIT>levdown<CODESPLIT>computes the autocorrelation coefficients R based on the prediction polynomial A and the final prediction error Efinal using the stepdown algorithm .<CODESPLIT>def levdown ( anxt , enxt = None ) : #% Some preliminaries first #if nargout>=2 & nargin<2 #    raise ValueError('Insufficient number of input arguments'); if anxt [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) anxt = anxt [ 1 : ] #  Drop the leading 1, it is not needed #  in the step down # Extract the k+1'th reflection coefficient knxt = anxt [ - 1 ] if knxt == 1.0 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) # A Matrix formulation from Stoica is used to avoid looping acur = ( anxt [ 0 : - 1 ] - knxt * numpy . conj ( anxt [ - 2 : : - 1 ] ) ) / ( 1. - abs ( knxt ) ** 2 ) ecur = None if enxt is not None : ecur = enxt / ( 1. - numpy . dot ( knxt . conj ( ) . transpose ( ) , knxt ) ) acur = numpy . insert ( acur , 0 , 1 ) return acur , ecur

0<CODESPLIT>https://github.com/cokelaer/spectrum/blob/bad6c32e3f10e185098748f67bb421b378b06afe/src/spectrum/toeplitz.py#L84-L134<CODESPLIT>get_short_module_name<CODESPLIT>solve Tx = Z by a variation of Levinson algorithm where T is a complex hermitian toeplitz matrix<CODESPLIT>def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : # get the last working module name short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name

1<CODESPLIT>https://github.com/tonyfischetti/sake/blob/b7ad20fe8e7137db99a20ac06b8da26492601b00/sakelib/build.py#L189-L247<CODESPLIT>needs_to_run<CODESPLIT>Determines if a target needs to run . This can happen in two ways : ( a ) If a dependency of the target has changed ( b ) If an output of the target is missing<CODESPLIT>def needs_to_run ( G , target , in_mem_shas , from_store , settings ) : force = settings [ "force" ] sprint = settings [ "sprint" ] if ( force ) : sprint ( "Target rebuild is being forced so {} needs to run" . format ( target ) , level = "verbose" ) return True node_dict = get_the_node_dict ( G , target ) if 'output' in node_dict : for output in acts . get_all_outputs ( node_dict ) : if not os . path . isfile ( output ) : outstr = "Output file '{}' is missing so it needs to run" sprint ( outstr . format ( output ) , level = "verbose" ) return True if 'dependencies' not in node_dict : # if it has no dependencies, it always needs to run sprint ( "Target {} has no dependencies and needs to run" . format ( target ) , level = "verbose" ) return True for dep in node_dict [ 'dependencies' ] : # because the shas are updated after all targets build, # its possible that the dependency's sha doesn't exist # in the current "in_mem" dictionary. If this is the case, # then the target needs to run if ( 'files' in in_mem_shas and dep not in in_mem_shas [ 'files' ] or 'files' not in in_mem_shas ) : outstr = "Dep '{}' doesn't exist in memory so it needs to run" sprint ( outstr . format ( dep ) , level = "verbose" ) return True now_sha = in_mem_shas [ 'files' ] [ dep ] [ 'sha' ] if ( 'files' in from_store and dep not in from_store [ 'files' ] or 'files' not in from_store ) : outst = "Dep '{}' doesn't exist in shastore so it needs to run" sprint ( outst . format ( dep ) , level = "verbose" ) return True old_sha = from_store [ 'files' ] [ dep ] [ 'sha' ] if now_sha != old_sha : outstr = "There's a mismatch for dep {} so it needs to run" sprint ( outstr . format ( dep ) , level = "verbose" ) return True sprint ( "Target '{}' doesn't need to run" . format ( target ) , level = "verbose" ) return False

0<CODESPLIT>https://github.com/openstack/quark/blob/1112e6a66917d3e98e44cb7b33b107fd5a74bb2e/quark/plugin_modules/subnets.py#L426-L445<CODESPLIT>delete_subnet<CODESPLIT>Return the number of subnets .<CODESPLIT>def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : # A tenant can't delete subnets on provider network raise n_exc . NotAuthorized ( subnet_id = id ) else : # Raise a NotFound here because the foreign tenant # does not have to know about other tenant's subnet # existence. raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )

0<CODESPLIT>https://github.com/boakley/robotframework-lint/blob/3e3578f4e39af9af9961aa0a715f146b74474091/rflint/parser/parser.py#L227-L232<CODESPLIT>RobotFile.type<CODESPLIT>Generator which returns all keywords in the suite<CODESPLIT>def type ( self ) : robot_tables = [ table for table in self . tables if not isinstance ( table , UnknownTable ) ] if len ( robot_tables ) == 0 : return None for table in self . tables : if isinstance ( table , TestcaseTable ) : return "suite" return "resource"

0<CODESPLIT>https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/web/page.py#L82-L92<CODESPLIT>Page._keywords<CODESPLIT>Returns the meta description in the page .<CODESPLIT>def _keywords ( self ) : meta = self . find ( "meta" , { "name" : "keywords" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : keywords = [ k . strip ( ) for k in meta [ "content" ] . split ( "," ) ] else : keywords = [ ] return keywords

0<CODESPLIT>https://github.com/gmarull/asyncqt/blob/292e42c0bf799e5aeee099ee2cec27a810b01870/asyncqt/__init__.py#L627-L635<CODESPLIT>asyncSlot<CODESPLIT>Allow to run async code before application is closed .<CODESPLIT>def asyncSlot ( * args ) : def outer_decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , * * kwargs ) : asyncio . ensure_future ( fn ( * args , * * kwargs ) ) return wrapper return outer_decorator

0<CODESPLIT>https://github.com/marl/jams/blob/b16778399b9528efbd71434842a079f7691a7a66/jams/eval.py#L455-L495<CODESPLIT>pattern_to_mireval<CODESPLIT>r Pattern detection evaluation<CODESPLIT>def pattern_to_mireval ( ann ) : # It's easier to work with dictionaries, since we can't assume # sequential pattern or occurrence identifiers patterns = defaultdict ( lambda : defaultdict ( list ) ) # Iterate over the data in interval-value format for time , observation in zip ( * ann . to_event_values ( ) ) : pattern_id = observation [ 'pattern_id' ] occurrence_id = observation [ 'occurrence_id' ] obs = ( time , observation [ 'midi_pitch' ] ) # Push this note observation into the correct pattern/occurrence patterns [ pattern_id ] [ occurrence_id ] . append ( obs ) # Convert to list-list-tuple format for mir_eval return [ list ( _ . values ( ) ) for _ in six . itervalues ( patterns ) ]

1<CODESPLIT>https://github.com/lc-guy/limf/blob/ad380feb70ef8e579a91ca09c807efec9e8af565/limf/cli.py#L10-L60<CODESPLIT>main<CODESPLIT>Creates arguments and parses user input<CODESPLIT>def main ( ) : parser = argparse . ArgumentParser ( description = _ ( 'Uploads selected file to working pomf.se clone' ) ) parser . add_argument ( 'files' , metavar = 'file' , nargs = '*' , type = str , help = _ ( 'Files to upload' ) ) parser . add_argument ( '-c' , metavar = 'host_number' , type = int , dest = 'host' , default = None , help = _ ( 'The number (0-n) of the selected host (default is random)' ) ) parser . add_argument ( '-l' , dest = 'only_link' , action = 'store_const' , const = True , default = False , help = _ ( 'Changes output to just link to the file' ) ) parser . add_argument ( '-e' , dest = 'encrypt' , action = 'store_const' , const = True , default = False , help = _ ( 'Encrypts then uploads the files.' ) ) parser . add_argument ( '-d' , dest = 'decrypt' , action = 'store_const' , const = True , default = False , help = _ ( 'Decrypts files from links with encrypted files' ) ) parser . add_argument ( '-j' , dest = "local_list" , default = False , help = _ ( 'Path to a local list file' ) ) parser . add_argument ( '-s' , dest = "show_list" , action = 'store_const' , const = True , default = False , help = _ ( 'Show the host list (will not upload your files when called)' ) ) parser . add_argument ( '-m' , dest = 'limit_size' , action = 'store_const' , const = True , default = False , help = _ ( 'Do not upload file if it exceeds the certain host limit' ) ) parser . add_argument ( '-nc' , dest = 'no_cloudflare' , action = 'store_const' , const = True , default = False , help = _ ( 'Do not use hosts which use Cloudflare.' ) ) parser . add_argument ( '--log-file' , metavar = "LOGFILE" , dest = "logfile" , default = "~/limf.log" , help = _ ( "The location of log file" ) ) parser . add_argument ( '--log' , dest = 'log' , action = "store_const" , const = True , default = False , help = _ ( "Enables the logging feature, default logfile is ~/limf.log" ) ) args = parser . parse_args ( ) try : if args . local_list : clone_list = retrieve_local_host_list ( args . local_list ) else : clone_list = retrieve_online_host_list ( ) if len ( min ( clone_list , key = len ) ) < 5 and ( args . limit_size or args . no_cloudflare ) : print ( _ ( "For newer options, please update your host_list." ) ) exit ( ) if args . host and not ( 0 <= args . host < len ( clone_list ) ) : print ( generate_host_string ( clone_list ) ) exit ( ) parse_arguments ( args , clone_list ) except FileNotFoundError : print ( _ ( 'Plese enter valid file.' ) )

0<CODESPLIT>https://github.com/jic-dtool/dtool-create/blob/12172363d14eaedba2db4c452ef995b14f1b630d/dtool_create/dataset.py#L119-L189<CODESPLIT>_prompt_for_values<CODESPLIT>Create a proto dataset .<CODESPLIT>def _prompt_for_values ( d ) : for key , value in d . items ( ) : if isinstance ( value , CommentedMap ) : _prompt_for_values ( value ) elif isinstance ( value , list ) : for item in value : _prompt_for_values ( item ) else : typ = type ( value ) if isinstance ( value , ScalarFloat ) : # Deal with ruamel.yaml floats. typ = float new_value = click . prompt ( key , type = typ , default = value ) d [ key ] = new_value return d

0<CODESPLIT>https://github.com/edaniszewski/colorutils/blob/bdff54091cb5d62aa8628ce39bc09abd40fb8dd0/colorutils/colorutils.py#L230-L248<CODESPLIT>color_run<CODESPLIT>Given a seed color generate a specified number of random colors ( 1 color by default ) determined by a randomized offset from the seed .<CODESPLIT>def color_run ( start_color , end_color , step_count , inclusive = True , to_color = True ) : if isinstance ( start_color , Color ) : start_color = start_color . rgb if isinstance ( end_color , Color ) : end_color = end_color . rgb step = tuple ( ( end_color [ i ] - start_color [ i ] ) / step_count for i in range ( 3 ) ) add = lambda x , y : tuple ( sum ( z ) for z in zip ( x , y ) ) mult = lambda x , y : tuple ( y * z for z in x ) run = [ add ( start_color , mult ( step , i ) ) for i in range ( 1 , step_count ) ] if inclusive : run = [ start_color ] + run + [ end_color ] return run if not to_color else [ Color ( c ) for c in run ]

0<CODESPLIT>https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/setup.py#L41-L56<CODESPLIT>spanish_metaphone<CODESPLIT>Read fn and return the contents .<CODESPLIT>def spanish_metaphone ( word , max_length = 6 , modified = False ) : return SpanishMetaphone ( ) . encode ( word , max_length , modified )

0<CODESPLIT>https://github.com/PiotrDabkowski/Js2Py/blob/c0fa43f5679cf91ca8986c5747fcb07a433dc584/js2py/legecy_translators/constants.py#L28-L42<CODESPLIT>parse_num<CODESPLIT>returns True if regexp starts at n else returns False checks whether it is not a division<CODESPLIT>def parse_num ( source , start , charset ) : while start < len ( source ) and source [ start ] in charset : start += 1 return start

1<CODESPLIT>https://github.com/Becksteinlab/GromacsWrapper/blob/d4f9a8cb6f48292732cf7c7e4ef4a6d2ccbc51b9/gromacs/utilities.py#L821-L844<CODESPLIT>number_pdbs<CODESPLIT>Rename pdbs x1 . pdb ... x345 . pdb -- > x0001 . pdb ... x0345 . pdb<CODESPLIT>def number_pdbs ( * args , * * kwargs ) : format = kwargs . pop ( 'format' , "%(num)04d" ) name_format = "%(prefix)s" + format + ".%(suffix)s" for f in itertools . chain . from_iterable ( map ( glob . glob , args ) ) : m = NUMBERED_PDB . search ( f ) if m is None : continue num = int ( m . group ( 'NUMBER' ) ) prefix = m . group ( 'PREFIX' ) suffix = m . group ( 'SUFFIX' ) newname = name_format % vars ( ) logger . info ( "Renaming {f!r} --> {newname!r}" . format ( * * vars ( ) ) ) try : os . rename ( f , newname ) except OSError : logger . exception ( "renaming failed" )

1<CODESPLIT>https://github.com/minrk/escapism/blob/35f4c194ad6de2bc3339bb8b0e522dca989143ff/escapism.py#L30-L36<CODESPLIT>_escape_char<CODESPLIT>Escape a single character<CODESPLIT>def _escape_char ( c , escape_char = ESCAPE_CHAR ) : buf = [ ] for byte in c . encode ( 'utf8' ) : buf . append ( escape_char ) buf . append ( '%X' % _ord ( byte ) ) return '' . join ( buf )

1<CODESPLIT>https://github.com/pybel/pybel-tools/blob/3491adea0ac4ee60f57275ef72f9b73da6dbfe0c/src/pybel_tools/summary/provenance.py#L94-L111<CODESPLIT>count_citations<CODESPLIT>Counts the citations in a graph based on a given filter<CODESPLIT>def count_citations ( graph : BELGraph , * * annotations ) -> Counter : citations = defaultdict ( set ) annotation_dict_filter = build_edge_data_filter ( annotations ) for u , v , _ , d in filter_edges ( graph , annotation_dict_filter ) : if CITATION not in d : continue citations [ u , v ] . add ( ( d [ CITATION ] [ CITATION_TYPE ] , d [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return Counter ( itt . chain . from_iterable ( citations . values ( ) ) )

1<CODESPLIT>https://github.com/Netflix-Skunkworks/cloudaux/blob/c4b0870c3ac68b1c69e71d33cf78b6a8bdf437ea/cloudaux/gcp/gcpcache.py#L93-L98<CODESPLIT>GCPCache.get_access_details<CODESPLIT>Get access details in cache .<CODESPLIT>def get_access_details ( self , key = None ) : if key in self . _CACHE_STATS : return self . _CACHE_STATS [ 'access_stats' ] [ key ] else : return self . _CACHE_STATS [ 'access_stats' ]

1<CODESPLIT>https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/native/cpu/arm.py#L661-L671<CODESPLIT>Armv7Cpu.MOVT<CODESPLIT>MOVT writes imm16 to Rd [ 31 : 16 ] . The write does not affect Rd [ 15 : 0 ] .<CODESPLIT>def MOVT ( cpu , dest , src ) : assert src . type == 'immediate' imm = src . read ( ) low_halfword = dest . read ( ) & Mask ( 16 ) dest . write ( ( imm << 16 ) | low_halfword )

1<CODESPLIT>https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/platforms/linux.py#L2690-L2711<CODESPLIT>SLinux.sys_openat<CODESPLIT>A version of openat that includes a symbolic path and symbolic directory file descriptor<CODESPLIT>def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) # Constrain to a valid fd and one past the end of fds self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )

0<CODESPLIT>https://github.com/Infinidat/infi.gevent_utils/blob/7eb3c1601b8f2c9aaa3a83154ee7dfce8e5e5a5a/src/infi/gevent_utils/safe_greenlets.py#L53-L65<CODESPLIT>safe_joinall<CODESPLIT>wraps ( instancemethod ) returns a function not an instancemethod so its repr () is all messed up ; we want the original repr to show up in the logs therefore we do this trick<CODESPLIT>def safe_joinall ( greenlets , timeout = None , raise_error = False ) : greenlets = list ( greenlets ) try : gevent . joinall ( greenlets , timeout = timeout , raise_error = raise_error ) except gevent . GreenletExit : [ greenlet . kill ( ) for greenlet in greenlets if not greenlet . ready ( ) ] raise return greenlets

0<CODESPLIT>https://github.com/apache/incubator-heron/blob/ad10325a0febe89ad337e561ebcbe37ec5d9a5ac/heron/tools/ui/src/python/handlers/api/topology.py#L148-L167<CODESPLIT>TopologyExceptionsJsonHandler.get<CODESPLIT>: param cluster : : param environ : : param topology : : return :<CODESPLIT>def get ( self , cluster , environ , topology , component ) : start_time = time . time ( ) futures = yield access . get_component_exceptions ( cluster , environ , topology , component ) result_map = dict ( status = 'success' , executiontime = time . time ( ) - start_time , result = futures ) self . write ( json . dumps ( result_map ) )

1<CODESPLIT>https://github.com/changhiskhan/poseidon/blob/6d1cecbe02f1e510dd185fe23f88f7af35eb737f/poseidon/droplet.py#L403-L414<CODESPLIT>DropletActions.take_snapshot<CODESPLIT>Take a snapshot of this droplet ( must be powered off )<CODESPLIT>def take_snapshot ( self , name , wait = True ) : return self . _action ( 'snapshot' , name = name , wait = wait )

1<CODESPLIT>https://github.com/dade-ai/snipy/blob/408520867179f99b3158b57520e2619f3fecd69b/snipy/dictobj.py#L51-L57<CODESPLIT>DictObj.intersect<CODESPLIT>self와 other 키가 동일한 아이템의 dictobj : type other : dict : rtype : dictobj :<CODESPLIT>def intersect ( self , other ) : return DictObj ( { k : self [ k ] for k in self if k in other } )

0<CODESPLIT>https://github.com/hobson/aima/blob/3572b2fb92039b4a1abe384be8545560fbd3d470/aima/learning.py#L360-L387<CODESPLIT>NeuralNetLearner<CODESPLIT>[ Fig . 18 . 11 ]<CODESPLIT>def NeuralNetLearner ( dataset , sizes ) : activations = map ( lambda n : [ 0.0 for i in range ( n ) ] , sizes ) weights = [ ] def predict ( example ) : unimplemented ( ) return predict

0<CODESPLIT>https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/utils/schedule.py#L58-L64<CODESPLIT>Schedule.includes<CODESPLIT>Returns the next Period this event is in effect or None if the event has no remaining periods .<CODESPLIT>def includes ( self , query ) : query = self . to_timezone ( query ) return any ( self . intervals ( range_start = query , range_end = query ) )

0<CODESPLIT>https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/native/cpu/abstractcpu.py#L655-L675<CODESPLIT>Cpu.write_bytes<CODESPLIT>Reads int from memory<CODESPLIT>def write_bytes ( self , where , data , force = False ) : mp = self . memory . map_containing ( where ) # TODO (ehennenfent) - fast write can have some yet-unstudied unintended side effects. # At the very least, using it in non-concrete mode will break the symbolic strcmp/strlen models. The 1024 byte # minimum is intended to minimize the potential effects of this by ensuring that if there _are_ any other # issues, they'll only crop up when we're doing very large writes, which are fairly uncommon. can_write_raw = type ( mp ) is AnonMap and isinstance ( data , ( str , bytes ) ) and ( mp . end - mp . start + 1 ) >= len ( data ) >= 1024 and not issymbolic ( data ) and self . _concrete if can_write_raw : logger . debug ( "Using fast write" ) offset = mp . _get_offset ( where ) if isinstance ( data , str ) : data = bytes ( data . encode ( 'utf-8' ) ) mp . _data [ offset : offset + len ( data ) ] = data self . _publish ( 'did_write_memory' , where , data , 8 * len ( data ) ) else : for i in range ( len ( data ) ) : self . write_int ( where + i , Operators . ORD ( data [ i ] ) , 8 , force )

0<CODESPLIT>https://github.com/adafruit/Adafruit_Python_GPIO/blob/a92a23d6b5869663b2bc1ccf78bb11585076a9c4/Adafruit_GPIO/GPIO.py#L183-L188<CODESPLIT>BaseGPIO.setup_pins<CODESPLIT>Set the input or output mode for a specified pin . Mode should be either OUTPUT or INPUT .<CODESPLIT>def setup_pins ( self , pins ) : # General implementation that can be optimized by derived classes. for pin , value in iter ( pins . items ( ) ) : self . setup ( pin , value )

1<CODESPLIT>https://github.com/edx/edx-enterprise/blob/aea91379ab0a87cd3bc798961fce28b60ee49a80/enterprise/admin/forms.py#L149-L181<CODESPLIT>ManageLearnersForm.clean_email_or_username<CODESPLIT>Clean email form field<CODESPLIT>def clean_email_or_username ( self ) : email_or_username = self . cleaned_data [ self . Fields . EMAIL_OR_USERNAME ] . strip ( ) if not email_or_username : # The field is blank; we just return the existing blank value. return email_or_username email = email_or_username__to__email ( email_or_username ) bulk_entry = len ( split_usernames_and_emails ( email ) ) > 1 if bulk_entry : for email in split_usernames_and_emails ( email ) : validate_email_to_link ( email , None , ValidationMessages . INVALID_EMAIL_OR_USERNAME , ignore_existing = True ) email = email_or_username else : validate_email_to_link ( email , email_or_username , ValidationMessages . INVALID_EMAIL_OR_USERNAME , ignore_existing = True ) return email

1<CODESPLIT>https://github.com/apache/incubator-heron/blob/ad10325a0febe89ad337e561ebcbe37ec5d9a5ac/heron/instance/src/python/utils/metrics/metrics_helper.py#L42-L45<CODESPLIT>BaseMetricsHelper.register_metrics<CODESPLIT>Registers its metrics to a given metrics collector with a given interval<CODESPLIT>def register_metrics ( self , metrics_collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics_collector . register_metric ( field , metrics , interval )

0<CODESPLIT>https://github.com/spdx/tools-python/blob/301d72f6ae57c832c1da7f6402fa49b192de6810/spdx/parsers/tagvaluebuilders.py#L587-L602<CODESPLIT>PackageBuilder.set_pkg_down_location<CODESPLIT>Sets the package originator if not already set . entity - Organization Person or NoAssert . Raises CardinalityError if already has an originator . Raises OrderError if no package previously defined .<CODESPLIT>def set_pkg_down_location ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_down_location_set : self . package_down_location_set = True doc . package . download_location = location return True else : raise CardinalityError ( 'Package::DownloadLocation' )

1<CODESPLIT>https://github.com/westurner/pgs/blob/1cc2bf2c41479d8d3ba50480f003183f1675e518/pgs/app.py#L460-L542<CODESPLIT>git_static_file<CODESPLIT>This method is derived from bottle . static_file :<CODESPLIT>def git_static_file ( filename , mimetype = 'auto' , download = False , charset = 'UTF-8' ) : # root = os.path.abspath(root) + os.sep # filename = os.path.abspath(pathjoin(root, filename.strip('/\\'))) filename = filename . strip ( '/\\' ) headers = dict ( ) FS = request . app . config [ 'pgs.FS' ] # if not filename.startswith(root): #    return HTTPError(403, "Access denied.") if not FS . exists ( filename ) : return HTTPError ( 404 , "Not found." ) # if not os.access(filename, os.R_OK): # return HTTPError(403, "You do not have permission to access this file.") if mimetype == 'auto' : if download and download is not True : mimetype , encoding = mimetypes . guess_type ( download ) else : mimetype , encoding = mimetypes . guess_type ( filename ) if encoding : headers [ 'Content-Encoding' ] = encoding if mimetype : if mimetype [ : 5 ] == 'text/' and charset and 'charset' not in mimetype : mimetype += '; charset=%s' % charset headers [ 'Content-Type' ] = mimetype if download : download = os . path . basename ( filename if download else download ) headers [ 'Content-Disposition' ] = 'attachment; filename="%s"' % download # stats = os.stat(filename) info = FS . getinfo ( filename ) headers [ 'Content-Length' ] = clen = info [ 'size' ] lm = time . strftime ( "%a, %d %b %Y %H:%M:%S GMT" , time . gmtime ( info [ 'modified_time' ] ) ) headers [ 'Last-Modified' ] = lm ims = request . environ . get ( 'HTTP_IF_MODIFIED_SINCE' ) if ims : ims = parse_date ( ims . split ( ";" ) [ 0 ] . strip ( ) ) mtime = info [ 'modified_time' ] if mtime and ims is not None and ims >= int ( mtime ) : headers [ 'Date' ] = time . strftime ( "%a, %d %b %Y %H:%M:%S GMT" , time . gmtime ( ) ) return HTTPResponse ( status = 304 , * * headers ) body = '' if request . method == 'HEAD' else FS . get_fileobj ( filename ) clen # headers["Accept-Ranges"] = "bytes" # ranges = request.environ.get('HTTP_RANGE') # if 'HTTP_RANGE' in request.environ: #    ranges = list(parse_range_header(request.environ['HTTP_RANGE'], clen)) #     if not ranges: #         return HTTPError(416, "Requested Range Not Satisfiable") #    offset, end = ranges[0] #    headers["Content-Range"] = "bytes %d-%d/%d" % (offset, end - 1, clen) #    headers["Content-Length"] = str(end - offset) #    if body: body = _file_iter_range(body, offset, end - offset) #     return HTTPResponse(body, status=206, **headers) return HTTPResponse ( body , * * headers )

1<CODESPLIT>https://github.com/ChargePoint/pydnp3/blob/5bcd8240d1fc0aa1579e71f2efcab63b4c61c547/examples/master.py#L186-L208<CODESPLIT>SOEHandler.Process<CODESPLIT>Process measurement data .<CODESPLIT>def Process ( self , info , values ) : visitor_class_types = { opendnp3 . ICollectionIndexedBinary : VisitorIndexedBinary , opendnp3 . ICollectionIndexedDoubleBitBinary : VisitorIndexedDoubleBitBinary , opendnp3 . ICollectionIndexedCounter : VisitorIndexedCounter , opendnp3 . ICollectionIndexedFrozenCounter : VisitorIndexedFrozenCounter , opendnp3 . ICollectionIndexedAnalog : VisitorIndexedAnalog , opendnp3 . ICollectionIndexedBinaryOutputStatus : VisitorIndexedBinaryOutputStatus , opendnp3 . ICollectionIndexedAnalogOutputStatus : VisitorIndexedAnalogOutputStatus , opendnp3 . ICollectionIndexedTimeAndInterval : VisitorIndexedTimeAndInterval } visitor_class = visitor_class_types [ type ( values ) ] visitor = visitor_class ( ) values . Foreach ( visitor ) for index , value in visitor . index_and_value : log_string = 'SOEHandler.Process {0}\theaderIndex={1}\tdata_type={2}\tindex={3}\tvalue={4}' _log . debug ( log_string . format ( info . gv , info . headerIndex , type ( values ) . __name__ , index , value ) )

0<CODESPLIT>https://github.com/alphatwirl/alphatwirl/blob/5138eeba6cd8a334ba52d6c2c022b33c61e3ba38/alphatwirl/concurrently/TaskPackageDropbox.py#L135-L160<CODESPLIT>TaskPackageDropbox.put_multiple<CODESPLIT>return pairs of package indices and results of all tasks<CODESPLIT>def put_multiple ( self , packages ) : pkgidxs = [ self . workingArea . put_package ( p ) for p in packages ] logger = logging . getLogger ( __name__ ) logger . info ( 'submitting {}' . format ( ', ' . join ( [ '{}' . format ( self . workingArea . package_relpath ( i ) ) for i in pkgidxs ] ) ) ) runids = self . dispatcher . run_multiple ( self . workingArea , pkgidxs ) self . runid_pkgidx_map . update ( zip ( runids , pkgidxs ) ) return pkgidxs

0<CODESPLIT>https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/layers/utils.py#L316-L338<CODESPLIT>merge_networks<CODESPLIT>Print information of trainable or all variables without tl . layers . initialize_global_variables ( sess ) .<CODESPLIT>def merge_networks ( layers = None ) : if layers is None : raise Exception ( "layers should be a list of TensorLayer's Layers." ) layer = layers [ 0 ] all_params = [ ] all_layers = [ ] all_drop = { } for l in layers : all_params . extend ( l . all_params ) all_layers . extend ( l . all_layers ) all_drop . update ( l . all_drop ) layer . all_params = list ( all_params ) layer . all_layers = list ( all_layers ) layer . all_drop = dict ( all_drop ) layer . all_layers = list_remove_repeat ( layer . all_layers ) layer . all_params = list_remove_repeat ( layer . all_params ) return layer

1<CODESPLIT>https://github.com/goose3/goose3/blob/e6994b1b1826af2720a091d1bff5ca15594f558d/goose3/__init__.py#L103-L113<CODESPLIT>Goose.extract<CODESPLIT>Extract the most likely article content from the html page<CODESPLIT>def extract ( self , url = None , raw_html = None ) : crawl_candidate = CrawlCandidate ( self . config , url , raw_html ) return self . __crawl ( crawl_candidate )

0<CODESPLIT>https://github.com/Contraz/demosys-py/blob/6466128a3029c4d09631420ccce73024025bd5b6/demosys/context/base.py#L97-L107<CODESPLIT>BaseWindow.clear<CODESPLIT>Draws a frame . Internally it calls the configured timeline s draw method .<CODESPLIT>def clear ( self ) : self . ctx . fbo . clear ( red = self . clear_color [ 0 ] , green = self . clear_color [ 1 ] , blue = self . clear_color [ 2 ] , alpha = self . clear_color [ 3 ] , depth = self . clear_depth , )

1<CODESPLIT>https://github.com/yinkaisheng/Python-UIAutomation-for-Windows/blob/2cc91060982cc8b777152e698d677cc2989bf263/uiautomation/uiautomation.py#L7634-L7645<CODESPLIT>EnumAndLogControlAncestors<CODESPLIT>Print and log control and its ancestors propertyies . control : Control or its subclass . showAllName : bool if False print the first 30 characters of control . Name .<CODESPLIT>def EnumAndLogControlAncestors ( control : Control , showAllName : bool = True ) -> None : lists = [ ] while control : lists . insert ( 0 , control ) control = control . GetParentControl ( ) for i , control in enumerate ( lists ) : LogControl ( control , i , showAllName )

0<CODESPLIT>https://github.com/apache/incubator-heron/blob/ad10325a0febe89ad337e561ebcbe37ec5d9a5ac/heron/executor/src/python/heron_executor.py#L1025-L1051<CODESPLIT>HeronExecutor.get_command_changes<CODESPLIT>Prepare either TMaster or Streaming commands according to shard . The Shell command is attached to all containers . The empty container plan and non - exist container plan are bypassed .<CODESPLIT>def get_command_changes ( self , current_commands , updated_commands ) : commands_to_kill = { } commands_to_keep = { } commands_to_start = { } # if the current command has a matching command in the updated commands we keep it # otherwise we kill it for current_name , current_command in current_commands . items ( ) : # We don't restart tmaster since it watches the packing plan and updates itself. The stream # manager is restarted just to reset state, but we could update it to do so without a restart if current_name in updated_commands . keys ( ) and current_command == updated_commands [ current_name ] and not current_name . startswith ( 'stmgr-' ) : commands_to_keep [ current_name ] = current_command else : commands_to_kill [ current_name ] = current_command # updated commands not in the keep list need to be started for updated_name , updated_command in updated_commands . items ( ) : if updated_name not in commands_to_keep . keys ( ) : commands_to_start [ updated_name ] = updated_command return commands_to_kill , commands_to_keep , commands_to_start

0<CODESPLIT>https://github.com/Jajcus/pyxmpp2/blob/14a40a3950910a9cd008b55f0d8905aa0186ce18/pyxmpp2/sasl/scram.py#L247-L297<CODESPLIT>SCRAMOperations.Hi<CODESPLIT>Process a challenge and return the response .<CODESPLIT>def Hi ( self , str_ , salt , i ) : # pylint: disable=C0103 Uj = self . HMAC ( str_ , salt + b"\000\000\000\001" ) # U1 result = Uj for _ in range ( 2 , i + 1 ) : Uj = self . HMAC ( str_ , Uj ) # Uj = HMAC(str, Uj-1) result = self . XOR ( result , Uj ) # ... XOR Uj-1 XOR Uj return result

0<CODESPLIT>https://github.com/mwgielen/jackal/blob/7fe62732eb5194b7246215d5277fb37c398097bf/jackal/scripts/filter.py#L27-L56<CODESPLIT>WindowsPath2.relative_to<CODESPLIT>Formats the output of another tool in the given way . Has default styles for ranges hosts and services .<CODESPLIT>def relative_to ( self , other ) : return super ( WindowsPath2 , Path2 ( self . path ) ) . relative_to ( Path2 ( other ) . path )

1<CODESPLIT>https://github.com/mwickert/scikit-dsp-comm/blob/5c1353412a4d81a8d7da169057564ecf940f8b5b/sk_dsp_comm/pyaudio_helper.py#L301-L315<CODESPLIT>DSP_io_stream.stream_stats<CODESPLIT>Display basic statistics of callback execution : ideal period between callbacks average measured period between callbacks and average time spent in the callback .<CODESPLIT>def stream_stats ( self ) : Tp = self . frame_length / float ( self . fs ) * 1000 print ( 'Delay (latency) in Entering the Callback the First Time = %6.2f (ms)' % ( self . DSP_tic [ 0 ] * 1000 , ) ) print ( 'Ideal Callback period = %1.2f (ms)' % Tp ) Tmp_mean = np . mean ( np . diff ( np . array ( self . DSP_tic ) ) [ 1 : ] * 1000 ) print ( 'Average Callback Period = %1.2f (ms)' % Tmp_mean ) Tprocess_mean = np . mean ( np . array ( self . DSP_toc ) - np . array ( self . DSP_tic ) ) * 1000 print ( 'Average Callback process time = %1.2f (ms)' % Tprocess_mean )

0<CODESPLIT>https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/prepro.py#L2432-L2459<CODESPLIT>obj_box_coord_scale_to_pixelunit<CODESPLIT>Scale down one coordinates from pixel unit to the ratio of image size i . e . in the range of [ 0 1 ] . It is the reverse process of obj_box_coord_scale_to_pixelunit .<CODESPLIT>def obj_box_coord_scale_to_pixelunit ( coord , shape = None ) : if shape is None : shape = [ 100 , 100 ] imh , imw = shape [ 0 : 2 ] x = int ( coord [ 0 ] * imw ) x2 = int ( coord [ 2 ] * imw ) y = int ( coord [ 1 ] * imh ) y2 = int ( coord [ 3 ] * imh ) return [ x , y , x2 , y2 ]

0<CODESPLIT>https://github.com/google/grumpy/blob/3ec87959189cfcdeae82eb68a47648ac25ceb10b/third_party/stdlib/difflib.py#L986-L989<CODESPLIT>Differ.compare<CODESPLIT>Generate comparison results for a same - tagged range .<CODESPLIT>def compare ( self , a , b ) : cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line

0<CODESPLIT>https://github.com/googleapis/oauth2client/blob/50d20532a748f18e53f7d24ccbe6647132c979a9/oauth2client/client.py#L1033-L1045<CODESPLIT>_save_private_file<CODESPLIT>Detect if the code is running in the Compute Engine environment .<CODESPLIT>def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )

1<CODESPLIT>https://github.com/andsor/pypercolate/blob/92478c1fc4d4ff5ae157f7607fd74f6f9ec360ac/percolate/hpc.py#L638-L702<CODESPLIT>bond_reduce<CODESPLIT>Reduce the canonical averages over several runs<CODESPLIT>def bond_reduce ( row_a , row_b ) : spanning_cluster = ( 'percolation_probability_mean' in row_a . dtype . names and 'percolation_probability_mean' in row_b . dtype . names and 'percolation_probability_m2' in row_a . dtype . names and 'percolation_probability_m2' in row_b . dtype . names ) # initialize return array ret = np . empty_like ( row_a ) def _reducer ( key , transpose = False ) : mean_key = '{}_mean' . format ( key ) m2_key = '{}_m2' . format ( key ) res = simoa . stats . online_variance ( * [ ( row [ 'number_of_runs' ] , row [ mean_key ] . T if transpose else row [ mean_key ] , row [ m2_key ] . T if transpose else row [ m2_key ] , ) for row in [ row_a , row_b ] ] ) ( ret [ mean_key ] , ret [ m2_key ] , ) = ( res [ 1 ] . T , res [ 2 ] . T , ) if transpose else res [ 1 : ] if spanning_cluster : _reducer ( 'percolation_probability' ) _reducer ( 'max_cluster_size' ) _reducer ( 'moments' , transpose = True ) ret [ 'number_of_runs' ] = row_a [ 'number_of_runs' ] + row_b [ 'number_of_runs' ] return ret

1<CODESPLIT>https://github.com/mwickert/scikit-dsp-comm/blob/5c1353412a4d81a8d7da169057564ecf940f8b5b/sk_dsp_comm/multirate_helper.py#L176-L182<CODESPLIT>multirate_IIR.up<CODESPLIT>Upsample and filter the signal<CODESPLIT>def up ( self , x , L_change = 12 ) : y = L_change * ssd . upsample ( x , L_change ) y = signal . sosfilt ( self . sos , y ) return y

1<CODESPLIT>https://github.com/pilosus/ForgeryPy3/blob/e15f2e59538deb4cbfceaac314f5ea897f2d5450/forgery_py/forgery/lorem_ipsum.py#L47-L62<CODESPLIT>words<CODESPLIT>Return random words .<CODESPLIT>def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )

1<CODESPLIT>https://github.com/jazzband/django-analytical/blob/5487fd677bd47bc63fc2cf39597a0adc5d6c9ab3/analytical/templatetags/google_analytics_js.py#L44-L55<CODESPLIT>google_analytics_js<CODESPLIT>Google Analytics tracking template tag .<CODESPLIT>def google_analytics_js ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return GoogleAnalyticsJsNode ( )

1<CODESPLIT>https://github.com/worldcompany/djangoembed/blob/f3f2be283441d91d1f89db780444dc75f7b51902/oembed/parsers/text.py#L12-L61<CODESPLIT>TextBlockParser.parse_data<CODESPLIT>Parses a block of text indiscriminately<CODESPLIT>def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : # create a dictionary of user urls -> rendered responses replacements = { } user_urls = set ( re . findall ( URL_RE , text ) ) for user_url in user_urls : try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) except OEmbedException : if urlize_all_links : replacements [ user_url ] = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) replacement = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) replacements [ user_url ] = replacement . strip ( ) # go through the text recording URLs that can be replaced # taking note of their start & end indexes user_urls = re . finditer ( URL_RE , text ) matches = [ ] for match in user_urls : if match . group ( ) in replacements : matches . append ( [ match . start ( ) , match . end ( ) , match . group ( ) ] ) # replace the URLs in order, offsetting the indices each go for indx , ( start , end , user_url ) in enumerate ( matches ) : replacement = replacements [ user_url ] difference = len ( replacement ) - len ( user_url ) # insert the replacement between two slices of text surrounding the # original url text = text [ : start ] + replacement + text [ end : ] # iterate through the rest of the matches offsetting their indices # based on the difference between replacement/original for j in xrange ( indx + 1 , len ( matches ) ) : matches [ j ] [ 0 ] += difference matches [ j ] [ 1 ] += difference return mark_safe ( text )

1<CODESPLIT>https://github.com/klen/muffin-peewee/blob/8e893e3ea1dfc82fbcfc6efe784308c8d4e2852e/muffin_peewee/mpeewee.py#L89-L96<CODESPLIT>AIODatabase.init_async<CODESPLIT>Use when application is starting .<CODESPLIT>def init_async ( self , loop = None ) : self . _loop = loop or asyncio . get_event_loop ( ) self . _async_lock = asyncio . Lock ( loop = loop ) # FIX: SQLITE in memory database if not self . database == ':memory:' : self . _state = ConnectionLocal ( )

0<CODESPLIT>https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/shoebot/data/geometry.py#L355-L364<CODESPLIT>Bounds.union<CODESPLIT>Returns bounds that encompass the intersection of the two . If there is no overlap between the two None is returned .<CODESPLIT>def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )

0<CODESPLIT>https://github.com/opencobra/cobrapy/blob/9d1987cdb3a395cf4125a3439c3b002ff2be2009/cobra/manipulation/delete.py#L207-L237<CODESPLIT>delete_model_genes<CODESPLIT>remove genes entirely from the model<CODESPLIT>def delete_model_genes ( cobra_model , gene_list , cumulative_deletions = True , disable_orphans = False ) : if disable_orphans : raise NotImplementedError ( "disable_orphans not implemented" ) if not hasattr ( cobra_model , '_trimmed' ) : cobra_model . _trimmed = False cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } # Store the old bounds in here. # older models have this if cobra_model . _trimmed_genes is None : cobra_model . _trimmed_genes = [ ] if cobra_model . _trimmed_reactions is None : cobra_model . _trimmed_reactions = { } # Allow a single gene to be fed in as a string instead of a list. if not hasattr ( gene_list , '__iter__' ) or hasattr ( gene_list , 'id' ) : # cobra.Gene has __iter__ gene_list = [ gene_list ] if not hasattr ( gene_list [ 0 ] , 'id' ) : if gene_list [ 0 ] in cobra_model . genes : tmp_gene_dict = dict ( [ ( x . id , x ) for x in cobra_model . genes ] ) else : # assume we're dealing with names if no match to an id tmp_gene_dict = dict ( [ ( x . name , x ) for x in cobra_model . genes ] ) gene_list = [ tmp_gene_dict [ x ] for x in gene_list ] # Make the genes non-functional for x in gene_list : x . functional = False if cumulative_deletions : gene_list . extend ( cobra_model . _trimmed_genes ) else : undelete_model_genes ( cobra_model ) for the_reaction in find_gene_knockout_reactions ( cobra_model , gene_list ) : # Running this on an already deleted reaction will overwrite the # stored reaction bounds. if the_reaction in cobra_model . _trimmed_reactions : continue old_lower_bound = the_reaction . lower_bound old_upper_bound = the_reaction . upper_bound cobra_model . _trimmed_reactions [ the_reaction ] = ( old_lower_bound , old_upper_bound ) the_reaction . lower_bound = 0. the_reaction . upper_bound = 0. cobra_model . _trimmed = True cobra_model . _trimmed_genes = list ( set ( cobra_model . _trimmed_genes + gene_list ) )

0<CODESPLIT>https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/algorithms/temporal_memory.py#L1206-L1277<CODESPLIT>TemporalMemory.write<CODESPLIT>Reads deserialized data from proto object .<CODESPLIT>def write ( self , proto ) : # capnp fails to save a tuple.  Let's force columnDimensions to list. proto . columnDimensions = list ( self . columnDimensions ) proto . cellsPerColumn = self . cellsPerColumn proto . activationThreshold = self . activationThreshold proto . initialPermanence = round ( self . initialPermanence , EPSILON_ROUND ) proto . connectedPermanence = round ( self . connectedPermanence , EPSILON_ROUND ) proto . minThreshold = self . minThreshold proto . maxNewSynapseCount = self . maxNewSynapseCount proto . permanenceIncrement = round ( self . permanenceIncrement , EPSILON_ROUND ) proto . permanenceDecrement = round ( self . permanenceDecrement , EPSILON_ROUND ) proto . predictedSegmentDecrement = self . predictedSegmentDecrement proto . maxSegmentsPerCell = self . maxSegmentsPerCell proto . maxSynapsesPerSegment = self . maxSynapsesPerSegment self . connections . write ( proto . connections ) self . _random . write ( proto . random ) proto . activeCells = list ( self . activeCells ) proto . winnerCells = list ( self . winnerCells ) protoActiveSegments = proto . init ( "activeSegments" , len ( self . activeSegments ) ) for i , segment in enumerate ( self . activeSegments ) : protoActiveSegments [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoActiveSegments [ i ] . idxOnCell = idx protoMatchingSegments = proto . init ( "matchingSegments" , len ( self . matchingSegments ) ) for i , segment in enumerate ( self . matchingSegments ) : protoMatchingSegments [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoMatchingSegments [ i ] . idxOnCell = idx protoNumActivePotential = proto . init ( "numActivePotentialSynapsesForSegment" , len ( self . numActivePotentialSynapsesForSegment ) ) for i , numActivePotentialSynapses in enumerate ( self . numActivePotentialSynapsesForSegment ) : segment = self . connections . segmentForFlatIdx ( i ) if segment is not None : protoNumActivePotential [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoNumActivePotential [ i ] . idxOnCell = idx protoNumActivePotential [ i ] . number = numActivePotentialSynapses proto . iteration = self . iteration protoLastUsedIteration = proto . init ( "lastUsedIterationForSegment" , len ( self . numActivePotentialSynapsesForSegment ) ) for i , lastUsed in enumerate ( self . lastUsedIterationForSegment ) : segment = self . connections . segmentForFlatIdx ( i ) if segment is not None : protoLastUsedIteration [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoLastUsedIteration [ i ] . idxOnCell = idx protoLastUsedIteration [ i ] . number = lastUsed

0<CODESPLIT>https://github.com/Jajcus/pyxmpp2/blob/14a40a3950910a9cd008b55f0d8905aa0186ce18/pyxmpp2/streamtls.py#L166-L184<CODESPLIT>StreamTLSHandler.handle_tls_connected_event<CODESPLIT>Initiate TLS connection .<CODESPLIT>def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )

1<CODESPLIT>https://github.com/ixc/django-model-settings/blob/654233bf7f13619e4531741f9158e7034eac031b/model_settings/models.py#L25-L30<CODESPLIT>SettingQuerySet.as_dict<CODESPLIT>Returns a SettingDict object for this queryset .<CODESPLIT>def as_dict ( self , default = None ) : settings = SettingDict ( queryset = self , default = default ) return settings

0<CODESPLIT>https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/distance/_sift4.py#L196-L229<CODESPLIT>dist_sift4<CODESPLIT>Return the common Sift4 distance between two terms .<CODESPLIT>def dist_sift4 ( src , tar , max_offset = 5 , max_distance = 0 ) : return Sift4 ( ) . dist ( src , tar , max_offset , max_distance )

1<CODESPLIT>https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/database/connection.py#L59-L83<CODESPLIT>enableConcurrencyChecks<CODESPLIT>Enable the diagnostic feature for debugging unexpected concurrency in acquiring ConnectionWrapper instances .<CODESPLIT>def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return

1<CODESPLIT>https://github.com/swharden/SWHLab/blob/a86c3c65323cec809a4bd4f81919644927094bf5/doc/oldcode/swhlab/core/common.py#L875-L882<CODESPLIT>TK_message<CODESPLIT>use the GUI to pop up a message .<CODESPLIT>def TK_message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) #hide tk window root . attributes ( "-topmost" , True ) #always on top root . lift ( ) #bring to top tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )

1<CODESPLIT>https://github.com/joelfrederico/SciSalt/blob/7bf57c49c7dde0a8b0aa337fbd2fbd527ce7a67f/scisalt/facettools/logbookForm.py#L454-L519<CODESPLIT>LogSelectMenu.setupUI<CODESPLIT>Create graphical objects for menus .<CODESPLIT>def setupUI ( self ) : labelSizePolicy = QSizePolicy ( QSizePolicy . Fixed , QSizePolicy . Fixed ) labelSizePolicy . setHorizontalStretch ( 0 ) labelSizePolicy . setVerticalStretch ( 0 ) menuSizePolicy = QSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Fixed ) menuSizePolicy . setHorizontalStretch ( 0 ) menuSizePolicy . setVerticalStretch ( 0 ) logTypeLayout = QHBoxLayout ( ) logTypeLayout . setSpacing ( 0 ) typeLabel = QLabel ( "Log Type:" ) typeLabel . setMinimumSize ( QSize ( 65 , 0 ) ) typeLabel . setMaximumSize ( QSize ( 65 , 16777215 ) ) typeLabel . setSizePolicy ( labelSizePolicy ) logTypeLayout . addWidget ( typeLabel ) self . logType = QComboBox ( self ) self . logType . setMinimumSize ( QSize ( 100 , 0 ) ) self . logType . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . logType . sizePolicy ( ) . hasHeightForWidth ( ) ) self . logType . setSizePolicy ( menuSizePolicy ) logTypeLayout . addWidget ( self . logType ) logTypeLayout . setStretch ( 1 , 6 ) programLayout = QHBoxLayout ( ) programLayout . setSpacing ( 0 ) programLabel = QLabel ( "Program:" ) programLabel . setMinimumSize ( QSize ( 60 , 0 ) ) programLabel . setMaximumSize ( QSize ( 60 , 16777215 ) ) programLabel . setSizePolicy ( labelSizePolicy ) programLayout . addWidget ( programLabel ) self . programName = QComboBox ( self ) self . programName . setMinimumSize ( QSize ( 100 , 0 ) ) self . programName . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . programName . sizePolicy ( ) . hasHeightForWidth ( ) ) self . programName . setSizePolicy ( menuSizePolicy ) programLayout . addWidget ( self . programName ) programLayout . setStretch ( 1 , 6 ) # Initial instance allows adding additional menus, all following menus can only remove themselves. if self . initialInstance : self . logButton = QPushButton ( "+" , self ) self . logButton . setToolTip ( "Add logbook" ) else : self . logButton = QPushButton ( "-" ) self . logButton . setToolTip ( "Remove logbook" ) self . logButton . setMinimumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setMaximumSize ( QSize ( 16 , 16 ) ) # 24x24 self . logButton . setObjectName ( "roundButton" ) # self.logButton.setAutoFillBackground(True) # region = QRegion(QRect(self.logButton.x()+15, self.logButton.y()+14, 20, 20), QRegion.Ellipse) # self.logButton.setMask(region) self . logButton . setStyleSheet ( "QPushButton {border-radius: 8px;}" ) self . _logSelectLayout = QHBoxLayout ( ) self . _logSelectLayout . setSpacing ( 6 ) self . _logSelectLayout . addLayout ( logTypeLayout ) self . _logSelectLayout . addLayout ( programLayout ) self . _logSelectLayout . addWidget ( self . logButton ) self . _logSelectLayout . setStretch ( 0 , 6 ) self . _logSelectLayout . setStretch ( 1 , 6 )

1<CODESPLIT>https://github.com/trivago/Protector/blob/7ebe7bde965e27737b961a0cb5740724d174fdc7/protector/parser/query_parser.py#L228-L235<CODESPLIT>QueryParser._parse_resolution<CODESPLIT>Parse resolution from the GROUP BY statement . E . g . GROUP BY time ( 10s ) would mean a 10 second resolution : param tokens : : return :<CODESPLIT>def _parse_resolution ( self , tokens ) : return self . resolution_parser . parse ( self . parse_keyword ( Keyword . GROUP_BY , tokens ) )

1<CODESPLIT>https://github.com/ManiacalLabs/BiblioPixel/blob/fd97e6c651a4bbcade64733847f4eec8f7704b7c/bibliopixel/drivers/driver_base.py#L184-L196<CODESPLIT>DriverBase._render<CODESPLIT>Typically called from : py : func : _compute_packet this applies brightness and gamma correction to the pixels controlled by this driver .<CODESPLIT>def _render ( self ) : if self . set_device_brightness : level = 1.0 else : level = self . _brightness / 255.0 gam , ( r , g , b ) = self . gamma . get , self . c_order for i in range ( min ( self . numLEDs , len ( self . _buf ) / 3 ) ) : c = [ int ( level * x ) for x in self . _colors [ i + self . _pos ] ] self . _buf [ i * 3 : ( i + 1 ) * 3 ] = gam ( c [ r ] ) , gam ( c [ g ] ) , gam ( c [ b ] )

0<CODESPLIT>https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmenters/contrast.py#L185-L252<CODESPLIT>adjust_contrast_sigmoid<CODESPLIT>Adjust contrast by scaling each pixel value to 255 * gain * log_2 ( 1 + I_ij / 255 ) .<CODESPLIT>def adjust_contrast_sigmoid ( arr , gain , cutoff ) : # int8 is also possible according to docs # https://docs.opencv.org/3.0-beta/modules/core/doc/operations_on_arrays.html#cv2.LUT , but here it seemed # like `d` was 0 for CV_8S, causing that to fail if arr . dtype . name == "uint8" : min_value , _center_value , max_value = iadt . get_value_range_of_dtype ( arr . dtype ) dynamic_range = max_value - min_value value_range = np . linspace ( 0 , 1.0 , num = dynamic_range + 1 , dtype = np . float32 ) # 255 * 1/(1 + exp(gain*(cutoff - I_ij/255))) # using np.float32(.) here still works when the input is a numpy array of size 1 gain = np . float32 ( gain ) cutoff = np . float32 ( cutoff ) table = min_value + dynamic_range * 1 / ( 1 + np . exp ( gain * ( cutoff - value_range ) ) ) arr_aug = cv2 . LUT ( arr , np . clip ( table , min_value , max_value ) . astype ( arr . dtype ) ) if arr . ndim == 3 and arr_aug . ndim == 2 : return arr_aug [ ... , np . newaxis ] return arr_aug else : return ski_exposure . adjust_sigmoid ( arr , cutoff = cutoff , gain = gain )

1<CODESPLIT>https://github.com/CalebBell/thermo/blob/3857ed023a3e64fd3039a32d53576c24990ef1c3/thermo/reaction.py#L198-L274<CODESPLIT>Hf_g<CODESPLIT>r This function handles the retrieval of a chemical s gas heat of formation . Lookup is based on CASRNs . Will automatically select a data source to use if no Method is provided ; returns None if the data is not available .<CODESPLIT>def Hf_g ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in ATcT_g . index : methods . append ( ATCT_G ) if CASRN in TRC_gas_data . index and not np . isnan ( TRC_gas_data . at [ CASRN , 'Hf' ] ) : methods . append ( TRC ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ATCT_G : _Hfg = float ( ATcT_g . at [ CASRN , 'Hf_298K' ] ) elif Method == TRC : _Hfg = float ( TRC_gas_data . at [ CASRN , 'Hf' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Hfg

0<CODESPLIT>https://github.com/openai/baselines/blob/3301089b48c42b87b396e246ea3f56fa4bfc9678/baselines/common/schedules.py#L64-L73<CODESPLIT>_subproc_worker<CODESPLIT>See Schedule . value<CODESPLIT>def _subproc_worker ( pipe , parent_pipe , env_fn_wrapper , obs_bufs , obs_shapes , obs_dtypes , keys ) : def _write_obs ( maybe_dict_obs ) : flatdict = obs_to_dict ( maybe_dict_obs ) for k in keys : dst = obs_bufs [ k ] . get_obj ( ) dst_np = np . frombuffer ( dst , dtype = obs_dtypes [ k ] ) . reshape ( obs_shapes [ k ] ) # pylint: disable=W0212 np . copyto ( dst_np , flatdict [ k ] ) env = env_fn_wrapper . x ( ) parent_pipe . close ( ) try : while True : cmd , data = pipe . recv ( ) if cmd == 'reset' : pipe . send ( _write_obs ( env . reset ( ) ) ) elif cmd == 'step' : obs , reward , done , info = env . step ( data ) if done : obs = env . reset ( ) pipe . send ( ( _write_obs ( obs ) , reward , done , info ) ) elif cmd == 'render' : pipe . send ( env . render ( mode = 'rgb_array' ) ) elif cmd == 'close' : pipe . send ( None ) break else : raise RuntimeError ( 'Got unrecognized cmd %s' % cmd ) except KeyboardInterrupt : print ( 'ShmemVecEnv worker: got KeyboardInterrupt' ) finally : env . close ( )

0<CODESPLIT>https://github.com/bachiraoun/pyrep/blob/0449bf2fad3e3e8dda855d4686a8869efeefd433/OldRepository.py#L629-L648<CODESPLIT>Repository.walk_directories_relative_path<CODESPLIT>Walk repository and yield all found directories relative path .<CODESPLIT>def walk_directories_relative_path ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) dirNames = dict . keys ( directories ) for d in sorted ( dirNames ) : yield os . path . join ( relativePath , d ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )

1<CODESPLIT>https://github.com/tensorforce/tensorforce/blob/520a8d992230e382f08e315ede5fc477f5e26bfb/tensorforce/core/optimizers/solvers/line_search.py#L53-L67<CODESPLIT>LineSearch.tf_solve<CODESPLIT>Iteratively optimizes $f ( x ) $ for $x$ on the line between $x $ and $x_0$ .<CODESPLIT>def tf_solve ( self , fn_x , x_init , base_value , target_value , estimated_improvement = None ) : return super ( LineSearch , self ) . tf_solve ( fn_x , x_init , base_value , target_value , estimated_improvement )

1<CODESPLIT>https://github.com/DataBiosphere/dsub/blob/443ce31daa6023dc2fd65ef2051796e19d18d5a7/dsub/commands/ddel.py#L30-L90<CODESPLIT>_parse_arguments<CODESPLIT>Parses command line arguments .<CODESPLIT>def _parse_arguments ( ) : # Handle version flag and exit if it was passed. param_util . handle_version_flag ( ) parser = provider_base . create_parser ( sys . argv [ 0 ] ) parser . add_argument ( '--version' , '-v' , default = False , help = 'Print the dsub version and exit.' ) parser . add_argument ( '--jobs' , '-j' , required = True , nargs = '*' , help = 'List of job-ids to delete. Use "*" to delete all running jobs.' ) parser . add_argument ( '--tasks' , '-t' , nargs = '*' , help = 'List of tasks in an array job to delete.' ) parser . add_argument ( '--users' , '-u' , nargs = '*' , default = [ ] , help = """Deletes only those jobs which were submitted by the list of users.           Use "*" to delete jobs of any user.""" ) parser . add_argument ( '--age' , help = """Deletes only those jobs newer than the specified age. Ages can be           listed using a number followed by a unit. Supported units are           s (seconds), m (minutes), h (hours), d (days), w (weeks).           For example: '7d' (7 days). Bare numbers are treated as UTC.""" ) parser . add_argument ( '--label' , nargs = '*' , action = param_util . ListParamAction , default = [ ] , help = 'User labels to match. Tasks returned must match all labels.' , metavar = 'KEY=VALUE' ) # Shared arguments between the "google" and "google-v2" providers google_common = parser . add_argument_group ( title = 'google-common' , description = 'Options common to the "google" and "google-v2" providers' ) google_common . add_argument ( '--project' , help = 'Cloud project ID in which to find and delete the job(s)' ) return provider_base . parse_args ( parser , { 'google' : [ 'project' ] , 'google-v2' : [ 'project' ] , 'test-fails' : [ ] , 'local' : [ ] , } , sys . argv [ 1 : ] )

0<CODESPLIT>https://github.com/edx/ease/blob/a7890ed403da94d03726b0639cd8ebda45af6bbb/ease/predictor_extractor.py#L35-L62<CODESPLIT>create_essay_set_and_dump_model<CODESPLIT>Initialize dictionaries with the textual inputs in the PredictorSet object p_set - PredictorSet object that has had data fed in<CODESPLIT>def create_essay_set_and_dump_model ( text , score , prompt , model_path , additional_array = None ) : essay_set = create_essay_set ( text , score , prompt ) feature_ext , clf = extract_features_and_generate_model ( essay_set , additional_array ) dump_model_to_file ( prompt , feature_ext , clf , model_path )

1<CODESPLIT>https://github.com/profitbricks/profitbricks-sdk-python/blob/2c804b141688eccb07d6ae56601d5c60a62abebd/profitbricks/client.py#L98-L108<CODESPLIT>ProfitBricksService._save_config<CODESPLIT>Save the given user configuration .<CODESPLIT>def _save_config ( self , filename = None ) : if filename is None : filename = self . _config_filename parent_path = os . path . dirname ( filename ) if not os . path . isdir ( parent_path ) : os . makedirs ( parent_path ) with open ( filename , "w" ) as configfile : self . _config . write ( configfile )

0<CODESPLIT>https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/native/cpu/arm.py#L411-L417<CODESPLIT>Armv7RegisterFile._write_APSR<CODESPLIT>Toggle between ARM and Thumb mode<CODESPLIT>def _write_APSR ( self , apsr ) : V = Operators . EXTRACT ( apsr , 28 , 1 ) C = Operators . EXTRACT ( apsr , 29 , 1 ) Z = Operators . EXTRACT ( apsr , 30 , 1 ) N = Operators . EXTRACT ( apsr , 31 , 1 ) self . write ( 'APSR_V' , V ) self . write ( 'APSR_C' , C ) self . write ( 'APSR_Z' , Z ) self . write ( 'APSR_N' , N )

1<CODESPLIT>https://github.com/kata198/indexedredis/blob/f9c85adcf5218dac25acb06eedc63fc2950816fa/IndexedRedis/__init__.py#L1763-L1808<CODESPLIT>IndexedRedisQuery.getMultiple<CODESPLIT>getMultiple - Gets multiple objects with a single atomic operation<CODESPLIT>def getMultiple ( self , pks , cascadeFetch = False ) : if type ( pks ) == set : pks = list ( pks ) if len ( pks ) == 1 : # Optimization to not pipeline on 1 id return IRQueryableList ( [ self . get ( pks [ 0 ] , cascadeFetch = cascadeFetch ) ] , mdl = self . mdl ) conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) for pk in pks : key = self . _get_key_for_id ( pk ) pipeline . hgetall ( key ) res = pipeline . execute ( ) ret = IRQueryableList ( mdl = self . mdl ) i = 0 pksLen = len ( pks ) while i < pksLen : if res [ i ] is None : ret . append ( None ) i += 1 continue res [ i ] [ '_id' ] = pks [ i ] obj = self . _redisResultToObj ( res [ i ] ) ret . append ( obj ) i += 1 if cascadeFetch is True : for obj in ret : if not obj : continue self . _doCascadeFetch ( obj ) return ret

1<CODESPLIT>https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/ethereum/detectors.py#L340-L355<CODESPLIT>DetectIntegerOverflow._signed_sub_overflow<CODESPLIT>Sign extend the value to 512 bits and check the result can be represented in 256 . Following there is a 32 bit excerpt of this condition : a - b - 80000000 - 3fffffff - 00000001 + 00000000 + 00000001 + 3fffffff + 7fffffff + 80000000 False False False False True True True + c0000001 False False False False False False True + ffffffff False False False False False False False + 00000000 True False False False False False False + 00000001 True False False False False False False + 3fffffff True False False False False False False + 7fffffff True True True False False False False<CODESPLIT>def _signed_sub_overflow ( state , a , b ) : sub = Operators . SEXTEND ( a , 256 , 512 ) - Operators . SEXTEND ( b , 256 , 512 ) cond = Operators . OR ( sub < - ( 1 << 255 ) , sub >= ( 1 << 255 ) ) return cond

0<CODESPLIT>https://github.com/joelfrederico/SciSalt/blob/7bf57c49c7dde0a8b0aa337fbd2fbd527ce7a67f/scisalt/PWFA/match.py#L127-L131<CODESPLIT>MatchPlasma.plasma<CODESPLIT>The plasma density in SI units .<CODESPLIT>def plasma ( self , species = _pt . hydrogen ) : return _Plasma ( self . n_p , species = species )

1<CODESPLIT>https://github.com/mar10/wsgidav/blob/cec0d84222fc24bea01be1cea91729001963f172/wsgidav/server/ext_wsgiutils_server.py#L369-L381<CODESPLIT>ExtServer.serve_forever_stoppable<CODESPLIT>Handle one request at a time until stop_serve_forever () .<CODESPLIT>def serve_forever_stoppable ( self ) : self . stop_request = False self . stopped = False while not self . stop_request : self . handle_request ( ) #        _logger.info "serve_forever_stoppable() stopped." self . stopped = True

1<CODESPLIT>https://github.com/googleapis/oauth2client/blob/50d20532a748f18e53f7d24ccbe6647132c979a9/oauth2client/service_account.py#L144-L191<CODESPLIT>ServiceAccountCredentials._from_parsed_json_keyfile<CODESPLIT>Helper for factory constructors from JSON keyfile .<CODESPLIT>def _from_parsed_json_keyfile ( cls , keyfile_dict , scopes , token_uri = None , revoke_uri = None ) : creds_type = keyfile_dict . get ( 'type' ) if creds_type != client . SERVICE_ACCOUNT : raise ValueError ( 'Unexpected credentials type' , creds_type , 'Expected' , client . SERVICE_ACCOUNT ) service_account_email = keyfile_dict [ 'client_email' ] private_key_pkcs8_pem = keyfile_dict [ 'private_key' ] private_key_id = keyfile_dict [ 'private_key_id' ] client_id = keyfile_dict [ 'client_id' ] if not token_uri : token_uri = keyfile_dict . get ( 'token_uri' , oauth2client . GOOGLE_TOKEN_URI ) if not revoke_uri : revoke_uri = keyfile_dict . get ( 'revoke_uri' , oauth2client . GOOGLE_REVOKE_URI ) signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) credentials = cls ( service_account_email , signer , scopes = scopes , private_key_id = private_key_id , client_id = client_id , token_uri = token_uri , revoke_uri = revoke_uri ) credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem return credentials

1<CODESPLIT>https://github.com/alberanid/python-iplib/blob/488b56fe57ad836b27feec9e76f51883db28faa6/iplib.py#L700-L703<CODESPLIT>IPv4NetMask.get_wildcard<CODESPLIT>Return the wildcard bits notation of the netmask .<CODESPLIT>def get_wildcard ( self ) : return _convert ( self . _ip , notation = NM_WILDCARD , inotation = IP_DOT , _check = False , _isnm = self . _isnm )

0<CODESPLIT>https://github.com/mwickert/scikit-dsp-comm/blob/5c1353412a4d81a8d7da169057564ecf940f8b5b/sk_dsp_comm/fec_conv.py#L248-L469<CODESPLIT>soft_Pk<CODESPLIT>A method which performs Viterbi decoding of noisy bit stream taking as input soft bit values centered on + / - 1 and returning hard decision 0 / 1 bits . Parameters ---------- x : Received noisy bit values centered on + / - 1 at one sample per bit metric_type : hard - Hard decision metric . Expects binary or 0 / 1 input values . unquant - unquantized soft decision decoding . Expects + / - 1 input values . soft - soft decision decoding . quant_level : The quantization level for soft decoding . Expected input values between 0 and 2^quant_level - 1 . 0 represents the most confident 0 and 2^quant_level - 1 represents the most confident 1 . Only used for soft metric type . Returns ------- y : Decoded 0 / 1 bit stream Examples -------- >>> import numpy as np >>> from numpy . random import randint >>> import sk_dsp_comm . fec_conv as fec >>> import sk_dsp_comm . digitalcom as dc >>> import matplotlib . pyplot as plt >>> # Soft decision rate 1 / 2 simulation >>> N_bits_per_frame = 10000 >>> EbN0 = 4 >>> total_bit_errors = 0 >>> total_bit_count = 0 >>> cc1 = fec . fec_conv (( 11101 10011 ) 25 ) >>> # Encode with shift register starting state of 0000 >>> state = 0000 >>> while total_bit_errors < 100 : >>> # Create 100000 random 0 / 1 bits >>> x = randint ( 0 2 N_bits_per_frame ) >>> y state = cc1 . conv_encoder ( x state ) >>> # Add channel noise to bits include antipodal level shift to [ - 1 1 ] >>> yn_soft = dc . cpx_AWGN ( 2 * y - 1 EbN0 - 3 1 ) # Channel SNR is 3 dB less for rate 1 / 2 >>> yn_hard = (( np . sign ( yn_soft . real ) + 1 ) / 2 ) . astype ( int ) >>> z = cc1 . viterbi_decoder ( yn_hard hard ) >>> # Count bit errors >>> bit_count bit_errors = dc . bit_errors ( x z ) >>> total_bit_errors + = bit_errors >>> total_bit_count + = bit_count >>> print ( Bits Received = %d Bit errors = %d BEP = %1 . 2e % \ ( total_bit_count total_bit_errors \ total_bit_errors / total_bit_count )) >>> print ( ***************************************************** ) >>> print ( Bits Received = %d Bit errors = %d BEP = %1 . 2e % \ ( total_bit_count total_bit_errors \ total_bit_errors / total_bit_count )) Rate 1 / 2 Object kmax = 0 taumax = 0 Bits Received = 9976 Bit errors = 77 BEP = 7 . 72e - 03 kmax = 0 taumax = 0 Bits Received = 19952 Bit errors = 175 BEP = 8 . 77e - 03 ***************************************************** Bits Received = 19952 Bit errors = 175 BEP = 8 . 77e - 03 >>> # Consider the trellis traceback after the sim completes >>> cc1 . traceback_plot () >>> plt . show () >>> # Compare a collection of simulation results with soft decision >>> # bounds >>> SNRdB = np . arange ( 0 12 . 1 ) >>> Pb_uc = fec . conv_Pb_bound ( 1 / 3 7 [ 4 12 20 72 225 ] SNRdB 2 ) >>> Pb_s_third_3 = fec . conv_Pb_bound ( 1 / 3 8 [ 3 0 15 ] SNRdB 1 ) >>> Pb_s_third_4 = fec . conv_Pb_bound ( 1 / 3 10 [ 6 0 6 0 ] SNRdB 1 ) >>> Pb_s_third_5 = fec . conv_Pb_bound ( 1 / 3 12 [ 12 0 12 0 56 ] SNRdB 1 ) >>> Pb_s_third_6 = fec . conv_Pb_bound ( 1 / 3 13 [ 1 8 26 20 19 62 ] SNRdB 1 ) >>> Pb_s_third_7 = fec . conv_Pb_bound ( 1 / 3 14 [ 1 0 20 0 53 0 184 ] SNRdB 1 ) >>> Pb_s_third_8 = fec . conv_Pb_bound ( 1 / 3 16 [ 1 0 24 0 113 0 287 0 ] SNRdB 1 ) >>> Pb_s_half = fec . conv_Pb_bound ( 1 / 2 7 [ 4 12 20 72 225 ] SNRdB 1 ) >>> plt . figure ( figsize = ( 5 5 )) >>> plt . semilogy ( SNRdB Pb_uc ) >>> plt . semilogy ( SNRdB Pb_s_third_3 -- ) >>> plt . semilogy ( SNRdB Pb_s_third_4 -- ) >>> plt . semilogy ( SNRdB Pb_s_third_5 g ) >>> plt . semilogy ( SNRdB Pb_s_third_6 -- ) >>> plt . semilogy ( SNRdB Pb_s_third_7 -- ) >>> plt . semilogy ( SNRdB Pb_s_third_8 -- ) >>> plt . semilogy ( [ 0 1 2 3 4 5 ] [ 9 . 08e - 02 2 . 73e - 02 6 . 52e - 03 \ 8 . 94e - 04 8 . 54e - 05 5e - 6 ] gs ) >>> plt . axis ( [ 0 12 1e - 7 1e0 ] ) >>> plt . title ( r Soft Decision Rate 1 / 2 Coding Measurements ) >>> plt . xlabel ( r $E_b / N_0$ ( dB ) ) >>> plt . ylabel ( r Symbol Error Probability ) >>> plt . legend (( Uncoded BPSK R = 1 / 3 K = 3 Soft \ R = 1 / 3 K = 4 Soft R = 1 / 3 K = 5 Soft \ R = 1 / 3 K = 6 Soft R = 1 / 3 K = 7 Soft \ R = 1 / 3 K = 8 Soft R = 1 / 3 K = 5 Sim \ Simulation ) loc = upper right ) >>> plt . grid () ; >>> plt . show () >>> # Hard decision rate 1 / 3 simulation >>> N_bits_per_frame = 10000 >>> EbN0 = 3 >>> total_bit_errors = 0 >>> total_bit_count = 0 >>> cc2 = fec . fec_conv (( 11111 11011 10101 ) 25 ) >>> # Encode with shift register starting state of 0000 >>> state = 0000 >>> while total_bit_errors < 100 : >>> # Create 100000 random 0 / 1 bits >>> x = randint ( 0 2 N_bits_per_frame ) >>> y state = cc2 . conv_encoder ( x state ) >>> # Add channel noise to bits include antipodal level shift to [ - 1 1 ] >>> yn_soft = dc . cpx_AWGN ( 2 * y - 1 EbN0 - 10 * np . log10 ( 3 ) 1 ) # Channel SNR is 10 * log10 ( 3 ) dB less >>> yn_hard = (( np . sign ( yn_soft . real ) + 1 ) / 2 ) . astype ( int ) >>> z = cc2 . viterbi_decoder ( yn_hard . real hard ) >>> # Count bit errors >>> bit_count bit_errors = dc . bit_errors ( x z ) >>> total_bit_errors + = bit_errors >>> total_bit_count + = bit_count >>> print ( Bits Received = %d Bit errors = %d BEP = %1 . 2e % \ ( total_bit_count total_bit_errors \ total_bit_errors / total_bit_count )) >>> print ( ***************************************************** ) >>> print ( Bits Received = %d Bit errors = %d BEP = %1 . 2e % \ ( total_bit_count total_bit_errors \ total_bit_errors / total_bit_count )) Rate 1 / 3 Object kmax = 0 taumax = 0 Bits Received = 9976 Bit errors = 251 BEP = 2 . 52e - 02 ***************************************************** Bits Received = 9976 Bit errors = 251 BEP = 2 . 52e - 02 >>> # Compare a collection of simulation results with hard decision >>> # bounds >>> SNRdB = np . arange ( 0 12 . 1 ) >>> Pb_uc = fec . conv_Pb_bound ( 1 / 3 7 [ 4 12 20 72 225 ] SNRdB 2 ) >>> Pb_s_third_3_hard = fec . conv_Pb_bound ( 1 / 3 8 [ 3 0 15 0 58 0 201 0 ] SNRdB 0 ) >>> Pb_s_third_5_hard = fec . conv_Pb_bound ( 1 / 3 12 [ 12 0 12 0 56 0 320 0 ] SNRdB 0 ) >>> Pb_s_third_7_hard = fec . conv_Pb_bound ( 1 / 3 14 [ 1 0 20 0 53 0 184 ] SNRdB 0 ) >>> Pb_s_third_5_hard_sim = np . array ( [ 8 . 94e - 04 1 . 11e - 04 8 . 73e - 06 ] ) >>> plt . figure ( figsize = ( 5 5 )) >>> plt . semilogy ( SNRdB Pb_uc ) >>> plt . semilogy ( SNRdB Pb_s_third_3_hard r -- ) >>> plt . semilogy ( SNRdB Pb_s_third_5_hard g -- ) >>> plt . semilogy ( SNRdB Pb_s_third_7_hard k -- ) >>> plt . semilogy ( np . array ( [ 5 6 7 ] ) Pb_s_third_5_hard_sim sg ) >>> plt . axis ( [ 0 12 1e - 7 1e0 ] ) >>> plt . title ( r Hard Decision Rate 1 / 3 Coding Measurements ) >>> plt . xlabel ( r $E_b / N_0$ ( dB ) ) >>> plt . ylabel ( r Symbol Error Probability ) >>> plt . legend (( Uncoded BPSK R = 1 / 3 K = 3 Hard \ R = 1 / 3 K = 5 Hard R = 1 / 3 K = 7 Hard \ ) loc = upper right ) >>> plt . grid () ; >>> plt . show () >>> # Show the traceback for the rate 1 / 3 hard decision case >>> cc2 . traceback_plot ()<CODESPLIT>def soft_Pk ( k , R , SNR , M = 2 ) : if M == 2 : Pk = Q_fctn ( np . sqrt ( 2. * k * R * SNR ) ) else : Pk = 4. / np . log2 ( M ) * ( 1 - 1. / np . sqrt ( M ) ) * Q_fctn ( np . sqrt ( 3 * k * R * np . log2 ( M ) / float ( M - 1 ) * SNR ) ) return Pk

1<CODESPLIT>https://github.com/fantasticfears/kgekit/blob/5e464e1fc3ae9c7e216f6dd94f879a967d065247/kgekit/io.py#L60-L65<CODESPLIT>read_labels<CODESPLIT>read label files . Format : ent label<CODESPLIT>def read_labels ( filename , delimiter = DEFAULT_DELIMITER ) : _assert_good_file ( filename ) with open ( filename ) as f : labels = [ _label_processing ( l , delimiter ) for l in f ] return labels

1<CODESPLIT>https://github.com/ungarj/mapchete/blob/d482918d0e66a5b414dff6aa7cc854e01fc60ee4/mapchete/io/vector.py#L446-L463<CODESPLIT>multipart_to_singleparts<CODESPLIT>Yield single part geometries if geom is multipart otherwise yield geom .<CODESPLIT>def multipart_to_singleparts ( geom ) : if isinstance ( geom , base . BaseGeometry ) : if hasattr ( geom , "geoms" ) : for subgeom in geom : yield subgeom else : yield geom

0<CODESPLIT>https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/core/smtlib/visitors.py#L362-L368<CODESPLIT>ArithmeticSimplifier.visit_BitVecConcat<CODESPLIT>constant folding if all operands of an expression are a Constant do the math<CODESPLIT>def visit_BitVecConcat ( self , expression , * operands ) : op = expression . operands [ 0 ] value = None end = None begining = None for o in operands : # If found a non BitVecExtract, do not apply if not isinstance ( o , BitVecExtract ) : return None # Set the value for the first item if value is None : value = o . value begining = o . begining end = o . end else : # If concat of extracts of different values do not apply if value is not o . value : return None # If concat of non contiguous extracs do not apply if begining != o . end + 1 : return None # update begining variable begining = o . begining if value is not None : if end + 1 == value . size and begining == 0 : return value else : return BitVecExtract ( value , begining , end - begining + 1 , taint = expression . taint )

0<CODESPLIT>https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/distance/_minkowski.py#L45-L107<CODESPLIT>sim_minkowski<CODESPLIT>Return the Minkowski distance ( : math : L^p - norm ) of two strings .<CODESPLIT>def sim_minkowski ( src , tar , qval = 2 , pval = 1 , alphabet = None ) : return Minkowski ( ) . sim ( src , tar , qval , pval , alphabet )

0<CODESPLIT>https://github.com/madprime/vcf2clinvar/blob/d5bbf6df2902c6cabe9ef1894cfac527e90fa32a/vcf2clinvar/common.py#L78-L84<CODESPLIT>match_to_clinvar<CODESPLIT>Return Allele data as dict object .<CODESPLIT>def match_to_clinvar ( genome_file , clin_file ) : clin_curr_line = _next_line ( clin_file ) genome_curr_line = _next_line ( genome_file ) # Ignores all the lines that start with a hashtag while clin_curr_line . startswith ( '#' ) : clin_curr_line = _next_line ( clin_file ) while genome_curr_line . startswith ( '#' ) : genome_curr_line = _next_line ( genome_file ) # Advance through both files simultaneously to find matches while clin_curr_line and genome_curr_line : # Advance a file when positions aren't equal. clin_curr_pos = VCFLine . get_pos ( clin_curr_line ) genome_curr_pos = VCFLine . get_pos ( genome_curr_line ) try : if clin_curr_pos [ 'chrom' ] > genome_curr_pos [ 'chrom' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'chrom' ] < genome_curr_pos [ 'chrom' ] : clin_curr_line = _next_line ( clin_file ) continue if clin_curr_pos [ 'pos' ] > genome_curr_pos [ 'pos' ] : genome_curr_line = _next_line ( genome_file ) continue elif clin_curr_pos [ 'pos' ] < genome_curr_pos [ 'pos' ] : clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break # If we get here, start positions match. # Look for allele matching. genome_vcf_line = GenomeVCFLine ( vcf_line = genome_curr_line , skip_info = True ) # We can skip if genome has no allele information for this point. if not genome_vcf_line . genotype_allele_indexes : genome_curr_line = _next_line ( genome_file ) continue # Match only if ClinVar and Genome ref_alleles match. clinvar_vcf_line = ClinVarVCFLine ( vcf_line = clin_curr_line ) if not genome_vcf_line . ref_allele == clinvar_vcf_line . ref_allele : try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) continue except StopIteration : break # Determine genome alleles and zygosity. Zygosity is assumed to be one # of: heterozygous, homozygous, or hemizygous. genotype_allele_indexes = genome_vcf_line . genotype_allele_indexes genome_alleles = [ genome_vcf_line . alleles [ x ] for x in genotype_allele_indexes ] if len ( genome_alleles ) == 1 : zygosity = 'Hem' elif len ( genome_alleles ) == 2 : if genome_alleles [ 0 ] . sequence == genome_alleles [ 1 ] . sequence : zygosity = 'Hom' genome_alleles = [ genome_alleles [ 0 ] ] else : zygosity = 'Het' else : raise ValueError ( 'This code only expects to work on genomes ' + 'with one or two alleles called at each ' + 'location. The following line violates this:' + str ( genome_vcf_line ) ) # Look for matches to ClinVar alleles. for genome_allele in genome_alleles : for allele in clinvar_vcf_line . alleles : if genome_allele . sequence == allele . sequence : # The 'records' attribute is specific to ClinVarAlleles. if hasattr ( allele , 'records' ) : yield ( genome_vcf_line , allele , zygosity ) # Done matching, move on. try : genome_curr_line = _next_line ( genome_file ) clin_curr_line = _next_line ( clin_file ) except StopIteration : break

1<CODESPLIT>https://github.com/sam-washington/requests-aws4auth/blob/1201e470c6d5847b7fe42e937a55755e1895e72c/requests_aws4auth/aws4auth.py#L397-L452<CODESPLIT>AWS4Auth.parse_date<CODESPLIT>Check if date_str is in a recognised format and return an ISO yyyy - mm - dd format version if so . Raise DateFormatError if not .<CODESPLIT>def parse_date ( date_str ) : months = [ 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' ] formats = { # RFC 7231, e.g. 'Mon, 09 Sep 2011 23:36:00 GMT' r'^(?:\w{3}, )?(\d{2}) (\w{3}) (\d{4})\D.*$' : lambda m : '{}-{:02d}-{}' . format ( m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , # RFC 850 (e.g. Sunday, 06-Nov-94 08:49:37 GMT) # assumes current century r'^\w+day, (\d{2})-(\w{3})-(\d{2})\D.*$' : lambda m : '{}{}-{:02d}-{}' . format ( str ( datetime . date . today ( ) . year ) [ : 2 ] , m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , # C time, e.g. 'Wed Dec 4 00:00:00 2002' r'^\w{3} (\w{3}) (\d{1,2}) \d{2}:\d{2}:\d{2} (\d{4})$' : lambda m : '{}-{:02d}-{:02d}' . format ( m . group ( 3 ) , months . index ( m . group ( 1 ) . lower ( ) ) + 1 , int ( m . group ( 2 ) ) ) , # x-amz-date format dates, e.g. 20100325T010101Z r'^(\d{4})(\d{2})(\d{2})T\d{6}Z$' : lambda m : '{}-{}-{}' . format ( * m . groups ( ) ) , # ISO 8601 / RFC 3339, e.g. '2009-03-25T10:11:12.13-01:00' r'^(\d{4}-\d{2}-\d{2})(?:[Tt].*)?$' : lambda m : m . group ( 1 ) , } out_date = None for regex , xform in formats . items ( ) : m = re . search ( regex , date_str ) if m : out_date = xform ( m ) break if out_date is None : raise DateFormatError else : return out_date

