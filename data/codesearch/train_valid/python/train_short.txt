1<CODESPLIT>https://github.com/rosenbrockc/acorn/blob/9a44d1a1ad8bfc2c54a6b56d9efe54433a797820/acorn/ipython.py#L201-L256<CODESPLIT>InteractiveDecorator._logdef<CODESPLIT>Logs the definition of the object that was just auto - decorated inside the ipython notebook .<CODESPLIT>def _logdef ( self , n , o , otype ) : import re try : #The latest input cell will be the one that this got executed #from. TODO: actually, if acorn got imported after the fact, then #the import would have caused all the undecorated functions to be #decorated as soon as acorn imported. I suppose we just won't have #any code for that case. if otype == "classes" : cellno = max ( [ int ( k [ 2 : ] ) for k in self . shell . user_ns . keys ( ) if re . match ( "_i\d+" , k ) ] ) elif otype == "functions" : cellno = int ( o . __code__ . co_filename . strip ( "<>" ) . split ( '-' ) [ 2 ] ) except : #This must not have been an ipython notebook declaration, so we #don't store the code. cellno = None pass code = "" if cellno is not None : cellstr = "_i{0:d}" . format ( cellno ) if cellstr in self . shell . user_ns : cellcode = self . shell . user_ns [ cellstr ] import ast astm = ast . parse ( cellcode ) ab = astm . body parts = { ab [ i ] . name : ( ab [ i ] . lineno , None if i + 1 >= len ( ab ) else ab [ i + 1 ] . lineno ) for i , d in enumerate ( ab ) } if n in parts : celllines = cellcode . split ( '\n' ) start , end = parts [ n ] if end is not None : code = celllines [ start - 1 : end - 1 ] else : code = celllines [ start - 1 : ] #Now, we actually create the entry. Since the execution for function #definitions is almost instantaneous, we just log the pre and post #events at the same time. from time import time from acorn . logging . database import record entry = { "m" : "def" , "a" : None , "s" : time ( ) , "r" : None , "c" : code , } from acorn import msg record ( "__main__.{}" . format ( n ) , entry , diff = True ) msg . info ( entry , 1 )

0<CODESPLIT>https://github.com/EmbodiedCognition/py-c3d/blob/391493d9cb4c6b4aaeee4de2930685e3a67f5845/c3d.py#L962-L989<CODESPLIT>Writer._write_frames<CODESPLIT>Write metadata to a file handle .<CODESPLIT>def _write_frames ( self , handle ) : assert handle . tell ( ) == 512 * ( self . header . data_block - 1 ) scale = abs ( self . point_scale ) is_float = self . point_scale < 0 point_dtype = [ np . int16 , np . float32 ] [ is_float ] point_scale = [ scale , 1 ] [ is_float ] point_format = 'if' [ is_float ] raw = np . empty ( ( self . point_used , 4 ) , point_dtype ) for points , analog in self . _frames : valid = points [ : , 3 ] > - 1 raw [ ~ valid , 3 ] = - 1 raw [ valid , : 3 ] = points [ valid , : 3 ] / self . _point_scale raw [ valid , 3 ] = ( ( ( points [ valid , 4 ] ) . astype ( np . uint8 ) << 8 ) | ( points [ valid , 3 ] / scale ) . astype ( np . uint16 ) ) point = array . array ( point_format ) point . extend ( raw . flatten ( ) ) point . tofile ( handle ) analog = array . array ( point_format ) analog . extend ( analog ) analog . tofile ( handle ) self . _pad_block ( handle )

1<CODESPLIT>https://github.com/python-wink/python-wink/blob/cf8bdce8c6518f30b91b23aa7aa32e89c2ce48da/src/pywink/api.py#L205-L231<CODESPLIT>WinkApiInterface.update_firmware<CODESPLIT>Make a call to the update_firmware endpoint . As far as I know this is only valid for Wink hubs .<CODESPLIT>def update_firmware ( self , device , id_override = None , type_override = None ) : object_id = id_override or device . object_id ( ) object_type = type_override or device . object_type ( ) url_string = "{}/{}s/{}/update_firmware" . format ( self . BASE_URL , object_type , object_id ) try : arequest = requests . post ( url_string , headers = API_HEADERS ) response_json = arequest . json ( ) return response_json except requests . exceptions . RequestException : return None

0<CODESPLIT>https://github.com/whiteclover/dbpy/blob/3d9ce85f55cfb39cced22081e525f79581b26b3a/db/__init__.py#L173-L182<CODESPLIT>select<CODESPLIT>datbase dialect<CODESPLIT>def select ( table , key = 'default' ) : database = choice ( __db [ key + '.slave' ] ) return database . select ( table )

1<CODESPLIT>https://github.com/raghakot/keras-vis/blob/668b0e11dab93f3487f23c17e07f40554a8939e9/vis/optimizer.py#L69-L87<CODESPLIT>Optimizer._rmsprop<CODESPLIT>Uses RMSProp to compute step from gradients .<CODESPLIT>def _rmsprop ( self , grads , cache = None , decay_rate = 0.95 ) : if cache is None : cache = np . zeros_like ( grads ) cache = decay_rate * cache + ( 1 - decay_rate ) * grads ** 2 step = - grads / np . sqrt ( cache + K . epsilon ( ) ) return step , cache

1<CODESPLIT>https://github.com/guaix-ucm/numina/blob/6c829495df8937f77c2de9383c1038ffb3e713e3/numina/array/ccd_line.py#L229-L232<CODESPLIT>ArcLine.linspace_pix<CODESPLIT>Return x y values evaluated with a given pixel step .<CODESPLIT>def linspace_pix ( self , start = None , stop = None , pixel_step = 1 , y_vs_x = False ) : return CCDLine . linspace_pix ( self , start = start , stop = stop , pixel_step = pixel_step , y_vs_x = y_vs_x )

1<CODESPLIT>https://github.com/ModisWorks/modis/blob/1f1225c9841835ec1d1831fc196306527567db8b/modis/discord_modis/modules/_tools/ui_embed.py#L33-L70<CODESPLIT>UI.build<CODESPLIT>Builds Discord embed GUI<CODESPLIT>def build ( self ) : if self . colour : embed = discord . Embed ( title = self . title , type = 'rich' , description = self . description , colour = self . colour ) else : embed = discord . Embed ( title = self . title , type = 'rich' , description = self . description ) if self . thumbnail : embed . set_thumbnail ( url = self . thumbnail ) if self . image : embed . set_image ( url = self . image ) embed . set_author ( name = "Modis" , url = "https://musicbyango.com/modis/" , icon_url = "http://musicbyango.com/modis/dp/modis64t.png" ) for pack in self . datapacks : embed . add_field ( name = pack [ 0 ] , value = pack [ 1 ] , inline = pack [ 2 ] ) return embed

1<CODESPLIT>https://github.com/Microsoft/azure-devops-python-api/blob/4777ffda2f5052fabbaddb2abe9cb434e0cf1aa8/azure-devops/azure/devops/v5_0/build/build_client.py#L1759-L1778<CODESPLIT>BuildClient.save_template<CODESPLIT>SaveTemplate . Updates an existing build definition template . : param : class : <BuildDefinitionTemplate > <azure . devops . v5_0 . build . models . BuildDefinitionTemplate > template : The new version of the template . : param str project : Project ID or project name : param str template_id : The ID of the template . : rtype : : class : <BuildDefinitionTemplate > <azure . devops . v5_0 . build . models . BuildDefinitionTemplate ><CODESPLIT>def save_template ( self , template , project , template_id ) : route_values = { } if project is not None : route_values [ 'project' ] = self . _serialize . url ( 'project' , project , 'str' ) if template_id is not None : route_values [ 'templateId' ] = self . _serialize . url ( 'template_id' , template_id , 'str' ) content = self . _serialize . body ( template , 'BuildDefinitionTemplate' ) response = self . _send ( http_method = 'PUT' , location_id = 'e884571e-7f92-4d6a-9274-3f5649900835' , version = '5.0' , route_values = route_values , content = content ) return self . _deserialize ( 'BuildDefinitionTemplate' , response )

0<CODESPLIT>https://github.com/jaybaird/python-bloomfilter/blob/2bbe01ad49965bf759e31781e6820408068862ac/pybloom/pybloom.py#L388-L410<CODESPLIT>ScalableBloomFilter.fromfile<CODESPLIT>Serialize this ScalableBloomFilter into the file - object f .<CODESPLIT>def fromfile ( cls , f ) : filter = cls ( ) filter . _setup ( * unpack ( cls . FILE_FMT , f . read ( calcsize ( cls . FILE_FMT ) ) ) ) nfilters , = unpack ( b'<l' , f . read ( calcsize ( b'<l' ) ) ) if nfilters > 0 : header_fmt = b'<' + b'Q' * nfilters bytes = f . read ( calcsize ( header_fmt ) ) filter_lengths = unpack ( header_fmt , bytes ) for fl in filter_lengths : filter . filters . append ( BloomFilter . fromfile ( f , fl ) ) else : filter . filters = [ ] return filter

1<CODESPLIT>https://github.com/awslabs/serverless-application-model/blob/cccb0c96b5c91e53355ebc07e542467303a5eedd/samtranslator/sdk/template.py#L54-L64<CODESPLIT>SamTemplate.get<CODESPLIT>Gets the resource at the given logicalId if present<CODESPLIT>def get ( self , logicalId ) : if logicalId not in self . resources : return None return SamResource ( self . resources . get ( logicalId ) )

0<CODESPLIT>https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/utils/analysisrequest.py#L130-L177<CODESPLIT>_resolve_items_to_service_uids<CODESPLIT>This function returns a list of UIDs from analyses services from its parameters . : param analyses_serv : A list ( or one object ) of service - related info items . see _resolve_items_to_service_uids () docstring . : type analyses_serv : list : param values : a dict where keys are AR|Sample schema field names . : type values : dict : returns : a list of analyses services UIDs<CODESPLIT>def _resolve_items_to_service_uids ( items ) : def resolve_to_uid ( item ) : if api . is_uid ( item ) : return item elif IAnalysisService . providedBy ( item ) : return item . UID ( ) elif IRoutineAnalysis . providedBy ( item ) : return item . getServiceUID ( ) bsc = api . get_tool ( "bika_setup_catalog" ) brains = bsc ( portal_type = 'AnalysisService' , getKeyword = item ) if brains : return brains [ 0 ] . UID brains = bsc ( portal_type = 'AnalysisService' , title = item ) if brains : return brains [ 0 ] . UID raise RuntimeError ( str ( item ) + " should be the UID, title, keyword " " or title of an AnalysisService." ) # Maybe only a single item was passed if type ( items ) not in ( list , tuple ) : items = [ items , ] service_uids = map ( resolve_to_uid , list ( set ( items ) ) ) return list ( set ( service_uids ) )

1<CODESPLIT>https://github.com/mozilla/build-mar/blob/d8c3b3469e55654d31f430cb343fd89392196c4e/src/mardor/writer.py#L147-L158<CODESPLIT>MarWriter.add_fileobj<CODESPLIT>Add the contents of a file object to the MAR file .<CODESPLIT>def add_fileobj ( self , fileobj , path , compress , flags = None ) : f = file_iter ( fileobj ) flags = flags or os . stat ( path ) & 0o777 return self . add_stream ( f , path , compress , flags )

0<CODESPLIT>https://github.com/allenai/allennlp/blob/648a36f77db7e45784c047176074f98534c76636/allennlp/nn/util.py#L337-L367<CODESPLIT>masked_max<CODESPLIT>To calculate mean along certain dimensions on masked values<CODESPLIT>def masked_max ( vector : torch . Tensor , mask : torch . Tensor , dim : int , keepdim : bool = False , min_val : float = - 1e7 ) -> torch . Tensor : one_minus_mask = ( 1.0 - mask ) . byte ( ) replaced_vector = vector . masked_fill ( one_minus_mask , min_val ) max_value , _ = replaced_vector . max ( dim = dim , keepdim = keepdim ) return max_value

0<CODESPLIT>https://github.com/kyper-data/python-highcharts/blob/a4c488ae5c2e125616efad5a722f3dfd8a9bc450/highcharts/highmaps/highmaps.py#L201-L223<CODESPLIT>Highmap.add_CSSsource<CODESPLIT>set data for series option in highmaps<CODESPLIT>def add_CSSsource ( self , new_src ) : if isinstance ( new_src , list ) : for h in new_src : self . CSSsource . append ( h ) elif isinstance ( new_src , basestring ) : self . CSSsource . append ( new_src ) else : raise OptionTypeError ( "Option: %s Not Allowed For Series Type: %s" % type ( new_src ) )

0<CODESPLIT>https://github.com/kappius/pyheaderfile/blob/8d587dadae538adcec527fd8e74ad89ed5e2006a/pyheaderfile/drive.py#L101-L110<CODESPLIT>GSheet._login<CODESPLIT>Makes imports : return :<CODESPLIT>def _login ( self ) : # TODO(dmvieira) login changed to oauth2 self . gc = self . gspread . login ( self . email , self . password )

1<CODESPLIT>https://github.com/SKA-ScienceDataProcessor/integration-prototype/blob/8c8006de6ad71dcd44114b0338780738079c87d4/sip/execution_control/configuration_db/sip_config_db/scheduling/processing_block.py#L36-L47<CODESPLIT>ProcessingBlock.get_id<CODESPLIT>Generate a Processing Block ( PB ) Instance ID .<CODESPLIT>def get_id ( date : datetime . datetime ) -> str : date = date . strftime ( '%Y%m%d' ) return 'PB-{}-{}-{:03d}' . format ( date , 'sip' , randint ( 0 , 100 ) )

0<CODESPLIT>https://github.com/thejunglejane/datums/blob/2250b365e37ba952c2426edc615c1487afabae6e/datums/models/base.py#L161-L172<CODESPLIT>ServerCommon.common_update_sys<CODESPLIT>If a record matching the instance already exists in the database update both the column and venue column attributes else create a new record .<CODESPLIT>def common_update_sys ( self ) : try : sudo ( 'apt-get update -y --fix-missing' ) except Exception as e : print ( e ) print ( green ( 'System package is up to date.' ) ) print ( )

1<CODESPLIT>https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/utils/__init__.py#L515-L576<CODESPLIT>format_supsub<CODESPLIT>Mainly used for Analysis Service s unit . Transform the text adding sub and super html scripts : For super - scripts use ^ char For sub - scripts use _ char The expression cm^2 will be translated to cm² and the expression b_ ( n - 1 ) will be translated to b n - 1 . The expression n_ ( fibras ) / cm^3 will be translated as n fibras / cm³ : param text : text to be formatted<CODESPLIT>def format_supsub ( text ) : out = [ ] subsup = [ ] clauses = [ ] insubsup = True for c in str ( text ) : if c == '(' : if insubsup is False : out . append ( c ) clauses . append ( ')' ) else : clauses . append ( '' ) elif c == ')' : if len ( clauses ) > 0 : out . append ( clauses . pop ( ) ) if len ( subsup ) > 0 : out . append ( subsup . pop ( ) ) elif c == '^' : subsup . append ( '</sup>' ) out . append ( '<sup>' ) insubsup = True continue elif c == '_' : subsup . append ( '</sub>' ) out . append ( '<sub>' ) insubsup = True continue elif c == ' ' : if insubsup is True : out . append ( subsup . pop ( ) ) else : out . append ( c ) elif c in [ '+' , '-' ] : if len ( clauses ) == 0 and len ( subsup ) > 0 : out . append ( subsup . pop ( ) ) out . append ( c ) else : out . append ( c ) insubsup = False while True : if len ( subsup ) == 0 : break out . append ( subsup . pop ( ) ) return '' . join ( out )

1<CODESPLIT>https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/commonlib/oqvalidation.py#L392-L407<CODESPLIT>OqParam.min_iml<CODESPLIT>: returns : a numpy array of intensities one per IMT<CODESPLIT>def min_iml ( self ) : mini = self . minimum_intensity if mini : for imt in self . imtls : try : mini [ imt ] = calc . filters . getdefault ( mini , imt ) except KeyError : raise ValueError ( 'The parameter `minimum_intensity` in the job.ini ' 'file is missing the IMT %r' % imt ) if 'default' in mini : del mini [ 'default' ] return F32 ( [ mini . get ( imt , 0 ) for imt in self . imtls ] )

1<CODESPLIT>https://github.com/juiceinc/recipe/blob/2e60c2242aeaea3029a2274b31bc3a937761e568/recipe/core.py#L276-L305<CODESPLIT>Recipe.filters<CODESPLIT>Add a list of Filter ingredients to the query . These can either be Filter objects or strings representing filters on the service s shelf . . filters () are additive calling . filters () more than once will add to the list of filters being used by the recipe .<CODESPLIT>def filters ( self , * filters ) : def filter_constructor ( f , shelf = None ) : if isinstance ( f , BinaryExpression ) : return Filter ( f ) else : return f for f in filters : self . _cauldron . use ( self . _shelf . find ( f , ( Filter , Having ) , constructor = filter_constructor ) ) self . dirty = True return self

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/cloud/clouds/qingcloud.py#L131-L201<CODESPLIT>query<CODESPLIT>Make a web call to QingCloud IaaS API .<CODESPLIT>def query ( params = None ) : path = 'https://api.qingcloud.com/iaas/' access_key_id = config . get_cloud_config_value ( 'access_key_id' , get_configured_provider ( ) , __opts__ , search_global = False ) access_key_secret = config . get_cloud_config_value ( 'secret_access_key' , get_configured_provider ( ) , __opts__ , search_global = False ) # public interface parameters real_parameters = { 'access_key_id' : access_key_id , 'signature_version' : DEFAULT_QINGCLOUD_SIGNATURE_VERSION , 'time_stamp' : time . strftime ( '%Y-%m-%dT%H:%M:%SZ' , time . gmtime ( ) ) , 'version' : DEFAULT_QINGCLOUD_API_VERSION , } # include action or function parameters if params : for key , value in params . items ( ) : if isinstance ( value , list ) : for i in range ( 1 , len ( value ) + 1 ) : if isinstance ( value [ i - 1 ] , dict ) : for sk , sv in value [ i - 1 ] . items ( ) : if isinstance ( sv , dict ) or isinstance ( sv , list ) : sv = salt . utils . json . dumps ( sv , separators = ( ',' , ':' ) ) real_parameters [ '{0}.{1}.{2}' . format ( key , i , sk ) ] = sv else : real_parameters [ '{0}.{1}' . format ( key , i ) ] = value [ i - 1 ] else : real_parameters [ key ] = value # Calculate the string for Signature signature = _compute_signature ( real_parameters , access_key_secret , 'GET' , '/iaas/' ) real_parameters [ 'signature' ] = signature # print('parameters:') # pprint.pprint(real_parameters) request = requests . get ( path , params = real_parameters , verify = False ) # print('url:') # print(request.url) if request . status_code != 200 : raise SaltCloudSystemExit ( 'An error occurred while querying QingCloud. HTTP Code: {0}  ' 'Error: \'{1}\'' . format ( request . status_code , request . text ) ) log . debug ( request . url ) content = request . text result = salt . utils . json . loads ( content ) # print('response:') # pprint.pprint(result) if result [ 'ret_code' ] != 0 : raise SaltCloudSystemExit ( pprint . pformat ( result . get ( 'message' , { } ) ) ) return result

1<CODESPLIT>https://github.com/monarch-initiative/dipper/blob/24cc80db355bbe15776edc5c7b41e0886959ba41/dipper/sources/SGD.py#L83-L203<CODESPLIT>SGD.make_association<CODESPLIT>contstruct the association : param record : : return : modeled association of genotype to mammalian??? phenotype<CODESPLIT>def make_association ( self , record ) : # prep record # remove description and mapp Experiment Type to apo term experiment_type = record [ 'Experiment Type' ] . split ( '(' ) [ 0 ] experiment_type = experiment_type . split ( ',' ) record [ 'experiment_type' ] = list ( ) for exp_type in experiment_type : exp_type = exp_type . lstrip ( ) . rstrip ( ) record [ 'experiment_type' ] . append ( { 'id' : self . apo_term_id [ exp_type ] , 'term' : exp_type , } ) sgd_phenotype = record [ 'Phenotype' ] pheno_obj = { 'entity' : { 'term' : None , 'apo_id' : None } , 'quality' : { 'term' : None , 'apo_id' : None } , 'has_quality' : False # descriptive and don't bother looking for a quality } phenotype = record [ 'Phenotype' ] if ':' in phenotype : pheno_obj [ 'has_quality' ] = True ent_qual = sgd_phenotype . split ( ': ' ) entity = ent_qual [ 0 ] quality = ent_qual [ 1 ] pheno_obj [ 'entity' ] [ 'term' ] = entity pheno_obj [ 'entity' ] [ 'apo_id' ] = self . apo_term_id [ entity ] pheno_obj [ 'quality' ] [ 'term' ] = quality pheno_obj [ 'quality' ] [ 'apo_id' ] = self . apo_term_id [ quality ] else : pheno_obj [ 'entity' ] [ 'term' ] = phenotype pheno_obj [ 'entity' ] [ 'apo_id' ] = self . apo_term_id [ phenotype ] record [ 'pheno_obj' ] = pheno_obj # begin modeling model = Model ( self . graph ) # define the triple gene = 'SGD:{}' . format ( record [ 'SGDID' ] ) relation = self . globaltt [ 'has phenotype' ] if record [ 'pheno_obj' ] [ 'has_quality' ] : pheno_label = '{0}:{1}' . format ( record [ 'pheno_obj' ] [ 'entity' ] [ 'term' ] , record [ 'pheno_obj' ] [ 'quality' ] [ 'term' ] ) pheno_id = 'MONARCH:{0}{1}' . format ( record [ 'pheno_obj' ] [ 'entity' ] [ 'apo_id' ] . replace ( ':' , '_' ) , record [ 'pheno_obj' ] [ 'quality' ] [ 'apo_id' ] . replace ( ':' , '_' ) ) g2p_assoc = Assoc ( self . graph , self . name , sub = gene , obj = pheno_id , pred = relation ) else : pheno_label = record [ 'pheno_obj' ] [ 'entity' ] [ 'term' ] pheno_id = record [ 'pheno_obj' ] [ 'entity' ] [ 'apo_id' ] g2p_assoc = Assoc ( self . graph , self . name , sub = gene , obj = pheno_id , pred = relation ) assoc_id = g2p_assoc . make_association_id ( 'yeastgenome.org' , gene , relation , pheno_id ) g2p_assoc . set_association_id ( assoc_id = assoc_id ) # add to graph to mint assoc id g2p_assoc . add_association_to_graph ( ) model . addLabel ( subject_id = gene , label = record [ 'Gene Name' ] ) # add the association triple model . addTriple ( subject_id = gene , predicate_id = relation , obj = pheno_id ) model . addTriple ( subject_id = pheno_id , predicate_id = self . globaltt [ 'subclass_of' ] , obj = self . globaltt [ 'Phenotype' ] ) # label nodes # pheno label model . addLabel ( subject_id = pheno_id , label = pheno_label ) g2p_assoc . description = self . _make_description ( record ) # add the references references = record [ 'Reference' ] references = references . replace ( ' ' , '' ) references = references . split ( '|' ) #  created Ref prefix in curie map to route to proper reference URL in SGD if len ( references ) > 0 : # make first ref in list the source g2p_assoc . add_source ( identifier = references [ 0 ] ) ref_model = Reference ( self . graph , references [ 0 ] , self . globaltt [ 'publication' ] ) ref_model . addRefToGraph ( ) if len ( references ) > 1 : # create equivalent source for any other refs in list for ref in references [ 1 : ] : model . addSameIndividual ( sub = references [ 0 ] , obj = ref ) # add experiment type as evidence for exp_type in record [ 'experiment_type' ] : g2p_assoc . add_evidence ( exp_type [ 'id' ] ) model . addLabel ( subject_id = exp_type [ 'id' ] , label = exp_type [ 'term' ] ) try : g2p_assoc . add_association_to_graph ( ) except Exception as e : print ( e ) return

0<CODESPLIT>https://github.com/onelogin/python3-saml/blob/064b7275fba1e5f39a9116ba1cdcc5d01fc34daa/src/onelogin/saml2/xml_utils.py#L107-L134<CODESPLIT>OneLogin_Saml2_XML.cleanup_namespaces<CODESPLIT>Extracts nodes that match the query from the Element<CODESPLIT>def cleanup_namespaces ( tree_or_element , top_nsmap = None , keep_ns_prefixes = None ) : all_prefixes_to_keep = [ OneLogin_Saml2_Constants . NS_PREFIX_XS ] if keep_ns_prefixes : all_prefixes_to_keep = list ( set ( all_prefixes_to_keep . extend ( keep_ns_prefixes ) ) ) return etree . cleanup_namespaces ( tree_or_element , keep_ns_prefixes = all_prefixes_to_keep )

0<CODESPLIT>https://github.com/audreyr/cookiecutter/blob/3bc7b987e4ae9dcee996ae0b00375c1325b8d866/cookiecutter/generate.py#L75-L114<CODESPLIT>generate_file<CODESPLIT>Generate the context for a Cookiecutter project template .<CODESPLIT>def generate_file ( project_dir , infile , context , env ) : logger . debug ( 'Processing file {}' . format ( infile ) ) # Render the path to the output file (not including the root project dir) outfile_tmpl = env . from_string ( infile ) outfile = os . path . join ( project_dir , outfile_tmpl . render ( * * context ) ) file_name_is_empty = os . path . isdir ( outfile ) if file_name_is_empty : logger . debug ( 'The resulting file name is empty: {0}' . format ( outfile ) ) return logger . debug ( 'Created file at {0}' . format ( outfile ) ) # Just copy over binary files. Don't render. logger . debug ( "Check {} to see if it's a binary" . format ( infile ) ) if is_binary ( infile ) : logger . debug ( 'Copying binary {} to {} without rendering' '' . format ( infile , outfile ) ) shutil . copyfile ( infile , outfile ) else : # Force fwd slashes on Windows for get_template # This is a by-design Jinja issue infile_fwd_slashes = infile . replace ( os . path . sep , '/' ) # Render the file try : tmpl = env . get_template ( infile_fwd_slashes ) except TemplateSyntaxError as exception : # Disable translated so that printed exception contains verbose # information about syntax error location exception . translated = False raise rendered_file = tmpl . render ( * * context ) logger . debug ( 'Writing contents to file {}' . format ( outfile ) ) with io . open ( outfile , 'w' , encoding = 'utf-8' ) as fh : fh . write ( rendered_file ) # Apply file permissions to output file shutil . copymode ( infile , outfile )

1<CODESPLIT>https://github.com/facelessuser/backrefs/blob/3b3d60f5d57b02044f880aa29c9c5add0e31a34f/backrefs/uniprops/__init__.py#L98-L109<CODESPLIT>get_grapheme_cluster_break_property<CODESPLIT>Get GRAPHEME CLUSTER BREAK property .<CODESPLIT>def get_grapheme_cluster_break_property ( value , is_bytes = False ) : obj = unidata . ascii_grapheme_cluster_break if is_bytes else unidata . unicode_grapheme_cluster_break if value . startswith ( '^' ) : negated = value [ 1 : ] value = '^' + unidata . unicode_alias [ 'graphemeclusterbreak' ] . get ( negated , negated ) else : value = unidata . unicode_alias [ 'graphemeclusterbreak' ] . get ( value , value ) return obj [ value ]

0<CODESPLIT>https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_numpy.py#L229-L259<CODESPLIT>spaced_indexes<CODESPLIT>intersect2d<CODESPLIT>def spaced_indexes ( len_ , n , trunc = False ) : if n is None : return np . arange ( len_ ) all_indexes = np . arange ( len_ ) if trunc : n = min ( len_ , n ) if n == 0 : return np . empty ( 0 ) stride = len_ // n try : indexes = all_indexes [ 0 : - 1 : stride ] except ValueError : raise ValueError ( 'cannot slice list of len_=%r into n=%r parts' % ( len_ , n ) ) return indexes

0<CODESPLIT>https://github.com/grundic/yagocd/blob/4c75336ae6f107c8723d37b15e52169151822127/yagocd/resources/job.py#L218-L232<CODESPLIT>JobInstance.properties<CODESPLIT>Property for accessing artifact manager of the current job .<CODESPLIT>def properties ( self ) : return PropertyManager ( session = self . _session , pipeline_name = self . pipeline_name , pipeline_counter = self . pipeline_counter , stage_name = self . stage_name , stage_counter = self . stage_counter , job_name = self . data . name )

0<CODESPLIT>https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/calculators/views.py#L714-L727<CODESPLIT>view_global_hmaps<CODESPLIT>Display global probabilities averaged on all sites and all GMPEs<CODESPLIT>def view_global_hmaps ( token , dstore ) : oq = dstore [ 'oqparam' ] dt = numpy . dtype ( [ ( '%s-%s' % ( imt , poe ) , F32 ) for imt in oq . imtls for poe in oq . poes ] ) array = dstore [ 'hmaps/mean' ] . value . view ( dt ) [ : , 0 ] res = numpy . zeros ( 1 , array . dtype ) for name in array . dtype . names : res [ name ] = array [ name ] . mean ( ) return rst_table ( res )

1<CODESPLIT>https://github.com/JdeRobot/base/blob/303b18992785b2fe802212f2d758a60873007f1f/src/drivers/MAVLinkServer/MAVProxy/modules/mavproxy_graph.py#L23-L45<CODESPLIT>GraphModule.cmd_graph<CODESPLIT>graph command<CODESPLIT>def cmd_graph ( self , args ) : if len ( args ) == 0 : # list current graphs for i in range ( len ( self . graphs ) ) : print ( "Graph %u: %s" % ( i , self . graphs [ i ] . fields ) ) return elif args [ 0 ] == "help" : print ( "graph <timespan|tickresolution|expression>" ) elif args [ 0 ] == "timespan" : if len ( args ) == 1 : print ( "timespan: %.1f" % self . timespan ) return self . timespan = float ( args [ 1 ] ) elif args [ 0 ] == "tickresolution" : if len ( args ) == 1 : print ( "tickresolution: %.1f" % self . tickresolution ) return self . tickresolution = float ( args [ 1 ] ) else : # start a new graph self . graphs . append ( Graph ( self , args [ : ] ) )

0<CODESPLIT>https://github.com/pallets/werkzeug/blob/a220671d66755a94630a212378754bb432811158/src/werkzeug/datastructures.py#L2310-L2323<CODESPLIT>Range.make_content_range<CODESPLIT>If the range is for bytes the length is not None and there is exactly one range and it is satisfiable it returns a ( start stop ) tuple otherwise None .<CODESPLIT>def make_content_range ( self , length ) : rng = self . range_for_length ( length ) if rng is not None : return ContentRange ( self . units , rng [ 0 ] , rng [ 1 ] , length )

1<CODESPLIT>https://github.com/aiogram/aiogram/blob/2af930149ce2482547721e2c8755c10307295e48/examples/middleware_and_antiflood.py#L78-L107<CODESPLIT>ThrottlingMiddleware.message_throttled<CODESPLIT>Notify user only on first exceed and notify about unlocking only on last exceed<CODESPLIT>async def message_throttled ( self , message : types . Message , throttled : Throttled ) : handler = current_handler . get ( ) dispatcher = Dispatcher . get_current ( ) if handler : key = getattr ( handler , 'throttling_key' , f"{self.prefix}_{handler.__name__}" ) else : key = f"{self.prefix}_message" # Calculate how many time is left till the block ends delta = throttled . rate - throttled . delta # Prevent flooding if throttled . exceeded_count <= 2 : await message . reply ( 'Too many requests! ' ) # Sleep. await asyncio . sleep ( delta ) # Check lock status thr = await dispatcher . check_key ( key ) # If current message is not last with current key - do not send message if thr . exceeded_count == throttled . exceeded_count : await message . reply ( 'Unlocked.' )

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/win_service.py#L1103-L1139<CODESPLIT>enable<CODESPLIT>Enable the named service to start at boot<CODESPLIT>def enable ( name , start_type = 'auto' , start_delayed = False , * * kwargs ) : modify ( name , start_type = start_type , start_delayed = start_delayed ) svcstat = info ( name ) if start_type . lower ( ) == 'auto' : return svcstat [ 'StartType' ] . lower ( ) == start_type . lower ( ) and svcstat [ 'StartTypeDelayed' ] == start_delayed else : return svcstat [ 'StartType' ] . lower ( ) == start_type . lower ( )

0<CODESPLIT>https://github.com/astooke/gtimer/blob/2146dab459e5d959feb291821733d3d3ba7c523c/gtimer/public/timer.py#L234-L255<CODESPLIT>stop<CODESPLIT>Pause the timer preventing subsequent time from accumulating in the total . Renders the timer inactive disabling other timing commands .<CODESPLIT>def stop ( name = None , backdate = None , unique = None , keep_subdivisions = None , quick_print = None , un = None , ks = None , qp = None ) : t = timer ( ) if f . t . stopped : raise StoppedError ( "Timer already stopped." ) if backdate is None : t_stop = t else : if f . t is f . root : raise BackdateError ( "Cannot backdate stop of root timer." ) if not isinstance ( backdate , float ) : raise TypeError ( "Backdate must be type float." ) if backdate > t : raise BackdateError ( "Cannot backdate to future time." ) if backdate < f . t . last_t : raise BackdateError ( "Cannot backdate to time earlier than last stamp." ) t_stop = backdate unique = SET [ 'UN' ] if ( unique is None and un is None ) else bool ( unique or un ) # bool(None) becomes False keep_subdivisions = SET [ 'KS' ] if ( keep_subdivisions is None and ks is None ) else bool ( keep_subdivisions or ks ) quick_print = SET [ 'QP' ] if ( quick_print is None and qp is None ) else bool ( quick_print or qp ) if name is not None : if f . t . paused : raise PausedError ( "Cannot stamp paused timer." ) elapsed = t_stop - f . t . last_t _stamp ( name , elapsed , unique , keep_subdivisions , quick_print ) else : times_priv . assign_subdivisions ( UNASGN , keep_subdivisions ) for s in f . t . rgstr_stamps : if s not in f . s . cum : f . s . cum [ s ] = 0. f . s . order . append ( s ) if not f . t . paused : f . t . tmp_total += t_stop - f . t . start_t f . t . tmp_total -= f . t . self_cut f . t . self_cut += timer ( ) - t # AFTER subtraction from tmp_total, before dump times_priv . dump_times ( ) f . t . stopped = True if quick_print : print ( "({}) Total: {:.4f}" . format ( f . t . name , f . r . total ) ) return t

0<CODESPLIT>https://github.com/wummel/patool/blob/d7e64d9fd60faaa4b3f824bd97c43ce59b185c40/patoolib/programs/ar.py#L36-L43<CODESPLIT>extract_cab<CODESPLIT>Create a AR archive .<CODESPLIT>def extract_cab ( archive , compression , cmd , verbosity , interactive , outdir ) : cmdlist = [ cmd , '-d' , outdir ] if verbosity > 0 : cmdlist . append ( '-v' ) cmdlist . append ( archive ) return cmdlist

0<CODESPLIT>https://github.com/abilian/abilian-core/blob/0a71275bf108c3d51e13ca9e093c0249235351e3/abilian/core/util.py#L178-L188<CODESPLIT>luhn<CODESPLIT>Slugify an Unicode string to make it URL friendly .<CODESPLIT>def luhn ( n ) : r = [ int ( ch ) for ch in str ( n ) ] [ : : - 1 ] return ( sum ( r [ 0 : : 2 ] ) + sum ( sum ( divmod ( d * 2 , 10 ) ) for d in r [ 1 : : 2 ] ) ) % 10 == 0

0<CODESPLIT>https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/tree.py#L978-L986<CODESPLIT>Tree.intersection<CODESPLIT>Return a generator that yields the nodes of the tree in postorder . If skip_seed = True then the root node is not included .<CODESPLIT>def intersection ( self , other ) : taxa1 = self . labels taxa2 = other . labels return taxa1 & taxa2

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/states/boto_ec2.py#L79-L133<CODESPLIT>key_present<CODESPLIT>Ensure key pair is present .<CODESPLIT>def key_present ( name , save_private = None , upload_public = None , region = None , key = None , keyid = None , profile = None ) : ret = { 'name' : name , 'result' : True , 'comment' : '' , 'changes' : { } } exists = __salt__ [ 'boto_ec2.get_key' ] ( name , region , key , keyid , profile ) log . debug ( 'exists is %s' , exists ) if upload_public is not None and 'salt://' in upload_public : try : upload_public = __salt__ [ 'cp.get_file_str' ] ( upload_public ) except IOError as e : log . debug ( e ) ret [ 'comment' ] = 'File {0} not found.' . format ( upload_public ) ret [ 'result' ] = False return ret if not exists : if __opts__ [ 'test' ] : ret [ 'comment' ] = 'The key {0} is set to be created.' . format ( name ) ret [ 'result' ] = None return ret if save_private and not upload_public : created = __salt__ [ 'boto_ec2.create_key' ] ( name , save_private , region , key , keyid , profile ) if created : ret [ 'result' ] = True ret [ 'comment' ] = 'The key {0} is created.' . format ( name ) ret [ 'changes' ] [ 'new' ] = created else : ret [ 'result' ] = False ret [ 'comment' ] = 'Could not create key {0} ' . format ( name ) elif not save_private and upload_public : imported = __salt__ [ 'boto_ec2.import_key' ] ( name , upload_public , region , key , keyid , profile ) if imported : ret [ 'result' ] = True ret [ 'comment' ] = 'The key {0} is created.' . format ( name ) ret [ 'changes' ] [ 'old' ] = None ret [ 'changes' ] [ 'new' ] = imported else : ret [ 'result' ] = False ret [ 'comment' ] = 'Could not create key {0} ' . format ( name ) else : ret [ 'result' ] = False ret [ 'comment' ] = 'You can either upload or download a private key ' else : ret [ 'result' ] = True ret [ 'comment' ] = 'The key name {0} already exists' . format ( name ) return ret

0<CODESPLIT>https://github.com/danielperna84/pyhomematic/blob/8b91f3e84c83f05d289c740d507293a0d6759d8e/pyhomematic/_hm.py#L457-L465<CODESPLIT>RPCFunctions.addDeviceNames<CODESPLIT>Call method on server side<CODESPLIT>def addDeviceNames ( self , remote ) : LOG . debug ( "RPCFunctions.addDeviceNames" ) # First try to get names from metadata when nur credentials are set if self . remotes [ remote ] [ 'resolvenames' ] == 'metadata' : for address in self . devices [ remote ] : try : name = self . devices [ remote ] [ address ] . _proxy . getMetadata ( address , 'NAME' ) self . devices [ remote ] [ address ] . NAME = name for address , device in self . devices [ remote ] [ address ] . CHANNELS . items ( ) : device . NAME = name self . devices_all [ remote ] [ device . ADDRESS ] . NAME = name except Exception as err : LOG . debug ( "RPCFunctions.addDeviceNames: Unable to get name for %s from metadata." % str ( address ) ) # Then try to get names via JSON-RPC elif ( self . remotes [ remote ] [ 'resolvenames' ] == 'json' and self . remotes [ remote ] [ 'username' ] and self . remotes [ remote ] [ 'password' ] ) : LOG . debug ( "RPCFunctions.addDeviceNames: Getting names via JSON-RPC" ) try : session = False params = { "username" : self . remotes [ remote ] [ 'username' ] , "password" : self . remotes [ remote ] [ 'password' ] } response = self . jsonRpcPost ( self . remotes [ remote ] [ 'ip' ] , self . remotes [ remote ] . get ( 'jsonport' , DEFAULT_JSONPORT ) , "Session.login" , params ) if response [ 'error' ] is None and response [ 'result' ] : session = response [ 'result' ] if not session : LOG . warning ( "RPCFunctions.addDeviceNames: Unable to open session." ) return params = { "_session_id_" : session } response = self . jsonRpcPost ( self . remotes [ remote ] [ 'ip' ] , self . remotes [ remote ] . get ( 'jsonport' , DEFAULT_JSONPORT ) , "Interface.listInterfaces" , params ) interface = False if response [ 'error' ] is None and response [ 'result' ] : for i in response [ 'result' ] : if i [ 'port' ] in [ self . remotes [ remote ] [ 'port' ] , self . remotes [ remote ] [ 'port' ] + 30000 ] : interface = i [ 'name' ] break LOG . debug ( "RPCFunctions.addDeviceNames: Got interface: %s" % interface ) if not interface : params = { "_session_id_" : session } response = self . jsonRpcPost ( self . remotes [ remote ] [ 'ip' ] , self . remotes [ remote ] . get ( 'jsonport' , DEFAULT_JSONPORT ) , "Session.logout" , params ) return params = { "_session_id_" : session } response = self . jsonRpcPost ( self . remotes [ remote ] [ 'ip' ] , self . remotes [ remote ] . get ( 'jsonport' , DEFAULT_JSONPORT ) , "Device.listAllDetail" , params ) if response [ 'error' ] is None and response [ 'result' ] : LOG . debug ( "RPCFunctions.addDeviceNames: Resolving devicenames" ) for i in response [ 'result' ] : try : if i . get ( 'address' ) in self . devices [ remote ] : self . devices [ remote ] [ i [ 'address' ] ] . NAME = i [ 'name' ] except Exception as err : LOG . warning ( "RPCFunctions.addDeviceNames: Exception: %s" % str ( err ) ) params = { "_session_id_" : session } response = self . jsonRpcPost ( self . remotes [ remote ] [ 'ip' ] , self . remotes [ remote ] . get ( 'jsonport' , DEFAULT_JSONPORT ) , "Session.logout" , params ) except Exception as err : params = { "_session_id_" : session } response = self . jsonRpcPost ( self . remotes [ remote ] [ 'ip' ] , self . remotes [ remote ] . get ( 'jsonport' , DEFAULT_JSONPORT ) , "Session.logout" , params ) LOG . warning ( "RPCFunctions.addDeviceNames: Exception: %s" % str ( err ) ) # Then try to get names from XML-API elif self . remotes [ remote ] [ 'resolvenames' ] == 'xml' : LOG . warning ( "Resolving names with the XML-API addon will be disabled in a future release. Please switch to json." ) try : response = urllib . request . urlopen ( "http://%s%s" % ( self . remotes [ remote ] [ 'ip' ] , XML_API_URL ) , timeout = 5 ) device_list = response . read ( ) . decode ( "ISO-8859-1" ) except Exception as err : LOG . warning ( "RPCFunctions.addDeviceNames: Could not access XML-API: %s" % ( str ( err ) , ) ) return device_list_tree = ET . ElementTree ( ET . fromstring ( device_list ) ) for device in device_list_tree . getroot ( ) : address = device . attrib [ 'address' ] name = device . attrib [ 'name' ] if address in self . devices [ remote ] : self . devices [ remote ] [ address ] . NAME = name for address , device in self . devices [ remote ] [ address ] . CHANNELS . items ( ) : device . NAME = name self . devices_all [ remote ] [ device . ADDRESS ] . NAME = name

1<CODESPLIT>https://github.com/pyca/pynacl/blob/0df0c2c7693fa5d316846111ce510702756f5feb/src/nacl/bindings/utils.py#L97-L120<CODESPLIT>sodium_increment<CODESPLIT>Increment the value of a byte - sequence interpreted as the little - endian representation of a unsigned big integer .<CODESPLIT>def sodium_increment ( inp ) : ensure ( isinstance ( inp , bytes ) , raising = exc . TypeError ) ln = len ( inp ) buf = ffi . new ( "unsigned char []" , ln ) ffi . memmove ( buf , inp , ln ) lib . sodium_increment ( buf , ln ) return ffi . buffer ( buf , ln ) [ : ]

1<CODESPLIT>https://github.com/NetEaseGame/ATX/blob/f4415c57b45cb0730e08899cbc92a2af1c047ffb/scripts/adb_old.py#L202-L211<CODESPLIT>Adb.remove<CODESPLIT>Remove remote file Return : bool : true or false<CODESPLIT>def remove ( self , path ) : p = self . cmd ( 'shell' , 'rm' , path ) stdout , stderr = p . communicate ( ) if stdout or stderr : return False else : return True

1<CODESPLIT>https://github.com/jonathf/chaospy/blob/25ecfa7bf5608dc10c0b31d142ded0e3755f5d74/chaospy/distributions/sampler/generator.py#L76-L139<CODESPLIT>generate_samples<CODESPLIT>Sample generator .<CODESPLIT>def generate_samples ( order , domain = 1 , rule = "R" , antithetic = None ) : logger = logging . getLogger ( __name__ ) logger . debug ( "generating random samples using rule %s" , rule ) rule = rule . upper ( ) if isinstance ( domain , int ) : dim = domain trans = lambda x_data : x_data elif isinstance ( domain , ( tuple , list , numpy . ndarray ) ) : domain = numpy . asfarray ( domain ) if len ( domain . shape ) < 2 : dim = 1 else : dim = len ( domain [ 0 ] ) trans = lambda x_data : ( ( domain [ 1 ] - domain [ 0 ] ) * x_data . T + domain [ 0 ] ) . T else : dist = domain dim = len ( dist ) trans = dist . inv if antithetic is not None : from . antithetic import create_antithetic_variates antithetic = numpy . array ( antithetic , dtype = bool ) . flatten ( ) if antithetic . size == 1 and dim > 1 : antithetic = numpy . repeat ( antithetic , dim ) size = numpy . sum ( 1 * numpy . array ( antithetic ) ) order_saved = order order = int ( numpy . log ( order - dim ) ) order = order if order > 1 else 1 while order ** dim < order_saved : order += 1 trans_ = trans trans = lambda x_data : trans_ ( create_antithetic_variates ( x_data , antithetic ) [ : , : order_saved ] ) assert rule in SAMPLERS , "rule not recognised" sampler = SAMPLERS [ rule ] x_data = trans ( sampler ( order = order , dim = dim ) ) logger . debug ( "order: %d, dim: %d -> shape: %s" , order , dim , x_data . shape ) return x_data

0<CODESPLIT>https://github.com/nerdvegas/rez/blob/1d3b846d53b5b5404edfe8ddb9083f9ceec8c5e7/src/rez/vendor/amqp/channel.py#L2049-L2123<CODESPLIT>Channel.basic_qos<CODESPLIT>Publish a message<CODESPLIT>def basic_qos ( self , prefetch_size , prefetch_count , a_global ) : args = AMQPWriter ( ) args . write_long ( prefetch_size ) args . write_short ( prefetch_count ) args . write_bit ( a_global ) self . _send_method ( ( 60 , 10 ) , args ) return self . wait ( allowed_methods = [ ( 60 , 11 ) , # Channel.basic_qos_ok ] )

1<CODESPLIT>https://github.com/log2timeline/dfvfs/blob/2b3ccd115f9901d89f383397d4a1376a873c83c4/dfvfs/vfs/ntfs_file_entry.py#L65-L68<CODESPLIT>FileNameNTFSAttribute.entry_modification_time<CODESPLIT>dfdatetime . Filetime : entry modification time or None if not set .<CODESPLIT>def entry_modification_time ( self ) : timestamp = self . _fsntfs_attribute . get_entry_modification_time_as_integer ( ) return dfdatetime_filetime . Filetime ( timestamp = timestamp )

0<CODESPLIT>https://github.com/yueyoum/social-oauth/blob/80600ea737355b20931c8a0b5223f5b68175d930/example/_bottle.py#L624-L634<CODESPLIT>Route.all_plugins<CODESPLIT>Reset all routes ( force plugins to be re - applied ) and clear all caches . If an ID or route object is given only that specific route is affected .<CODESPLIT>def all_plugins ( self ) : unique = set ( ) for p in reversed ( self . app . plugins + self . plugins ) : if True in self . skiplist : break name = getattr ( p , 'name' , False ) if name and ( name in self . skiplist or name in unique ) : continue if p in self . skiplist or type ( p ) in self . skiplist : continue if name : unique . add ( name ) yield p

1<CODESPLIT>https://github.com/kamikaze/webdav/blob/6facff7224023d3e28c8e1592f3c58401c91a0e6/src/webdav/client.py#L284-L302<CODESPLIT>Client.mkdir<CODESPLIT>Makes new directory on WebDAV server . More information you can find by link http : // webdav . org / specs / rfc4918 . html#METHOD_MKCOL<CODESPLIT>def mkdir ( self , remote_path ) : directory_urn = Urn ( remote_path , directory = True ) try : response = self . execute_request ( action = 'mkdir' , path = directory_urn . quote ( ) ) return response . status_code in ( 200 , 201 ) except ResponseErrorCode as e : if e . code == 405 : return True raise

1<CODESPLIT>https://github.com/astrocatalogs/astrocats/blob/11abc3131c6366ecd23964369e55ff264add7805/astrocats/catalog/entry.py#L712-L723<CODESPLIT>Entry.merge_dupes<CODESPLIT>Merge two entries that correspond to the same entry .<CODESPLIT>def merge_dupes ( self ) : for dupe in self . dupe_of : if dupe in self . catalog . entries : if self . catalog . entries [ dupe ] . _stub : # merge = False to avoid infinite recursion self . catalog . load_entry_from_name ( dupe , delete = True , merge = False ) self . catalog . copy_entry_to_entry ( self . catalog . entries [ dupe ] , self ) del self . catalog . entries [ dupe ] self . dupe_of = [ ]

1<CODESPLIT>https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/_ext/variational/estimators/moments.py#L181-L224<CODESPLIT>_copy_convert<CODESPLIT>r Makes a copy or converts the data type if needed<CODESPLIT>def _copy_convert ( X , const = None , remove_mean = False , copy = True ) : # determine type dtype = np . float64 # default: convert to float64 in order to avoid cancellation errors if X . dtype . kind == 'b' and X . shape [ 0 ] < 2 ** 23 and not remove_mean : dtype = np . float32 # convert to float32 if we can represent all numbers # copy/convert if needed if X . dtype not in ( np . float64 , dtype ) : # leave as float64 (conversion is expensive), otherwise convert to dtype X = X . astype ( dtype , order = 'C' ) if const is not None : const = const . astype ( dtype , order = 'C' ) elif copy : X = X . copy ( order = 'C' ) if const is not None : const = const . copy ( order = 'C' ) return X , const

0<CODESPLIT>https://github.com/galaxyproject/pulsar/blob/9ab6683802884324652da0a9f0808c7eb59d3ab4/pulsar/client/manager.py#L83-L90<CODESPLIT>CliInterface.get_plugins<CODESPLIT>Build a client given specific destination parameters and job_id .<CODESPLIT>def get_plugins ( self , shell_params , job_params ) : shell = self . get_shell_plugin ( shell_params ) job_interface = self . get_job_interface ( job_params ) return shell , job_interface

0<CODESPLIT>https://github.com/Workiva/contour/blob/599e05c7ab6020b1ccc27e3f64f625abaec33ff2/contour/contour.py#L151-L189<CODESPLIT>_load_yaml_config<CODESPLIT>Traverse the directory tree identified by start until a directory already in checked is encountered or the path of countour . yaml is found .<CODESPLIT>def _load_yaml_config ( path = None ) : countour_yaml_path = path or find_contour_yaml ( ) if countour_yaml_path is None : logging . debug ( "countour.yaml not found." ) return None with open ( countour_yaml_path ) as yaml_file : return yaml_file . read ( )

0<CODESPLIT>https://github.com/gccxml/pygccxml/blob/2b1efbb9e37ceb2ae925c7f3ce1570f476db9e1e/pygccxml/declarations/traits_impl_details.py#L45-L88<CODESPLIT>impl_details.is_defined_in_xxx<CODESPLIT>implementation details<CODESPLIT>def is_defined_in_xxx ( xxx , cls ) : if not cls . parent : return False if not isinstance ( cls . parent , namespace . namespace_t ) : return False if xxx != cls . parent . name : return False xxx_ns = cls . parent if not xxx_ns . parent : return False if not isinstance ( xxx_ns . parent , namespace . namespace_t ) : return False if xxx_ns . parent . name != '::' : return False global_ns = xxx_ns . parent return None is global_ns . parent

1<CODESPLIT>https://github.com/KujiraProject/Flask-PAM/blob/d84f90ffd706c0f491af3539cd438e13771ada7e/flask_pam/auth.py#L142-L152<CODESPLIT>Auth.get_groups<CODESPLIT>Returns list of groups in which user is .<CODESPLIT>def get_groups ( self , username ) : groups = [ ] for group in grp . getgrall ( ) : if username in group . gr_mem : groups . append ( group . gr_name ) return groups

1<CODESPLIT>https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/internals/blocks.py#L1733-L1754<CODESPLIT>ExtensionBlock.take_nd<CODESPLIT>Take values according to indexer and return them as a block .<CODESPLIT>def take_nd ( self , indexer , axis = 0 , new_mgr_locs = None , fill_tuple = None ) : if fill_tuple is None : fill_value = None else : fill_value = fill_tuple [ 0 ] # axis doesn't matter; we are really a single-dim object # but are passed the axis depending on the calling routing # if its REALLY axis 0, then this will be a reindex and not a take new_values = self . values . take ( indexer , fill_value = fill_value , allow_fill = True ) if self . ndim == 1 and new_mgr_locs is None : new_mgr_locs = [ 0 ] else : if new_mgr_locs is None : new_mgr_locs = self . mgr_locs return self . make_block_same_class ( new_values , new_mgr_locs )

1<CODESPLIT>https://github.com/gwastro/pycbc/blob/7a64cdd104d263f1b6ea0b01e6841837d05a4cb3/pycbc/strain/recalibrate.py#L479-L520<CODESPLIT>PhysicalModel.map_to_adjust<CODESPLIT>Map an input dictionary of sampling parameters to the adjust_strain function by filtering the dictionary for the calibration parameters then calling adjust_strain .<CODESPLIT>def map_to_adjust ( self , strain , * * params ) : # calibration param names arg_names = [ 'delta_fs' , 'delta_fc' , 'delta_qinv' , 'kappa_c' , 'kappa_tst_re' , 'kappa_tst_im' , 'kappa_pu_re' , 'kappa_pu_im' ] # calibration param labels as they exist in config files arg_labels = [ '' . join ( [ 'calib_' , name ] ) for name in arg_names ] # default values for calibration params default_values = [ 0.0 , 0.0 , 0.0 , 1.0 , 1.0 , 0.0 , 1.0 , 0.0 ] # make list of calibration param values calib_args = [ ] for arg , val in zip ( arg_labels , default_values ) : if arg in params : calib_args . append ( params [ arg ] ) else : calib_args . append ( val ) # adjust the strain using calibration param values strain_adjusted = self . adjust_strain ( strain , delta_fs = calib_args [ 0 ] , delta_fc = calib_args [ 1 ] , delta_qinv = calib_args [ 2 ] , kappa_c = calib_args [ 3 ] , kappa_tst_re = calib_args [ 4 ] , kappa_tst_im = calib_args [ 5 ] , kappa_pu_re = calib_args [ 6 ] , kappa_pu_im = calib_args [ 7 ] ) return strain_adjusted

1<CODESPLIT>https://github.com/nephila/python-taiga/blob/5b471d6b8b59e5d410162a6f1c2f0d4188445a56/taiga/models/models.py#L1316-L1324<CODESPLIT>Project.add_issue_type<CODESPLIT>Add a Issue type to the project and returns a : class : IssueType object .<CODESPLIT>def add_issue_type ( self , name , * * attrs ) : return IssueTypes ( self . requester ) . create ( self . id , name , * * attrs )

0<CODESPLIT>https://github.com/twisted/txaws/blob/5c3317376cd47e536625027e38c3b37840175ce0/txaws/client/ssl.py#L86-L119<CODESPLIT>GeometricMeanAbsolutePercentageError._calculate<CODESPLIT>Retrieve a list of CAs at either the DEFAULT_CERTS_PATH or the env override TXAWS_CERTS_PATH .<CODESPLIT>def _calculate ( self , startingPercentage , endPercentage , startDate , endDate ) : # get the defined subset of error values errorValues = self . _get_error_values ( startingPercentage , endPercentage , startDate , endDate ) errorValues = filter ( lambda item : item is None , errorValues ) if errorValues [ 0 ] is None : return 1.0 share = 1.0 / float ( len ( errorValues ) ) product = 1.0 for errorValue in errorValues : # never multiply with zero! if 0 == errorValue : continue product *= errorValue ** share return product

1<CODESPLIT>https://github.com/pantsbuild/pants/blob/b72e650da0df685824ffdcc71988b8c282d0962d/src/python/pants/init/engine_initializer.py#L52-L99<CODESPLIT>_legacy_symbol_table<CODESPLIT>Construct a SymbolTable for the given BuildFileAliases .<CODESPLIT>def _legacy_symbol_table ( build_file_aliases ) : table = { alias : _make_target_adaptor ( TargetAdaptor , target_type ) for alias , target_type in build_file_aliases . target_types . items ( ) } for alias , factory in build_file_aliases . target_macro_factories . items ( ) : # TargetMacro.Factory with more than one target type is deprecated. # For default sources, this means that TargetMacro Factories with more than one target_type # will not parse sources through the engine, and will fall back to the legacy python sources # parsing. # Conveniently, multi-target_type TargetMacro.Factory, and legacy python source parsing, are # targeted to be removed in the same version of pants. if len ( factory . target_types ) == 1 : table [ alias ] = _make_target_adaptor ( TargetAdaptor , tuple ( factory . target_types ) [ 0 ] , ) # TODO: The alias replacement here is to avoid elevating "TargetAdaptors" into the public # API until after https://github.com/pantsbuild/pants/issues/3560 has been completed. # These should likely move onto Target subclasses as the engine gets deeper into beta # territory. table [ 'python_library' ] = _make_target_adaptor ( PythonTargetAdaptor , PythonLibrary ) table [ 'jvm_app' ] = _make_target_adaptor ( AppAdaptor , JvmApp ) table [ 'jvm_binary' ] = _make_target_adaptor ( JvmBinaryAdaptor , JvmBinary ) table [ 'python_app' ] = _make_target_adaptor ( AppAdaptor , PythonApp ) table [ 'python_tests' ] = _make_target_adaptor ( PythonTestsAdaptor , PythonTests ) table [ 'python_binary' ] = _make_target_adaptor ( PythonBinaryAdaptor , PythonBinary ) table [ 'remote_sources' ] = _make_target_adaptor ( RemoteSourcesAdaptor , RemoteSources ) table [ 'page' ] = _make_target_adaptor ( PageAdaptor , Page ) # Note that these don't call _make_target_adaptor because we don't have a handy reference to the # types being constructed. They don't have any default_sources behavior, so this should be ok, # but if we end up doing more things in _make_target_adaptor, we should make sure they're # applied here too. table [ 'pants_plugin' ] = PantsPluginAdaptor table [ 'contrib_plugin' ] = PantsPluginAdaptor return SymbolTable ( table )

0<CODESPLIT>https://github.com/vertexproject/synapse/blob/22e67c5a8f6d7caddbcf34b39ab1bd2d6c4a6e0b/synapse/daemon.py#L134-L173<CODESPLIT>Model.addBaseType<CODESPLIT>Bind and listen on the given host / port with possible SSL .<CODESPLIT>def addBaseType ( self , item ) : ctor = '.' . join ( [ item . __class__ . __module__ , item . __class__ . __qualname__ ] ) self . _modeldef [ 'ctors' ] . append ( ( ( item . name , ctor , dict ( item . opts ) , dict ( item . info ) ) ) ) self . types [ item . name ] = item

0<CODESPLIT>https://github.com/mixer/beam-interactive-python/blob/e035bc45515dea9315b77648a24b5ae8685aa5cf/beam_interactive/connection.py#L52-L67<CODESPLIT>Connection._push_packet<CODESPLIT>Reads data from the connection and adds it to _push_packet until the connection is closed or the task in cancelled .<CODESPLIT>def _push_packet ( self , packet ) : self . _read_queue . append ( ( decode ( packet ) , packet ) ) if self . _read_waiter is not None : w , self . _read_waiter = self . _read_waiter , None w . set_result ( None )

1<CODESPLIT>https://github.com/pawamoy/django-suit-dashboard/blob/c008f55f4168f7af62ba31e8b621df214bea1e96/src/suit_dashboard/views.py#L45-L66<CODESPLIT>DashboardView.get<CODESPLIT>Django view get function .<CODESPLIT>def get ( self , request , * args , * * kwargs ) : context = self . get_context_data ( * * kwargs ) context . update ( self . extra_context ) context [ 'crumbs' ] = self . get_crumbs ( ) context [ 'title' ] = self . title context [ 'suit' ] = 'suit' in settings . INSTALLED_APPS if context . get ( 'dashboard_grid' , None ) is None and self . grid : context [ 'dashboard_grid' ] = self . grid return self . render_to_response ( context )

1<CODESPLIT>https://github.com/quintusdias/glymur/blob/8b8fb091130fff00f1028dc82219e69e3f9baf6d/glymur/jp2box.py#L99-L108<CODESPLIT>Jp2kBox._dispatch_validation_error<CODESPLIT>Issue either a warning or an error depending on circumstance .<CODESPLIT>def _dispatch_validation_error ( self , msg , writing = False ) : if writing : raise IOError ( msg ) else : warnings . warn ( msg )

0<CODESPLIT>https://github.com/manahl/arctic/blob/57e110b6e182dbab00e7e214dc26f7d9ec47c120/arctic/scripts/utils.py#L10-L39<CODESPLIT>get_arctic_lib<CODESPLIT>Attempts to authenticate against the mongo instance .<CODESPLIT>def get_arctic_lib ( connection_string , * * kwargs ) : m = CONNECTION_STR . match ( connection_string ) if not m : raise ValueError ( "connection string incorrectly formed: %s" % connection_string ) library , host = m . group ( 1 ) , m . group ( 2 ) return _get_arctic ( host , * * kwargs ) [ library ]

0<CODESPLIT>https://github.com/ev3dev/ev3dev-lang-python/blob/afc98d35004b533dc161a01f7c966e78607d7c1e/ev3dev2/display.py#L294-L311<CODESPLIT>Display.line<CODESPLIT>Applies pending changes to the screen . Nothing will be drawn on the screen until this function is called .<CODESPLIT>def line ( self , clear_screen = True , x1 = 10 , y1 = 10 , x2 = 50 , y2 = 50 , line_color = 'black' , width = 1 ) : if clear_screen : self . clear ( ) return self . draw . line ( ( x1 , y1 , x2 , y2 ) , fill = line_color , width = width )

0<CODESPLIT>https://github.com/jlmadurga/listenclosely/blob/d6df9110c3ed6fd337e0236cccbe4d931bf217b0/listenclosely/app.py#L18-L30<CODESPLIT>FileSystem.checksums<CODESPLIT>Listen / Connect to message service loop to start receiving messages . Do not include in constructor in this way it can be included in tasks<CODESPLIT>def checksums ( self , path , hashtype = 'sha1' ) : with NamedTemporaryFile ( buffering = 0 ) as tempfile : self . _handler . checksums_out ( hashtype , posix_path ( path ) , tempfile . name ) yield from ( ( self . path ( f [ 1 ] . lstrip ( '.' ) ) , f [ 0 ] ) for f in ( l . decode ( 'utf8' ) . strip ( ) . split ( None , 1 ) for l in tempfile ) )

1<CODESPLIT>https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/editor/lsp/transport/main.py#L62-L76<CODESPLIT>logger_init<CODESPLIT>Initialize the logger for this thread .<CODESPLIT>def logger_init ( level ) : levellist = [ logging . ERROR , logging . WARNING , logging . INFO , logging . DEBUG ] handler = logging . StreamHandler ( ) fmt = ( '%(levelname) -10s %(asctime)s %(name) -30s %(funcName) ' '-35s %(lineno) -5d: %(message)s' ) handler . setFormatter ( logging . Formatter ( fmt ) ) logger = logging . root logger . addHandler ( handler ) logger . setLevel ( levellist [ level ] )

0<CODESPLIT>https://github.com/Duke-GCB/DukeDSClient/blob/117f68fb9bae82e4c81ea487ad5d61ac350f3726/ddsc/core/ddsapi.py#L766-L778<CODESPLIT>DataServiceApi.create_project_transfer<CODESPLIT>Send PUT request to one of the project transfer action endpoints : param action : str name of the action ( reject / accept / cancel ) : param transfer_id : str uuid of the project_transfer : param status_comment : str comment about the action optional : return : requests . Response containing the successful result<CODESPLIT>def create_project_transfer ( self , project_id , to_user_ids ) : data = { "to_users[][id]" : to_user_ids , } return self . _post ( "/projects/" + project_id + "/transfers" , data , content_type = ContentType . form )

0<CODESPLIT>https://github.com/tariqdaouda/pyGeno/blob/474b1250bf78ce5c7e7c3bbbfdbad9635d5a7d14/pyGeno/pyGenoObjectBases.py#L112-L114<CODESPLIT>CSVFile.toStr<CODESPLIT>Returns the number of elements satisfying the query<CODESPLIT>def toStr ( self ) : s = [ self . strLegend ] for l in self . lines : s . append ( l . toStr ( ) ) return self . lineSeparator . join ( s )

1<CODESPLIT>https://github.com/PyGithub/PyGithub/blob/f716df86bbe7dc276c6596699fa9712b61ef974c/github/Migration.py#L131-L144<CODESPLIT>Migration.get_status<CODESPLIT>: calls : GET / user / migrations / : migration_id _ : rtype : str<CODESPLIT>def get_status ( self ) : headers , data = self . _requester . requestJsonAndCheck ( "GET" , self . url , headers = { "Accept" : Consts . mediaTypeMigrationPreview } ) self . _useAttributes ( data ) return self . state

1<CODESPLIT>https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_bin.py#L204-L210<CODESPLIT>TcExBin.layout_json_outputs<CODESPLIT>Return layout . json outputs in a flattened dict with name param as key .<CODESPLIT>def layout_json_outputs ( self ) : if self . _layout_json_outputs is None : self . _layout_json_outputs = { } for o in self . layout_json . get ( 'outputs' , [ ] ) : self . _layout_json_outputs . setdefault ( o . get ( 'name' ) , o ) return self . _layout_json_outputs

1<CODESPLIT>https://github.com/rodynnz/xccdf/blob/1b9dc2f06b5cce8db2a54c5f95a8f6bcf5cb6981/src/xccdf/models/element.py#L71-L93<CODESPLIT>Element.as_dict<CODESPLIT>Serializes the object necessary data in a dictionary .<CODESPLIT>def as_dict ( self ) : element_dict = dict ( ) if hasattr ( self , 'namespace' ) : element_dict [ 'namespace' ] = self . namespace if hasattr ( self , 'name' ) : element_dict [ 'name' ] = self . name if hasattr ( self , 'text' ) : element_dict [ 'text' ] = self . text attr_dict = dict ( ) for attr in self . attrs : if hasattr ( self , attr ) : attr_dict [ attr ] = getattr ( self , attr ) element_dict [ 'attrs' ] = attr_dict return element_dict

1<CODESPLIT>https://github.com/goodmami/penman/blob/a2563ca16063a7330e2028eb489a99cc8e425c41/penman.py#L691-L704<CODESPLIT>Graph.edges<CODESPLIT>Return edges filtered by their * source * * relation * or * target * .<CODESPLIT>def edges ( self , source = None , relation = None , target = None ) : edgematch = lambda e : ( ( source is None or source == e . source ) and ( relation is None or relation == e . relation ) and ( target is None or target == e . target ) ) variables = self . variables ( ) edges = [ t for t in self . _triples if t . target in variables ] return list ( filter ( edgematch , edges ) )

1<CODESPLIT>https://github.com/SatelliteQE/nailgun/blob/c36d8c20862e87bf6975bd48ac1ca40a9e634eaa/nailgun/entities.py#L2738-L2758<CODESPLIT>ContentViewComponent.path<CODESPLIT>Extend nailgun . entity_mixins . Entity . path . The format of the returned path depends on the value of which :<CODESPLIT>def path ( self , which = None ) : if which in ( 'add' , 'remove' ) : return '{0}/{1}' . format ( super ( ContentViewComponent , self ) . path ( which = 'base' ) , which ) return super ( ContentViewComponent , self ) . path ( which )

1<CODESPLIT>https://github.com/vstconsulting/vstutils/blob/3d6d140c2463952dc9835a4e40caf758468b3049/vstutils/api/doc_generator.py#L309-L325<CODESPLIT>VSTOpenApiBase.get_object_example<CODESPLIT>Create example for response from object structure<CODESPLIT>def get_object_example ( self , def_name ) : def_model = self . definitions [ def_name ] example = dict ( ) for opt_name , opt_value in def_model . get ( 'properties' , dict ( ) ) . items ( ) : var_type = opt_value . get ( 'format' , None ) or opt_value . get ( 'type' , None ) example [ opt_name ] = self . get_response_example ( opt_name , var_type , opt_value ) if var_type == 'string' : example [ opt_name ] = example [ opt_name ] . format ( opt_name ) return example

0<CODESPLIT>https://github.com/ga4gh/ga4gh-server/blob/1aa18922ef136db8604f6f098cb1732cba6f2a76/ga4gh/server/backend.py#L168-L176<CODESPLIT>Backend.datasetsGenerator<CODESPLIT>Returns a generator over the ( phenotypeAssociationSet nextPageToken ) pairs defined by the specified request<CODESPLIT>def datasetsGenerator ( self , request ) : return self . _topLevelObjectGenerator ( request , self . getDataRepository ( ) . getNumDatasets ( ) , self . getDataRepository ( ) . getDatasetByIndex )

1<CODESPLIT>https://github.com/grabbles/grabbit/blob/83ff93df36019eaaee9d4e31f816a518e46cae07/grabbit/core.py#L557-L570<CODESPLIT>Layout._find_entity<CODESPLIT>Find an Entity instance by name . Checks both name and id fields .<CODESPLIT>def _find_entity ( self , entity ) : if entity in self . entities : return self . entities [ entity ] _ent = [ e for e in self . entities . values ( ) if e . name == entity ] if len ( _ent ) > 1 : raise ValueError ( "Entity name '%s' matches %d entities. To " "avoid ambiguity, please prefix the entity " "name with its domain (e.g., 'bids.%s'." % ( entity , len ( _ent ) , entity ) ) if _ent : return _ent [ 0 ] raise ValueError ( "No entity '%s' found." % entity )

0<CODESPLIT>https://github.com/shinux/PyTime/blob/f2b9f877507e2a1dddf5dd255fdff243a5dbed48/pytime/pytime.py#L50-L52<CODESPLIT>tomorrow<CODESPLIT>this day last year<CODESPLIT>def tomorrow ( date = None ) : if not date : return _date + datetime . timedelta ( days = 1 ) else : current_date = parse ( date ) return current_date + datetime . timedelta ( days = 1 )

1<CODESPLIT>https://github.com/blue-yonder/tsfresh/blob/c72c9c574371cf7dd7d54e00a466792792e5d202/tsfresh/feature_extraction/feature_calculators.py#L1630-L1647<CODESPLIT>value_count<CODESPLIT>Count occurrences of value in time series x .<CODESPLIT>def value_count ( x , value ) : if not isinstance ( x , ( np . ndarray , pd . Series ) ) : x = np . asarray ( x ) if np . isnan ( value ) : return np . isnan ( x ) . sum ( ) else : return x [ x == value ] . size

1<CODESPLIT>https://github.com/WoLpH/python-statsd/blob/a757da04375c48d03d322246405b33382d37f03f/statsd/raw.py#L24-L38<CODESPLIT>Raw.send<CODESPLIT>Send the data to statsd via self . connection<CODESPLIT>def send ( self , subname , value , timestamp = None ) : if timestamp is None : ts = int ( dt . datetime . now ( ) . strftime ( "%s" ) ) else : ts = timestamp name = self . _get_name ( self . name , subname ) self . logger . info ( '%s: %s %s' % ( name , value , ts ) ) return statsd . Client . _send ( self , { name : '%s|r|%s' % ( value , ts ) } )

1<CODESPLIT>https://github.com/erocarrera/pefile/blob/8a78a2e251a3f2336c232bf411133927b479edf2/pefile.py#L924-L963<CODESPLIT>Structure.dump<CODESPLIT>Returns a string representation of the structure .<CODESPLIT>def dump ( self , indentation = 0 ) : dump = [ ] dump . append ( '[{0}]' . format ( self . name ) ) printable_bytes = [ ord ( i ) for i in string . printable if i not in string . whitespace ] # Refer to the __set_format__ method for an explanation # of the following construct. for keys in self . __keys__ : for key in keys : val = getattr ( self , key ) if isinstance ( val , ( int , long ) ) : if key . startswith ( 'Signature_' ) : val_str = '%-8X' % ( val ) else : val_str = '0x%-8X' % ( val ) if key == 'TimeDateStamp' or key == 'dwTimeStamp' : try : val_str += ' [%s UTC]' % time . asctime ( time . gmtime ( val ) ) except ValueError as e : val_str += ' [INVALID TIME]' else : val_str = bytearray ( val ) if key . startswith ( 'Signature' ) : val_str = '' . join ( [ '{:02X}' . format ( i ) for i in val_str . rstrip ( b'\x00' ) ] ) else : val_str = '' . join ( [ chr ( i ) if ( i in printable_bytes ) else '\\x{0:02x}' . format ( i ) for i in val_str . rstrip ( b'\x00' ) ] ) dump . append ( '0x%-8X 0x%-3X %-30s %s' % ( self . __field_offsets__ [ key ] + self . __file_offset__ , self . __field_offsets__ [ key ] , key + ':' , val_str ) ) return dump

1<CODESPLIT>https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_dict.py#L2041-L2046<CODESPLIT>iteritems_sorted<CODESPLIT>change to iteritems ordered<CODESPLIT>def iteritems_sorted ( dict_ ) : if isinstance ( dict_ , OrderedDict ) : return six . iteritems ( dict_ ) else : return iter ( sorted ( six . iteritems ( dict_ ) ) )

0<CODESPLIT>https://github.com/priestc/moneywagon/blob/00518f1f557dcca8b3031f46d3564c2baa0227a3/moneywagon/crypto_data_extractor.py#L11-L30<CODESPLIT>uconcatenate<CODESPLIT>github_path can must be path on github such as bitcoin / bitcoin or litecoin - project / litecoin<CODESPLIT>def uconcatenate ( arrs , axis = 0 ) : v = np . concatenate ( arrs , axis = axis ) v = _validate_numpy_wrapper_units ( v , arrs ) return v

0<CODESPLIT>https://github.com/InfoAgeTech/django-core/blob/9664a145473b75120bf71e1644e9c8086e7e8955/django_core/db/models/mixins/tokens.py#L23-L40<CODESPLIT>BinaryOperator._popup<CODESPLIT>Preprocess the object before the object is saved . This automatically gets called when the save method gets called .<CODESPLIT>def _popup ( self ) : res = ( ) for child in self . formulas : if type ( child ) == type ( self ) : superchilds = child . formulas res += superchilds else : res += ( child , ) return tuple ( res )

1<CODESPLIT>https://github.com/jmoiron/humanize/blob/32c469bc378de22e8eabd5f9565bd7cffe7c7ae0/humanize/filesize.py#L13-L39<CODESPLIT>naturalsize<CODESPLIT>Format a number of byteslike a human readable filesize ( eg . 10 kB ) . By default decimal suffixes ( kB MB ) are used . Passing binary = true will use binary suffixes ( KiB MiB ) are used and the base will be 2 ** 10 instead of 10 ** 3 . If gnu is True the binary argument is ignored and GNU - style ( ls - sh style ) prefixes are used ( K M ) with the 2 ** 10 definition . Non - gnu modes are compatible with jinja2 s filesizeformat filter .<CODESPLIT>def naturalsize ( value , binary = False , gnu = False , format = '%.1f' ) : if gnu : suffix = suffixes [ 'gnu' ] elif binary : suffix = suffixes [ 'binary' ] else : suffix = suffixes [ 'decimal' ] base = 1024 if ( gnu or binary ) else 1000 bytes = float ( value ) if bytes == 1 and not gnu : return '1 Byte' elif bytes < base and not gnu : return '%d Bytes' % bytes elif bytes < base and gnu : return '%dB' % bytes for i , s in enumerate ( suffix ) : unit = base ** ( i + 2 ) if bytes < unit and not gnu : return ( format + ' %s' ) % ( ( base * bytes / unit ) , s ) elif bytes < unit and gnu : return ( format + '%s' ) % ( ( base * bytes / unit ) , s ) if gnu : return ( format + '%s' ) % ( ( base * bytes / unit ) , s ) return ( format + ' %s' ) % ( ( base * bytes / unit ) , s )

0<CODESPLIT>https://github.com/AliLozano/django-messages-extends/blob/141011981d44a6f28c6e82f9832815423b3b205f/messages_extends/storages.py#L234-L239<CODESPLIT>PersistentStorage.add<CODESPLIT>Delete all messages that are sticky and return the other messages This storage never save objects<CODESPLIT>def add ( self , level , message , extra_tags = '' , * args , * * kwargs ) : if not message : return # Check that the message level is not less than the recording level. level = int ( level ) if level < self . level : return # Add the message. self . added_new = True message = Message ( level , message , extra_tags = extra_tags ) message = self . process_message ( message , * args , * * kwargs ) if message : self . _queued_messages . append ( message )

1<CODESPLIT>https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/ofctl_string.py#L167-L189<CODESPLIT>nxm_field_name_to_ryu<CODESPLIT>Convert an ovs - ofctl style NXM_ / OXM_ field name to a ryu match field name .<CODESPLIT>def nxm_field_name_to_ryu ( field ) : if field . endswith ( "_W" ) : field = field [ : - 2 ] prefix = field [ : 7 ] field = field [ 7 : ] . lower ( ) mapped_result = None if prefix == 'NXM_NX_' : mapped_result = _NXM_FIELD_MAP . get ( field ) elif prefix == "NXM_OF_" : mapped_result = _NXM_OF_FIELD_MAP . get ( field ) elif prefix == "OXM_OF_" : # no mapping needed pass else : raise ValueError if mapped_result is not None : return mapped_result return field

1<CODESPLIT>https://github.com/python-openxml/python-docx/blob/6756f6cd145511d3eb6d1d188beea391b1ddfd53/docx/dml/color.py#L28-L51<CODESPLIT>ColorFormat.rgb<CODESPLIT>An |RGBColor| value or |None| if no RGB color is specified .<CODESPLIT>def rgb ( self ) : color = self . _color if color is None : return None if color . val == ST_HexColorAuto . AUTO : return None return color . val

0<CODESPLIT>https://github.com/lxc/python2-lxc/blob/b7ec757d2bea1e5787c3e65b1359b8893491ef90/lxc/__init__.py#L326-L350<CODESPLIT>Container.rename<CODESPLIT>Get a tuple of IPs for the container .<CODESPLIT>def rename ( self , new_name ) : if _lxc . Container . rename ( self , new_name ) : return Container ( new_name ) return False

0<CODESPLIT>https://github.com/codelv/enaml-native/blob/c33986e9eda468c508806e0a3e73c771401e5718/examples/playground/main.py#L25-L41<CODESPLIT>AndroidViewAnimator.init_widget<CODESPLIT>Called by PyBridge . start ()<CODESPLIT>def init_widget ( self ) : super ( AndroidViewAnimator , self ) . init_widget ( ) d = self . declaration if d . animate_first_view : self . set_animate_first_view ( d . animate_first_view ) if d . displayed_child : self . set_displayed_child ( d . displayed_child )

0<CODESPLIT>https://github.com/SectorLabs/django-localized-fields/blob/f0ac0f7f2503317fde5d75ba8481e34db83512bd/localized_fields/value.py#L59-L68<CODESPLIT>LocalizedValue.set<CODESPLIT>Deconstructs this value into a primitive type .<CODESPLIT>def set ( self , language : str , value : str ) : self [ language ] = value self . __dict__ . update ( self ) return self

0<CODESPLIT>https://github.com/bast/flanders/blob/792f9eed8511cb553e67a25b6c5ce60fd6ae97bc/cmake/autocmake/generate.py#L1-L31<CODESPLIT>AttributeFilterImmutable.as_dict<CODESPLIT>Generate CMake command .<CODESPLIT>def as_dict ( self ) : output_dictionary = dict ( ) for key , value in iter ( self . _key_map . items ( ) ) : if isinstance ( value , bool ) : output_dictionary [ key ] = value elif isinstance ( value , self . __class__ ) : output_dictionary [ key ] = value . as_dict ( ) return output_dictionary

1<CODESPLIT>https://github.com/gabstopper/smc-python/blob/e027b8a5dcfaf884eada32d113d41c1e56b32457/smc/api/session.py#L648-L677<CODESPLIT>Session.set_retry_on_busy<CODESPLIT>Mount a custom retry object on the current session that allows service level retries when the SMC might reply with a Service Unavailable ( 503 ) message . This can be possible in larger environments with higher database activity . You can all this on the existing session or provide as a dict to the login constructor . : param int total : total retries : param float backoff_factor : when to retry : param list status_forcelist : list of HTTP error codes to retry on : param list method_whitelist : list of methods to apply retries for GET POST and PUT by default : return : None<CODESPLIT>def set_retry_on_busy ( self , total = 5 , backoff_factor = 0.1 , status_forcelist = None , * * kwargs ) : if self . session : from requests . adapters import HTTPAdapter from requests . packages . urllib3 . util . retry import Retry method_whitelist = kwargs . pop ( 'method_whitelist' , [ ] ) or [ 'GET' , 'POST' , 'PUT' ] status_forcelist = frozenset ( status_forcelist ) if status_forcelist else frozenset ( [ 503 ] ) retry = Retry ( total = total , backoff_factor = backoff_factor , status_forcelist = status_forcelist , method_whitelist = method_whitelist ) for proto_str in ( 'http://' , 'https://' ) : self . session . mount ( proto_str , HTTPAdapter ( max_retries = retry ) ) logger . debug ( 'Mounting retry object to HTTP session: %s' % retry )

0<CODESPLIT>https://github.com/ashleysommer/sanicpluginsframework/blob/2cb1656d9334f04c30c738074784b0450c1b893e/spf/plugins/contextualize.py#L197-L222<CODESPLIT>Contextualize.listener<CODESPLIT>Create a plugin route from a decorated function . : param uri : endpoint at which the route will be accessible . : type uri : str : param args : captures all of the positional arguments passed in : type args : tuple ( Any ) : param kwargs : captures the keyword arguments passed in : type kwargs : dict ( Any ) : return : The exception function to use as the decorator : rtype : fn<CODESPLIT>def listener ( self , event , * args , * * kwargs ) : if len ( args ) == 1 and callable ( args [ 0 ] ) : raise RuntimeError ( "Cannot use the @listener decorator without " "arguments" ) kwargs [ 'with_context' ] = True # This is the whole point of this plugin def wrapper ( listener_f ) : nonlocal self , event , args , kwargs return super ( Contextualize , self ) . listener ( event , * args , * * kwargs ) ( listener_f ) return wrapper

0<CODESPLIT>https://github.com/petl-developers/petl/blob/1d33ca055f7e04e0d28a772041c9fd30c8d415d6/petl/transform/regex.py#L245-L293<CODESPLIT>sub<CODESPLIT>Perform a regular expression search returning rows that match a given pattern either anywhere in the row or within a specific field . E . g . ::<CODESPLIT>def sub ( table , field , pattern , repl , count = 0 , flags = 0 ) : prog = re . compile ( pattern , flags ) conv = lambda v : prog . sub ( repl , v , count = count ) return convert ( table , field , conv )

0<CODESPLIT>https://github.com/clintval/sample-sheet/blob/116ac6f26f6e61b57716c90f6e887d3d457756f3/sample_sheet/__init__.py#L183-L185<CODESPLIT>Sample.to_json<CODESPLIT>The number of total number of cycles in the structure .<CODESPLIT>def to_json ( self ) -> Mapping : return { str ( x ) : str ( y ) for x , y in self . items ( ) }

0<CODESPLIT>https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/old/fastai/learner.py#L127-L249<CODESPLIT>adjust_learning_rate<CODESPLIT>Method does some preparation before finally delegating to the fit method for fitting the model . Namely if cycle_len is defined it adds a Cosine Annealing scheduler for varying the learning rate across iterations .<CODESPLIT>def adjust_learning_rate ( optimizer , epoch , gammas , schedule ) : lr = args . learning_rate assert len ( gammas ) == len ( schedule ) , "length of gammas and schedule should be equal" for ( gamma , step ) in zip ( gammas , schedule ) : if ( epoch >= step ) : lr = lr * gamma else : break for param_group in optimizer . param_groups : param_group [ 'lr' ] = lr return lr

1<CODESPLIT>https://github.com/bukun/TorCMS/blob/6567c7fe2604a1d646d4570c017840958630ed2b/torcms/script/script_sitemap.py#L56-L67<CODESPLIT>run_sitemap<CODESPLIT>Generate the sitemap file . : param args : args : return : None<CODESPLIT>def run_sitemap ( _ ) : site_map_file = 'xx_sitemap.txt' if os . path . exists ( site_map_file ) : os . remove ( site_map_file ) gen_wiki_map ( site_map_file ) gen_post_map ( site_map_file )

0<CODESPLIT>https://github.com/summanlp/textrank/blob/6844bbe8c4b2b468020ae0dfd6574a743f9ad442/summa/preprocessing/textcleaner.py#L120-L126<CODESPLIT>ArabicStemmer.stem<CODESPLIT>Remove accentuation from the given string .<CODESPLIT>def stem ( self , word ) : # set initial values self . is_verb = True self . is_noun = True self . is_defined = False self . suffix_verb_step2a_success = False self . suffix_verb_step2b_success = False self . suffix_noun_step2c2_success = False self . suffix_noun_step1a_success = False self . suffix_noun_step2a_success = False self . suffix_noun_step2b_success = False self . suffixe_noun_step1b_success = False self . prefix_step2a_success = False self . prefix_step3a_noun_success = False self . prefix_step3b_noun_success = False modified_word = word # guess type and properties # checks1 self . __checks_1 ( modified_word ) # checks2 self . __checks_2 ( modified_word ) # Pre_Normalization modified_word = self . __normalize_pre ( modified_word ) # Start stemming if self . is_verb : modified_word = self . __Suffix_Verb_Step1 ( modified_word ) if self . suffixes_verb_step1_success : modified_word = self . __Suffix_Verb_Step2a ( modified_word ) if not self . suffix_verb_step2a_success : modified_word = self . __Suffix_Verb_Step2c ( modified_word ) # or next TODO: How to deal with or next instruction else : modified_word = self . __Suffix_Verb_Step2b ( modified_word ) if not self . suffix_verb_step2b_success : modified_word = self . __Suffix_Verb_Step2a ( modified_word ) if self . is_noun : modified_word = self . __Suffix_Noun_Step2c2 ( modified_word ) if not self . suffix_noun_step2c2_success : if not self . is_defined : modified_word = self . __Suffix_Noun_Step1a ( modified_word ) # if self.suffix_noun_step1a_success: modified_word = self . __Suffix_Noun_Step2a ( modified_word ) if not self . suffix_noun_step2a_success : modified_word = self . __Suffix_Noun_Step2b ( modified_word ) if ( not self . suffix_noun_step2b_success and not self . suffix_noun_step2a_success ) : modified_word = self . __Suffix_Noun_Step2c1 ( modified_word ) # or next ? todo : how to deal with or next else : modified_word = self . __Suffix_Noun_Step1b ( modified_word ) if self . suffixe_noun_step1b_success : modified_word = self . __Suffix_Noun_Step2a ( modified_word ) if not self . suffix_noun_step2a_success : modified_word = self . __Suffix_Noun_Step2b ( modified_word ) if ( not self . suffix_noun_step2b_success and not self . suffix_noun_step2a_success ) : modified_word = self . __Suffix_Noun_Step2c1 ( modified_word ) else : if not self . is_defined : modified_word = self . __Suffix_Noun_Step2a ( modified_word ) modified_word = self . __Suffix_Noun_Step2b ( modified_word ) modified_word = self . __Suffix_Noun_Step3 ( modified_word ) if not self . is_noun and self . is_verb : modified_word = self . __Suffix_All_alef_maqsura ( modified_word ) # prefixes modified_word = self . __Prefix_Step1 ( modified_word ) modified_word = self . __Prefix_Step2a ( modified_word ) if not self . prefix_step2a_success : modified_word = self . __Prefix_Step2b ( modified_word ) modified_word = self . __Prefix_Step3a_Noun ( modified_word ) if not self . prefix_step3a_noun_success and self . is_noun : modified_word = self . __Prefix_Step3b_Noun ( modified_word ) else : if not self . prefix_step3b_noun_success and self . is_verb : modified_word = self . __Prefix_Step3_Verb ( modified_word ) modified_word = self . __Prefix_Step4_Verb ( modified_word ) # post normalization stemming modified_word = self . __normalize_post ( modified_word ) stemmed_word = modified_word return stemmed_word

1<CODESPLIT>https://github.com/anjishnu/ask-alexa-pykit/blob/a47c278ca7a60532bbe1a9b789f6c37e609fea8b/ask/alexa_io.py#L177-L184<CODESPLIT>VoiceHandler.intent<CODESPLIT>Decorator to register intent handler<CODESPLIT>def intent ( self , intent ) : def _handler ( func ) : self . _handlers [ 'IntentRequest' ] [ intent ] = func return func return _handler

0<CODESPLIT>https://github.com/rosenbrockc/ci/blob/4d5a60291424a83124d1d962d17fb4c7718cde2b/pyci/server.py#L804-L846<CODESPLIT>detect_sys<CODESPLIT>Sends the email using the configured settings .<CODESPLIT>def detect_sys ( ) : system = platform . system ( ) bit = platform . architecture ( ) [ 0 ] compiler = platform . python_compiler ( ) ver = platform . python_version_tuple ( ) return { 'system' : system , 'bit' : bit , 'compiler' : compiler , 'python_version_tuple' : ver }

1<CODESPLIT>https://github.com/dadadel/pyment/blob/3d1bdf87d083ff56230bd0bf7c5252e20552b7b6/pyment/docstring.py#L41-L54<CODESPLIT>isin_start<CODESPLIT>Check if an element from a list starts a string .<CODESPLIT>def isin_start ( elems , line ) : found = False elems = [ elems ] if type ( elems ) is not list else elems for e in elems : if line . lstrip ( ) . lower ( ) . startswith ( e ) : found = True break return found

1<CODESPLIT>https://github.com/ArchiveTeam/wpull/blob/ddf051aa3322479325ba20aa778cb2cb97606bf5/wpull/scraper/html.py#L422-L444<CODESPLIT>ElementWalker.iter_links_meta_element<CODESPLIT>Iterate the meta element for links .<CODESPLIT>def iter_links_meta_element ( cls , element ) : if element . attrib . get ( 'http-equiv' , '' ) . lower ( ) == 'refresh' : content_value = element . attrib . get ( 'content' ) if content_value : link = parse_refresh ( content_value ) if link : yield LinkInfo ( element = element , tag = element . tag , attrib = 'http-equiv' , link = link , inline = False , linked = True , base_link = None , value_type = 'refresh' , link_type = None # treat it as a redirect ) else : for link_info in cls . iter_links_open_graph_meta ( element ) : yield link_info

0<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/nagios_rpc.py#L32-L49<CODESPLIT>_status_query<CODESPLIT>Get configuration items for URL Username and Password<CODESPLIT>def _status_query ( query , hostname , enumerate = None , service = None ) : config = _config ( ) data = None params = { 'hostname' : hostname , 'query' : query , } ret = { 'result' : False } if enumerate : params [ 'formatoptions' ] = 'enumerate' if service : params [ 'servicedescription' ] = service if config [ 'username' ] and config [ 'password' ] is not None : auth = ( config [ 'username' ] , config [ 'password' ] , ) else : auth = None try : result = salt . utils . http . query ( config [ 'url' ] , method = 'GET' , params = params , decode = True , data = data , text = True , status = True , header_dict = { } , auth = auth , backend = 'requests' , opts = __opts__ , ) except ValueError : ret [ 'error' ] = 'Please ensure Nagios is running.' ret [ 'result' ] = False return ret if result . get ( 'status' , None ) == salt . ext . six . moves . http_client . OK : try : ret [ 'json_data' ] = result [ 'dict' ] ret [ 'result' ] = True except ValueError : ret [ 'error' ] = 'Please ensure Nagios is running.' elif result . get ( 'status' , None ) == salt . ext . six . moves . http_client . UNAUTHORIZED : ret [ 'error' ] = 'Authentication failed. Please check the configuration.' elif result . get ( 'status' , None ) == salt . ext . six . moves . http_client . NOT_FOUND : ret [ 'error' ] = 'URL {0} was not found.' . format ( config [ 'url' ] ) else : ret [ 'error' ] = 'Results: {0}' . format ( result . text ) return ret

1<CODESPLIT>https://github.com/m-weigand/sip_models/blob/917da5d956215d9df2bf65b24123ba020e3e17c0/lib/sip_models/res/cc.py#L49-L88<CODESPLIT>cc_base._set_parameters<CODESPLIT>Sort out the various possible parameter inputs and return a config object ( dict )<CODESPLIT>def _set_parameters ( self , parameters ) : nr_f = self . f . size # sort out parameters rho0 , m , tau , c = self . _sort_parameters ( parameters ) newsize = ( nr_f , len ( m ) ) # rho0_resized = np.resize(rho0, newsize) m_resized = np . resize ( m , newsize ) tau_resized = np . resize ( tau , newsize ) c_resized = np . resize ( c , newsize ) omega = np . atleast_2d ( 2 * np . pi * self . f ) . T self . w = np . resize ( omega , ( len ( m ) , nr_f ) ) . T self . rho0 = rho0 self . m = m_resized self . tau = tau_resized self . c = c_resized # compute some common terms self . otc = ( self . w * self . tau ) ** self . c self . otc2 = ( self . w * self . tau ) ** ( 2 * self . c ) self . ang = self . c * np . pi / 2.0 # rad self . denom = 1 + 2 * self . otc * np . cos ( self . ang ) + self . otc2

0<CODESPLIT>https://github.com/BYU-Hydroinformatics/HydroErr/blob/42a84f3e006044f450edc7393ed54d59f27ef35b/HydroErr/HydroErr.py#L5110-L5189<CODESPLIT>treat_values<CODESPLIT>Compute the H6 mean absolute error .<CODESPLIT>def treat_values ( simulated_array , observed_array , replace_nan = None , replace_inf = None , remove_neg = False , remove_zero = False ) : sim_copy = np . copy ( simulated_array ) obs_copy = np . copy ( observed_array ) # Checking to see if the vectors are the same length assert sim_copy . ndim == 1 , "The simulated array is not one dimensional." assert obs_copy . ndim == 1 , "The observed array is not one dimensional." if sim_copy . size != obs_copy . size : raise RuntimeError ( "The two ndarrays are not the same size." ) # Treat missing data in observed_array and simulated_array, rows in simulated_array or # observed_array that contain nan values all_treatment_array = np . ones ( obs_copy . size , dtype = bool ) if np . any ( np . isnan ( obs_copy ) ) or np . any ( np . isnan ( sim_copy ) ) : if replace_nan is not None : # Finding the NaNs sim_nan = np . isnan ( sim_copy ) obs_nan = np . isnan ( obs_copy ) # Replacing the NaNs with the input sim_copy [ sim_nan ] = replace_nan obs_copy [ obs_nan ] = replace_nan warnings . warn ( "Elements(s) {} contained NaN values in the simulated array and " "elements(s) {} contained NaN values in the observed array and have been " "replaced (Elements are zero indexed)." . format ( np . where ( sim_nan ) [ 0 ] , np . where ( obs_nan ) [ 0 ] ) , UserWarning ) else : # Getting the indices of the nan values, combining them, and informing user. nan_indices_fcst = ~ np . isnan ( sim_copy ) nan_indices_obs = ~ np . isnan ( obs_copy ) all_nan_indices = np . logical_and ( nan_indices_fcst , nan_indices_obs ) all_treatment_array = np . logical_and ( all_treatment_array , all_nan_indices ) warnings . warn ( "Row(s) {} contained NaN values and the row(s) have been " "removed (Rows are zero indexed)." . format ( np . where ( ~ all_nan_indices ) [ 0 ] ) , UserWarning ) if np . any ( np . isinf ( obs_copy ) ) or np . any ( np . isinf ( sim_copy ) ) : if replace_nan is not None : # Finding the NaNs sim_inf = np . isinf ( sim_copy ) obs_inf = np . isinf ( obs_copy ) # Replacing the NaNs with the input sim_copy [ sim_inf ] = replace_inf obs_copy [ obs_inf ] = replace_inf warnings . warn ( "Elements(s) {} contained Inf values in the simulated array and " "elements(s) {} contained Inf values in the observed array and have been " "replaced (Elements are zero indexed)." . format ( np . where ( sim_inf ) [ 0 ] , np . where ( obs_inf ) [ 0 ] ) , UserWarning ) else : inf_indices_fcst = ~ ( np . isinf ( sim_copy ) ) inf_indices_obs = ~ np . isinf ( obs_copy ) all_inf_indices = np . logical_and ( inf_indices_fcst , inf_indices_obs ) all_treatment_array = np . logical_and ( all_treatment_array , all_inf_indices ) warnings . warn ( "Row(s) {} contained Inf or -Inf values and the row(s) have been removed (Rows " "are zero indexed)." . format ( np . where ( ~ all_inf_indices ) [ 0 ] ) , UserWarning ) # Treat zero data in observed_array and simulated_array, rows in simulated_array or # observed_array that contain zero values if remove_zero : if ( obs_copy == 0 ) . any ( ) or ( sim_copy == 0 ) . any ( ) : zero_indices_fcst = ~ ( sim_copy == 0 ) zero_indices_obs = ~ ( obs_copy == 0 ) all_zero_indices = np . logical_and ( zero_indices_fcst , zero_indices_obs ) all_treatment_array = np . logical_and ( all_treatment_array , all_zero_indices ) warnings . warn ( "Row(s) {} contained zero values and the row(s) have been removed (Rows are " "zero indexed)." . format ( np . where ( ~ all_zero_indices ) [ 0 ] ) , UserWarning ) # Treat negative data in observed_array and simulated_array, rows in simulated_array or # observed_array that contain negative values # Ignore runtime warnings from comparing if remove_neg : with np . errstate ( invalid = 'ignore' ) : obs_copy_bool = obs_copy < 0 sim_copy_bool = sim_copy < 0 if obs_copy_bool . any ( ) or sim_copy_bool . any ( ) : neg_indices_fcst = ~ sim_copy_bool neg_indices_obs = ~ obs_copy_bool all_neg_indices = np . logical_and ( neg_indices_fcst , neg_indices_obs ) all_treatment_array = np . logical_and ( all_treatment_array , all_neg_indices ) warnings . warn ( "Row(s) {} contained negative values and the row(s) have been " "removed (Rows are zero indexed)." . format ( np . where ( ~ all_neg_indices ) [ 0 ] ) , UserWarning ) obs_copy = obs_copy [ all_treatment_array ] sim_copy = sim_copy [ all_treatment_array ] return sim_copy , obs_copy

0<CODESPLIT>https://github.com/liamw9534/bt-manager/blob/51be2919394ce8134c698359649bfad09eedf4ec/bt_manager/audio.py#L374-L385<CODESPLIT>SBCAudioSink._property_change_event_handler<CODESPLIT>Helper to turn a2dp_sbc_t structure into a more usable set of SBC codec configuration params<CODESPLIT>def _property_change_event_handler ( self , signal , transport , * args ) : current_state = self . source . State if ( self . state == 'connected' and current_state == 'playing' ) : self . _acquire_media_transport ( transport , 'r' ) elif ( self . state == 'playing' and current_state == 'connected' ) : self . _release_media_transport ( transport , 'r' ) self . state = current_state

0<CODESPLIT>https://github.com/glue-viz/glue-vispy-viewers/blob/54a4351d98c1f90dfb1a557d1b447c1f57470eea/glue_vispy_viewers/extern/vispy/util/fetching.py#L174-L207<CODESPLIT>_chunk_write<CODESPLIT>Download a file chunk by chunk and show advancement<CODESPLIT>def _chunk_write ( chunk , local_file , progress ) : local_file . write ( chunk ) progress . update_with_increment_value ( len ( chunk ) )

1<CODESPLIT>https://github.com/RJT1990/pyflux/blob/297f2afc2095acd97c12e827dd500e8ea5da0c0f/pyflux/garch/egarch.py#L800-L824<CODESPLIT>EGARCH.ppc<CODESPLIT>Computes posterior predictive p - value<CODESPLIT>def ppc ( self , nsims = 1000 , T = np . mean ) : if self . latent_variables . estimation_method not in [ 'BBVI' , 'M-H' ] : raise Exception ( "No latent variables estimated!" ) else : lv_draws = self . draw_latent_variables ( nsims = nsims ) sigmas = [ self . _model ( lv_draws [ : , i ] ) [ 0 ] for i in range ( nsims ) ] data_draws = np . array ( [ ss . t . rvs ( loc = self . latent_variables . z_list [ - 1 ] . prior . transform ( lv_draws [ - 1 , i ] ) , df = self . latent_variables . z_list [ - 2 ] . prior . transform ( lv_draws [ - 2 , i ] ) , scale = np . exp ( sigmas [ i ] / 2.0 ) ) for i in range ( nsims ) ] ) T_sims = T ( self . sample ( nsims = nsims ) , axis = 1 ) T_actual = T ( self . data ) return len ( T_sims [ T_sims > T_actual ] ) / nsims

1<CODESPLIT>https://github.com/fhs/pyhdf/blob/dbdc1810a74a38df50dcad81fe903e239d2b388d/pyhdf/SD.py#L2721-L2738<CODESPLIT>SDS.setexternalfile<CODESPLIT>Store the dataset data in an external file .<CODESPLIT>def setexternalfile ( self , filename , offset = 0 ) : status = _C . SDsetexternalfile ( self . _id , filename , offset ) _checkErr ( 'setexternalfile' , status , 'execution error' )

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/boto_vpc.py#L2263-L2282<CODESPLIT>replace_route_table_association<CODESPLIT>Replaces a route table association .<CODESPLIT>def replace_route_table_association ( association_id , route_table_id , region = None , key = None , keyid = None , profile = None ) : try : conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) association_id = conn . replace_route_table_association_with_assoc ( association_id , route_table_id ) log . info ( 'Route table %s was reassociated with association id %s' , route_table_id , association_id ) return { 'replaced' : True , 'association_id' : association_id } except BotoServerError as e : return { 'replaced' : False , 'error' : __utils__ [ 'boto.get_error' ] ( e ) }

1<CODESPLIT>https://github.com/markomanninen/abnum/blob/9bfc8f06f34d9a51aab038638f87e2bb5f9f4c99/romanize/heb.py#L95-L103<CODESPLIT>preprocess<CODESPLIT>Preprocess string to transform all diacritics and remove other special characters than appropriate : param string : : return :<CODESPLIT>def preprocess ( string ) : string = unicode ( string , encoding = "utf-8" ) return regex . sub ( '' , string ) . encode ( 'utf-8' )

1<CODESPLIT>https://github.com/salu133445/pypianoroll/blob/6224dc1e29222de2124d249acb80f3d072166917/pypianoroll/utilities.py#L194-L208<CODESPLIT>transpose<CODESPLIT>Return a copy of the object with piano - roll ( s ) transposed by semitones semitones .<CODESPLIT>def transpose ( obj , semitone ) : _check_supported ( obj ) copied = deepcopy ( obj ) copied . transpose ( semitone ) return copied

0<CODESPLIT>https://github.com/senaite/senaite.api/blob/c79c53abcbe6e3a5ab3ced86d2f455275efa20cf/src/senaite/api/__init__.py#L394-L417<CODESPLIT>get_uid<CODESPLIT>Get the icon of the content object<CODESPLIT>def get_uid ( brain_or_object ) : if is_portal ( brain_or_object ) : return '0' if is_brain ( brain_or_object ) and base_hasattr ( brain_or_object , "UID" ) : return brain_or_object . UID return get_object ( brain_or_object ) . UID ( )

1<CODESPLIT>https://github.com/SiLab-Bonn/basil/blob/99052482d9334dd1f5598eb2d2fb4d5399a32291/basil/RL/TrackRegister.py#L38-L41<CODESPLIT>TrackRegister.clear<CODESPLIT>Clear tracks in memory - all zero<CODESPLIT>def clear ( self ) : for track in self . _tracks : self . _tracks [ track ] . setall ( False )

1<CODESPLIT>https://github.com/django-treebeard/django-treebeard/blob/8042ee939cb45394909237da447f8925e3cc6aa3/treebeard/mp_tree.py#L149-L157<CODESPLIT>MP_ComplexAddMoveHandler.get_sql_update_numchild<CODESPLIT>: returns : The sql needed the numchild value of a node<CODESPLIT>def get_sql_update_numchild ( self , path , incdec = 'inc' ) : sql = "UPDATE %s SET numchild=numchild%s1" " WHERE path=%%s" % ( connection . ops . quote_name ( get_result_class ( self . node_cls ) . _meta . db_table ) , { 'inc' : '+' , 'dec' : '-' } [ incdec ] ) vals = [ path ] return sql , vals

0<CODESPLIT>https://github.com/vertexproject/synapse/blob/22e67c5a8f6d7caddbcf34b39ab1bd2d6c4a6e0b/synapse/lib/version.py#L110-L124<CODESPLIT>unpackVersion<CODESPLIT>Join a string of parts together with a . separator .<CODESPLIT>def unpackVersion ( ver ) : major = ( ver >> 20 * 2 ) & mask20 minor = ( ver >> 20 ) & mask20 patch = ver & mask20 return major , minor , patch

0<CODESPLIT>https://github.com/pyQode/pyqode.core/blob/a99ec6cd22d519394f613309412f8329dc4e90cb/pyqode/core/widgets/tabs.py#L162-L173<CODESPLIT>TabWidget.index_from_filename<CODESPLIT>Save all editors .<CODESPLIT>def index_from_filename ( self , path ) : if path : for i in range ( self . count ( ) ) : widget = self . widget ( i ) try : if widget . file . path == path : return i except AttributeError : pass # not an editor widget return - 1

1<CODESPLIT>https://github.com/guidj/jsonuri-py/blob/0e266be86a52dc93d792b05e458e07f0b279e490/jsonuri/jsonuri.py#L40-L64<CODESPLIT>deserialize<CODESPLIT>Converts an encoded message ( string or bytes ) into a python dictionary<CODESPLIT>def deserialize ( message , b64_encoded = True , uri_encoded = True ) : data = message if uri_encoded : data = urllib . parse . unquote_plus ( data ) if b64_encoded : data = jsonuri . io . decompress ( data ) return json . loads ( data )

0<CODESPLIT>https://github.com/ucbvislab/radiotool/blob/01c9d878a811cf400b1482896d641d9c95e83ded/radiotool/utils.py#L115-L134<CODESPLIT>linear<CODESPLIT>Create an equal power blend of arr1 ( fading out ) and arr2 ( fading in )<CODESPLIT>def linear ( arr1 , arr2 ) : n = N . shape ( arr1 ) [ 0 ] try : channels = N . shape ( arr1 ) [ 1 ] except : channels = 1 f_in = N . linspace ( 0 , 1 , num = n ) f_out = N . linspace ( 1 , 0 , num = n ) # f_in = N.arange(n) / float(n - 1) # f_out = N.arange(n - 1, -1, -1) / float(n) if channels > 1 : f_in = N . tile ( f_in , ( channels , 1 ) ) . T f_out = N . tile ( f_out , ( channels , 1 ) ) . T vals = f_out * arr1 + f_in * arr2 return vals

1<CODESPLIT>https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/third_party/pep8/lib2to3/lib2to3/pytree.py#L183-L197<CODESPLIT>Base.next_sibling<CODESPLIT>The node immediately following the invocant in their parent s children list . If the invocant does not have a next sibling it is None<CODESPLIT>def next_sibling ( self ) : if self . parent is None : return None # Can't use index(); we need to test by identity for i , child in enumerate ( self . parent . children ) : if child is self : try : return self . parent . children [ i + 1 ] except IndexError : return None

0<CODESPLIT>https://github.com/agile-geoscience/welly/blob/ed4c991011d6290938fef365553041026ba29f42/welly/utils.py#L104-L120<CODESPLIT>sharey<CODESPLIT>Center ticklabels and hide any outside axes limits .<CODESPLIT>def sharey ( axes ) : linker = Linker ( axes ) for ax in axes : ax . _linker = linker

0<CODESPLIT>https://github.com/honzajavorek/tipi/blob/cbe51192725608b6fba1244a48610ae231b13e08/tipi/html.py#L151-L164<CODESPLIT>HTMLFragment._analyze_tree<CODESPLIT>Iterates over texts in given HTML tree .<CODESPLIT>def _analyze_tree ( self , tree ) : addresses = [ ] for text in self . _iter_texts ( tree ) : for i , char in enumerate ( text . content ) : if char in whitespace : char = ' ' addresses . append ( CharAddress ( char , text . element , text . attr , i ) ) # remove leading and trailing whitespace while addresses and addresses [ 0 ] . char == ' ' : del addresses [ 0 ] while addresses and addresses [ - 1 ] . char == ' ' : del addresses [ - 1 ] return addresses

1<CODESPLIT>https://github.com/willkg/everett/blob/5653134af59f439d2b33f3939fab2b8544428f11/everett/manager.py#L560-L564<CODESPLIT>ConfigManagerBase.with_options<CODESPLIT>Apply options component options to this configuration .<CODESPLIT>def with_options ( self , component ) : options = component . get_required_config ( ) component_name = _get_component_name ( component ) return BoundConfig ( self . _get_base_config ( ) , component_name , options )

0<CODESPLIT>https://github.com/orbingol/NURBS-Python/blob/b1c6a8b51cf143ff58761438e93ba6baef470627/geomdl/_voxelize.py#L86-L125<CODESPLIT>generate_voxel_grid<CODESPLIT>Finds if any point is contained inside the voxel boundaries ( inouts array ) .<CODESPLIT>def generate_voxel_grid ( bbox , szval , use_cubes = False ) : # Input validation if szval [ 0 ] <= 1 or szval [ 1 ] <= 1 or szval [ 2 ] <= 1 : raise GeomdlException ( "Size values must be bigger than 1" , data = dict ( sizevals = szval ) ) # Find step size for each direction steps = [ float ( bbox [ 1 ] [ idx ] - bbox [ 0 ] [ idx ] ) / float ( szval [ idx ] - 1 ) for idx in range ( 0 , 3 ) ] # It is possible to use cubes instead of cuboids if use_cubes : min_val = min ( * steps ) steps = [ min_val for _ in range ( 0 , 3 ) ] # Find range in each direction ranges = [ list ( linalg . frange ( bbox [ 0 ] [ idx ] , bbox [ 1 ] [ idx ] , steps [ idx ] ) ) for idx in range ( 0 , 3 ) ] voxel_grid = [ ] for u in ranges [ 0 ] : for v in ranges [ 1 ] : for w in ranges [ 2 ] : bbmin = [ u , v , w ] bbmax = [ k + l for k , l in zip ( bbmin , steps ) ] voxel_grid . append ( [ bbmin , bbmax ] ) return voxel_grid

1<CODESPLIT>https://github.com/dnanexus/dx-toolkit/blob/74befb53ad90fcf902d8983ae6d74580f402d619/src/python/dxpy/api.py#L830-L836<CODESPLIT>job_terminate<CODESPLIT>Invokes the / job - xxxx / terminate API method .<CODESPLIT>def job_terminate ( object_id , input_params = { } , always_retry = True , * * kwargs ) : return DXHTTPRequest ( '/%s/terminate' % object_id , input_params , always_retry = always_retry , * * kwargs )

1<CODESPLIT>https://github.com/Locu/chronology/blob/0edf3ee3286c76e242cbf92436ffa9c836b428e2/pykronos/pykronos/client.py#L312-L319<CODESPLIT>KronosClient.infer_schema<CODESPLIT>Queries the Kronos server and fetches the inferred schema for the requested stream .<CODESPLIT>def infer_schema ( self , stream , namespace = None ) : return self . _make_request ( self . _infer_schema_url , data = { 'stream' : stream , 'namespace' : namespace or self . namespace } )

0<CODESPLIT>https://github.com/bopo/mootdx/blob/7c4623e9464c75d3c87a06d48fe8734b027374fa/mootdx/reader.py#L129-L141<CODESPLIT>diff_matrix<CODESPLIT>获取日线数据<CODESPLIT>def diff_matrix ( array_1 , array_2 , cell_size ) : array_1_norm = np . array ( array_1 ) / np . array ( cell_size , dtype = float ) [ None , : ] array_2_norm = np . array ( array_2 ) / np . array ( cell_size , dtype = float ) [ None , : ] return array_2_norm - array_1_norm

0<CODESPLIT>https://github.com/pytroll/pyspectral/blob/fd296c0e0bdf5364fa180134a1292665d6bc50a3/pyspectral/utils.py#L237-L256<CODESPLIT>convert2wavenumber<CODESPLIT>Calculate the central wavelength or the central wavenumber depending on which parameters is input . On default the weighting funcion is f ( lambda ) = 1 . 0 but it is possible to add a custom weight e . g . f ( lambda ) = 1 . / lambda ** 4 for Rayleigh scattering calculations<CODESPLIT>def convert2wavenumber ( rsr ) : retv = { } for chname in rsr . keys ( ) : # Go through bands/channels retv [ chname ] = { } for det in rsr [ chname ] . keys ( ) : # Go through detectors retv [ chname ] [ det ] = { } if 'wavenumber' in rsr [ chname ] [ det ] . keys ( ) : # Make a copy. Data are already in wave number space retv [ chname ] [ det ] = rsr [ chname ] [ det ] . copy ( ) LOG . debug ( "RSR data already in wavenumber space. No conversion needed." ) continue for sat in rsr [ chname ] [ det ] . keys ( ) : if sat == "wavelength" : # micro meters to cm wnum = 1. / ( 1e-4 * rsr [ chname ] [ det ] [ sat ] ) retv [ chname ] [ det ] [ 'wavenumber' ] = wnum [ : : - 1 ] elif sat == "response" : # Flip the response array: if type ( rsr [ chname ] [ det ] [ sat ] ) is dict : retv [ chname ] [ det ] [ sat ] = { } for name in rsr [ chname ] [ det ] [ sat ] . keys ( ) : resp = rsr [ chname ] [ det ] [ sat ] [ name ] retv [ chname ] [ det ] [ sat ] [ name ] = resp [ : : - 1 ] else : resp = rsr [ chname ] [ det ] [ sat ] retv [ chname ] [ det ] [ sat ] = resp [ : : - 1 ] unit = 'cm-1' si_scale = 100.0 return retv , { 'unit' : unit , 'si_scale' : si_scale }

1<CODESPLIT>https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/assessment/objects.py#L1847-L1863<CODESPLIT>AssessmentOfferedForm.set_start_time<CODESPLIT>Sets the assessment start time .<CODESPLIT>def set_start_time ( self , start ) : # Implemented from template for osid.assessment.AssessmentOfferedForm.set_start_time_template if self . get_start_time_metadata ( ) . is_read_only ( ) : raise errors . NoAccess ( ) if not self . _is_valid_date_time ( start , self . get_start_time_metadata ( ) ) : raise errors . InvalidArgument ( ) self . _my_map [ 'startTime' ] = start

0<CODESPLIT>https://github.com/mozilla/configman/blob/83159fed61cc4cbbe5a4a6a00d3acad8a0c39c96/configman/value_sources/for_getopt.py#L64-L124<CODESPLIT>ValueSource.getopt_with_ignore<CODESPLIT>This is the black sheep of the crowd of ValueSource implementations . It needs to know ahead of time all of the parameters that it will need but we cannot give it . We may not know all the parameters because not all classes may have been expanded yet . The two parameters allow this ValueSource implementation to know what the parameters have already been defined . The ignore_mismatches parameter tells the implementation if it can or cannot ignore extraneous commandline options . The last time this function is called it will be required to test for illegal commandline options and respond accordingly .<CODESPLIT>def getopt_with_ignore ( args , shortopts , longopts = [ ] ) : opts = [ ] prog_args = [ ] if isinstance ( longopts , str ) : longopts = [ longopts ] else : longopts = list ( longopts ) while args : if args [ 0 ] == '--' : prog_args += args [ 1 : ] break if args [ 0 ] . startswith ( '--' ) : try : opts , args = getopt . do_longs ( opts , args [ 0 ] [ 2 : ] , longopts , args [ 1 : ] ) except getopt . GetoptError : args = args [ 1 : ] elif args [ 0 ] [ 0 ] == '-' : try : opts , args = getopt . do_shorts ( opts , args [ 0 ] [ 1 : ] , shortopts , args [ 1 : ] ) except getopt . GetoptError : args = args [ 1 : ] else : prog_args . append ( args [ 0 ] ) args = args [ 1 : ] return opts , prog_args

1<CODESPLIT>https://github.com/sernst/cauldron/blob/4086aec9c038c402ea212c79fe8bd0d27104f9cf/cauldron/cli/server/arguments.py#L4-L24<CODESPLIT>from_request<CODESPLIT>Fetches the arguments for the current Flask application request<CODESPLIT>def from_request ( request = None ) -> dict : request = request if request else flask_request try : json_args = request . get_json ( silent = True ) except Exception : json_args = None try : get_args = request . values except Exception : get_args = None arg_sources = list ( filter ( lambda arg : arg is not None , [ json_args , get_args , { } ] ) ) return arg_sources [ 0 ]

1<CODESPLIT>https://github.com/modin-project/modin/blob/5b77d242596560c646b8405340c9ce64acb183cb/modin/pandas/base.py#L1522-L1544<CODESPLIT>BasePandasDataset.mean<CODESPLIT>Computes mean across the DataFrame . Args : axis ( int ) : The axis to take the mean on . skipna ( bool ) : True to skip NA values false otherwise . Returns : The mean of the DataFrame . ( Pandas series )<CODESPLIT>def mean ( self , axis = None , skipna = None , level = None , numeric_only = None , * * kwargs ) : axis = self . _get_axis_number ( axis ) if axis is not None else 0 data = self . _validate_dtypes_sum_prod_mean ( axis , numeric_only , ignore_axis = False ) return data . _reduce_dimension ( data . _query_compiler . mean ( axis = axis , skipna = skipna , level = level , numeric_only = numeric_only , * * kwargs ) )

1<CODESPLIT>https://github.com/lambdamusic/Ontospy/blob/eb46cb13792b2b87f21babdf976996318eec7571/ontospy/extras/shell_lib.py#L836-L881<CODESPLIT>Shell.do_get<CODESPLIT>Finds entities matching a given string pattern . \ nOptions : [ ontologies | classes | properties | concepts ]<CODESPLIT>def do_get ( self , line ) : line = line . split ( ) _pattern = "" if len ( line ) > 1 : _pattern = line [ 1 ] opts = self . GET_OPTS if ( not line ) or ( line [ 0 ] not in opts ) or ( not _pattern ) : self . help_get ( ) return # self._print("Usage: get [%s] <name>" % "|".join([x for x in opts])) elif line [ 0 ] == "ontology" : if not self . all_ontologies : self . _help_nofiles ( ) else : self . _select_ontology ( _pattern ) elif line [ 0 ] in opts and not self . current : self . _help_noontology ( ) return elif line [ 0 ] == "class" : g = self . current [ 'graph' ] if g . all_classes : self . _select_class ( _pattern ) else : self . _print ( "No classes available." ) elif line [ 0 ] == "property" : g = self . current [ 'graph' ] if g . all_properties : self . _select_property ( _pattern ) else : self . _print ( "No properties available." ) elif line [ 0 ] == "concept" : g = self . current [ 'graph' ] if g . all_skos_concepts : self . _select_concept ( _pattern ) else : self . _print ( "No concepts available." ) else : # should never arrive here pass

1<CODESPLIT>https://github.com/e-dard/flask-s3/blob/b8c72b40eb38a05135eec36a90f1ee0c96248f72/flask_s3.py#L174-L204<CODESPLIT>_gather_files<CODESPLIT>Gets all files in static folders and returns in dict .<CODESPLIT>def _gather_files ( app , hidden , filepath_filter_regex = None ) : dirs = [ ( six . text_type ( app . static_folder ) , app . static_url_path ) ] if hasattr ( app , 'blueprints' ) : blueprints = app . blueprints . values ( ) bp_details = lambda x : ( x . static_folder , _bp_static_url ( x ) ) dirs . extend ( [ bp_details ( x ) for x in blueprints if x . static_folder ] ) valid_files = defaultdict ( list ) for static_folder , static_url_loc in dirs : if not os . path . isdir ( static_folder ) : logger . warning ( "WARNING - [%s does not exist]" % static_folder ) else : logger . debug ( "Checking static folder: %s" % static_folder ) for root , _ , files in os . walk ( static_folder ) : relative_folder = re . sub ( r'^/' , '' , root . replace ( static_folder , '' ) ) files = [ os . path . join ( root , x ) for x in files if ( ( hidden or x [ 0 ] != '.' ) and # Skip this file if the filter regex is # defined, and this file's path is a # negative match. ( filepath_filter_regex == None or re . search ( filepath_filter_regex , os . path . join ( relative_folder , x ) ) ) ) ] if files : valid_files [ ( static_folder , static_url_loc ) ] . extend ( files ) return valid_files

0<CODESPLIT>https://github.com/inveniosoftware/invenio-oauthclient/blob/2500dc6935738107617aeade79e050d7608004bb/invenio_oauthclient/handlers.py#L202-L211<CODESPLIT>token_getter<CODESPLIT>Remove OAuth access tokens from session .<CODESPLIT>def token_getter ( remote , token = '' ) : session_key = token_session_key ( remote . name ) if session_key not in session and current_user . is_authenticated : # Fetch key from token store if user is authenticated, and the key # isn't already cached in the session. remote_token = RemoteToken . get ( current_user . get_id ( ) , remote . consumer_key , token_type = token , ) if remote_token is None : return None # Store token and secret in session session [ session_key ] = remote_token . token ( ) return session . get ( session_key , None )

0<CODESPLIT>https://github.com/softlayer/softlayer-python/blob/9f181be08cc3668353b05a6de0cb324f52cff6fa/SoftLayer/managers/user.py#L215-L238<CODESPLIT>UserManager._get_id_from_username<CODESPLIT>Formats a list of permission key names into something the SLAPI will respect .<CODESPLIT>def _get_id_from_username ( self , username ) : _mask = "mask[id, username]" _filter = { 'users' : { 'username' : utils . query_filter ( username ) } } user = self . list_users ( _mask , _filter ) if len ( user ) == 1 : return [ user [ 0 ] [ 'id' ] ] elif len ( user ) > 1 : raise exceptions . SoftLayerError ( "Multiple users found with the name: %s" % username ) else : raise exceptions . SoftLayerError ( "Unable to find user id for %s" % username )

0<CODESPLIT>https://github.com/PmagPy/PmagPy/blob/c7984f8809bf40fe112e53dcc311a33293b62d0b/programs/demag_gui.py#L8394-L8420<CODESPLIT>Demag_GUI.on_save_interpretation_button<CODESPLIT>Allows the entering of new fit names in the fit combobox<CODESPLIT>def on_save_interpretation_button ( self , event ) : if self . current_fit : self . current_fit . saved = True calculation_type = self . current_fit . get ( self . COORDINATE_SYSTEM ) [ 'calculation_type' ] tmin = str ( self . tmin_box . GetValue ( ) ) tmax = str ( self . tmax_box . GetValue ( ) ) self . current_fit . put ( self . s , 'specimen' , self . get_PCA_parameters ( self . s , self . current_fit , tmin , tmax , 'specimen' , calculation_type ) ) if len ( self . Data [ self . s ] [ 'zijdblock_geo' ] ) > 0 : self . current_fit . put ( self . s , 'geographic' , self . get_PCA_parameters ( self . s , self . current_fit , tmin , tmax , 'geographic' , calculation_type ) ) if len ( self . Data [ self . s ] [ 'zijdblock_tilt' ] ) > 0 : self . current_fit . put ( self . s , 'tilt-corrected' , self . get_PCA_parameters ( self . s , self . current_fit , tmin , tmax , 'tilt-corrected' , calculation_type ) ) # calculate high level data self . calculate_high_levels_data ( ) self . plot_high_levels_data ( ) self . on_menu_save_interpretation ( event ) self . update_selection ( ) self . close_warning = True

0<CODESPLIT>https://github.com/qubole/qds-sdk-py/blob/77210fb64e5a7d567aedeea3b742a1d872fd0e5e/qds_sdk/cluster.py#L628-L641<CODESPLIT>Cluster._parse_update_snapshot_schedule<CODESPLIT>Parse command line arguments for updating hbase snapshot schedule or to get details .<CODESPLIT>def _parse_update_snapshot_schedule ( cls , args ) : argparser = ArgumentParser ( prog = "cluster snapshot_schedule" ) group = argparser . add_mutually_exclusive_group ( required = True ) group . add_argument ( "--id" , dest = "cluster_id" , help = "execute on cluster with this id" ) group . add_argument ( "--label" , dest = "label" , help = "execute on cluster with this label" ) argparser . add_argument ( "--frequency-num" , help = "frequency number" ) argparser . add_argument ( "--frequency-unit" , help = "frequency unit" ) argparser . add_argument ( "--s3-location" , help = "s3_location about where to store snapshots" ) argparser . add_argument ( "--status" , help = "status of periodic job you want to change to" , choices = [ "RUNNING" , "SUSPENDED" ] ) arguments = argparser . parse_args ( args ) return arguments

0<CODESPLIT>https://github.com/mozilla/DeepSpeech/blob/f64aa73e7fbe9dde40d4fcf23b42ab304747d152/examples/mic_vad_streaming/mic_vad_streaming.py#L103-L110<CODESPLIT>VADAudio.vad_collector<CODESPLIT>Generator that yields all audio frames from microphone .<CODESPLIT>def vad_collector ( self , padding_ms = 300 , ratio = 0.75 , frames = None ) : if frames is None : frames = self . frame_generator ( ) num_padding_frames = padding_ms // self . frame_duration_ms ring_buffer = collections . deque ( maxlen = num_padding_frames ) triggered = False for frame in frames : is_speech = self . vad . is_speech ( frame , self . sample_rate ) if not triggered : ring_buffer . append ( ( frame , is_speech ) ) num_voiced = len ( [ f for f , speech in ring_buffer if speech ] ) if num_voiced > ratio * ring_buffer . maxlen : triggered = True for f , s in ring_buffer : yield f ring_buffer . clear ( ) else : yield frame ring_buffer . append ( ( frame , is_speech ) ) num_unvoiced = len ( [ f for f , speech in ring_buffer if not speech ] ) if num_unvoiced > ratio * ring_buffer . maxlen : triggered = False yield None ring_buffer . clear ( )

0<CODESPLIT>https://github.com/ska-sa/montblanc/blob/8a2e742e7500bcc6196489b735f87b233075dd2d/montblanc/impl/rime/tensorflow/sources/ms_source_provider.py#L188-L197<CODESPLIT>MSSourceProvider.flag<CODESPLIT>Weight data source<CODESPLIT>def flag ( self , context ) : lrow , urow = MS . row_extents ( context ) flag = self . _manager . ordered_main_table . getcol ( MS . FLAG , startrow = lrow , nrow = urow - lrow ) return flag . reshape ( context . shape ) . astype ( context . dtype )

1<CODESPLIT>https://github.com/deschler/django-modeltranslation/blob/18fec04a5105cbd83fc3759f4fda20135b3a848c/modeltranslation/manager.py#L357-L362<CODESPLIT>MultilingualQuerySet.create<CODESPLIT>Allows to override population mode with a populate method .<CODESPLIT>def create ( self , * * kwargs ) : with auto_populate ( self . _populate_mode ) : return super ( MultilingualQuerySet , self ) . create ( * * kwargs )

1<CODESPLIT>https://github.com/NLeSC/noodles/blob/3759e24e6e54a3a1a364431309dbb1061f617c04/noodles/lib/streams.py#L259-L275<CODESPLIT>patch<CODESPLIT>Create a direct link between a source and a sink .<CODESPLIT>def patch ( source , sink ) : sink = sink ( ) for v in source ( ) : try : sink . send ( v ) except StopIteration : return

1<CODESPLIT>https://github.com/shichao-an/twitter-photos/blob/32de6e8805edcbb431d08af861e9d2f0ab221106/twphotos/increment.py#L19-L31<CODESPLIT>read_since_ids<CODESPLIT>Read max ids of the last downloads<CODESPLIT>def read_since_ids ( users ) : since_ids = { } for user in users : if config . has_option ( SECTIONS [ 'INCREMENTS' ] , user ) : since_ids [ user ] = config . getint ( SECTIONS [ 'INCREMENTS' ] , user ) + 1 return since_ids

1<CODESPLIT>https://github.com/bartromgens/geojsoncontour/blob/79e30718fa0c1d96a2459eb1f45d06d699d240ed/geojsoncontour/utilities/multipoly.py#L46-L59<CODESPLIT>keep_high_angle<CODESPLIT>Keep vertices with angles higher then given minimum .<CODESPLIT>def keep_high_angle ( vertices , min_angle_deg ) : accepted = [ ] v = vertices v1 = v [ 1 ] - v [ 0 ] accepted . append ( ( v [ 0 ] [ 0 ] , v [ 0 ] [ 1 ] ) ) for i in range ( 1 , len ( v ) - 2 ) : v2 = v [ i + 1 ] - v [ i - 1 ] diff_angle = np . fabs ( angle ( v1 , v2 ) * 180.0 / np . pi ) if diff_angle > min_angle_deg : accepted . append ( ( v [ i ] [ 0 ] , v [ i ] [ 1 ] ) ) v1 = v [ i ] - v [ i - 1 ] accepted . append ( ( v [ - 1 ] [ 0 ] , v [ - 1 ] [ 1 ] ) ) return np . array ( accepted , dtype = vertices . dtype )

1<CODESPLIT>https://github.com/bspaans/python-mingus/blob/aa5a5d992d45ada61be0f9f86261380731bd7749/mingus/containers/note.py#L184-L194<CODESPLIT>Note.from_hertz<CODESPLIT>Set the Note name and pitch calculated from the hertz value .<CODESPLIT>def from_hertz ( self , hertz , standard_pitch = 440 ) : value = ( ( log ( ( float ( hertz ) * 1024 ) / standard_pitch , 2 ) + 1.0 / 24 ) * 12 + 9 ) # notes.note_to_int("A") self . name = notes . int_to_note ( int ( value ) % 12 ) self . octave = int ( value / 12 ) - 6 return self

0<CODESPLIT>https://github.com/yamins81/tabular/blob/1caf091c8c395960a9ad7078f95158b533cc52dd/tabular/tab.py#L785-L802<CODESPLIT>tabarray.pivot<CODESPLIT>Aggregate a tabarray and include original data in the result .<CODESPLIT>def pivot ( self , a , b , Keep = None , NullVals = None , order = None , prefix = '_' ) : [ data , coloring ] = spreadsheet . pivot ( X = self , a = a , b = b , Keep = Keep , NullVals = NullVals , order = order , prefix = prefix ) data = data . view ( tabarray ) data . coloring = coloring return data

1<CODESPLIT>https://github.com/tchellomello/python-arlo/blob/db70aeb81705309c56ad32bbab1094f6cd146524/pyarlo/camera.py#L177-L193<CODESPLIT>ArloCamera.videos<CODESPLIT>Return all <ArloVideo > objects from camera given days range<CODESPLIT>def videos ( self , days = None ) : if days is None : days = self . _min_days_vdo_cache library = ArloMediaLibrary ( self . _session , preload = False ) try : return library . load ( only_cameras = [ self ] , days = days ) except ( AttributeError , IndexError ) : # make sure we are returning an empty list istead of None # returning an empty list, cache will be forced only when calling # the update method. Changing this can impact badly # in the Home Assistant performance return [ ]

0<CODESPLIT>https://github.com/ArangoDB-Community/pyArango/blob/dd72e5f6c540e5e148943d615ddf7553bb78ce0b/pyArango/document.py#L103-L120<CODESPLIT>DocumentStore.validate<CODESPLIT>Set the store using a dictionary<CODESPLIT>def validate ( self ) : if not self . mustValidate : return True res = { } for field in self . validators . keys ( ) : try : if isinstance ( self . validators [ field ] , dict ) and field not in self . store : self . store [ field ] = DocumentStore ( self . collection , validators = self . validators [ field ] , initDct = { } , subStore = True , validateInit = self . validateInit ) self . validateField ( field ) except InvalidDocument as e : res . update ( e . errors ) except ( ValidationError , SchemaViolation ) as e : res [ field ] = str ( e ) if len ( res ) > 0 : raise InvalidDocument ( res ) return True

1<CODESPLIT>https://github.com/dragnet-org/dragnet/blob/532c9d9f28e5b1b57f3cabc708218d3863a16322/dragnet/compat.py#L138-L173<CODESPLIT>str_block_cast<CODESPLIT>Converts any bytes - like items in input Block object to string - like values with respect to python version<CODESPLIT>def str_block_cast ( block , include_text = True , include_link_tokens = True , include_css = True , include_features = True , * * kwargs ) : if include_text : block . text = str_cast ( block . text , * * kwargs ) if include_link_tokens : block . link_tokens = str_list_cast ( block . link_tokens , * * kwargs ) if include_css : block . css = str_dict_cast ( block . css , * * kwargs ) if include_features : block . features = str_dict_cast ( block . features , * * kwargs ) return block

1<CODESPLIT>https://github.com/desbma/sacad/blob/a7a010c4d9618a0c90927f1acb530101ca05fac4/sacad/http_helpers.py#L15-L20<CODESPLIT>aiohttp_socket_timeout<CODESPLIT>Return a aiohttp . ClientTimeout object with only socket timeouts set .<CODESPLIT>def aiohttp_socket_timeout ( socket_timeout_s ) : return aiohttp . ClientTimeout ( total = None , connect = None , sock_connect = socket_timeout_s , sock_read = socket_timeout_s )

0<CODESPLIT>https://github.com/roboogle/gtkmvc3/blob/63405fd8d2056be26af49103b13a8d5e57fe4dff/gtkmvco/examples/niew/view.py#L71-L79<CODESPLIT>Widget.reparent<CODESPLIT>This is deprecated . Pass your controller to connect signals the old way .<CODESPLIT>def reparent ( self , other , name ) : # http://developer.gnome.org/gtk-faq/stable/x635.html # warns against reparent() old = self . toplevel . get_parent ( ) if old : old . remove ( self . toplevel ) new = other [ name ] new . add ( self . toplevel )

1<CODESPLIT>https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/learning/objects.py#L1270-L1288<CODESPLIT>ProficiencyForm.set_completion<CODESPLIT>Sets the completion percentage .<CODESPLIT>def set_completion ( self , completion ) : # Implemented from template for osid.grading.GradeSystemForm.set_lowest_numeric_score if self . get_completion_metadata ( ) . is_read_only ( ) : raise errors . NoAccess ( ) try : completion = float ( completion ) except ValueError : raise errors . InvalidArgument ( ) if not self . _is_valid_decimal ( completion , self . get_completion_metadata ( ) ) : raise errors . InvalidArgument ( ) self . _my_map [ 'completion' ] = completion

1<CODESPLIT>https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/clf/rcs956.py#L149-L153<CODESPLIT>Chipset.reset_mode<CODESPLIT>Send a Reset command to set the operation mode to 0 .<CODESPLIT>def reset_mode ( self ) : self . command ( 0x18 , b"\x01" , timeout = 0.1 ) self . transport . write ( Chipset . ACK ) time . sleep ( 0.010 )

0<CODESPLIT>https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/contrib/redshift.py#L293-L357<CODESPLIT>S3CopyToTable.run<CODESPLIT>Override to provide code for creating the target table .<CODESPLIT>def run ( self ) : if not ( self . table ) : raise Exception ( "table need to be specified" ) path = self . s3_load_path ( ) output = self . output ( ) connection = output . connect ( ) cursor = connection . cursor ( ) self . init_copy ( connection ) self . copy ( cursor , path ) self . post_copy ( cursor ) if self . enable_metadata_columns : self . post_copy_metacolumns ( cursor ) # update marker table output . touch ( connection ) connection . commit ( ) # commit and clean up connection . close ( )

0<CODESPLIT>https://github.com/tchellomello/python-arlo/blob/db70aeb81705309c56ad32bbab1094f6cd146524/pyarlo/base_station.py#L362-L369<CODESPLIT>ArloBaseStation.get_available_modes<CODESPLIT>Returns True if base_station is currently on a scheduled mode .<CODESPLIT>def get_available_modes ( self ) : resource = "modes" resource_event = self . publish_and_get_event ( resource ) if resource_event : properties = resource_event . get ( "properties" ) return properties . get ( "modes" ) return None

1<CODESPLIT>https://github.com/unitedstack/steth/blob/955884ceebf3bdc474c93cc5cf555e67d16458f1/stetho/agent/api.py#L27-L48<CODESPLIT>AgentApi.check_ports_on_br<CODESPLIT>Check ports exist on bridge .<CODESPLIT>def check_ports_on_br ( self , bridge = 'br-ex' , ports = [ 'eth3' ] ) : LOG . info ( "RPC: check_ports_on_br bridge: %s, ports: %s" % ( bridge , ports ) ) cmd = [ 'ovs-vsctl' , 'list-ports' , bridge ] stdcode , stdout = agent_utils . execute ( cmd , root = True ) data = dict ( ) if stdcode == 0 : for port in ports : if port in stdout : data [ port ] = True stdout . remove ( port ) else : data [ port ] = False return agent_utils . make_response ( code = stdcode , data = data ) # execute failed. message = stdout . pop ( 0 ) return agent_utils . make_response ( code = stdcode , message = message )

0<CODESPLIT>https://github.com/log2timeline/dfvfs/blob/2b3ccd115f9901d89f383397d4a1376a873c83c4/dfvfs/vfs/tsk_file_entry.py#L249-L349<CODESPLIT>TSKFileEntry._GetAttributes<CODESPLIT>Retrieves directory entries .<CODESPLIT>def _GetAttributes ( self ) : if self . _attributes is None : self . _attributes = [ ] for tsk_attribute in self . _tsk_file : if getattr ( tsk_attribute , 'info' , None ) is None : continue # At the moment there is no way to expose the attribute data # from pytsk3. attribute_object = TSKAttribute ( tsk_attribute ) self . _attributes . append ( attribute_object ) return self . _attributes

0<CODESPLIT>https://github.com/django-admin-bootstrapped/django-admin-bootstrapped/blob/30380f20cc82e898754e94b5d12cacafabca01bd/django_admin_bootstrapped/templatetags/bootstrapped_goodies_tags.py#L116-L123<CODESPLIT>DataSource.filenames<CODESPLIT>Wrapper for rendering the field via an external renderer<CODESPLIT>def filenames ( self ) : if self . _is_reader : assert self . _filenames is not None return self . _filenames else : return self . data_producer . filenames

1<CODESPLIT>https://github.com/dmlc/gluon-nlp/blob/4b83eb6bcc8881e5f1081a3675adaa19fac5c0ba/scripts/word_embeddings/data.py#L334-L343<CODESPLIT>skipgram_fasttext_batch<CODESPLIT>Create a batch for SG training objective with subwords .<CODESPLIT>def skipgram_fasttext_batch ( centers , contexts , num_tokens , subword_lookup , dtype , index_dtype ) : contexts = mx . nd . array ( contexts [ 2 ] , dtype = index_dtype ) data , row , col = subword_lookup ( centers ) centers = mx . nd . array ( centers , dtype = index_dtype ) centers_csr = mx . nd . sparse . csr_matrix ( ( data , ( row , col ) ) , dtype = dtype , shape = ( len ( centers ) , num_tokens ) ) # yapf: disable return centers_csr , contexts , centers

1<CODESPLIT>https://github.com/HarveyHunt/i3situation/blob/3160a21006fcc6961f240988874e228a5ec6f18e/i3situation/plugins/_plugin.py#L43-L61<CODESPLIT>Plugin.on_click<CODESPLIT>A function that should be overwritten by a plugin that wishes to react to events if it wants to perform any action other than running the supplied command related to a button .<CODESPLIT>def on_click ( self , event ) : if event [ 'button' ] == 1 and 'button1' in self . options : subprocess . call ( self . options [ 'button1' ] . split ( ) ) elif event [ 'button' ] == 2 and 'button2' in self . options : subprocess . call ( self . options [ 'button2' ] . split ( ) ) elif event [ 'button' ] == 3 and 'button3' in self . options : subprocess . call ( self . options [ 'button3' ] . split ( ) )

0<CODESPLIT>https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/environment.py#L190-L210<CODESPLIT>Environment.sys_prefix<CODESPLIT>The system path inside the environment<CODESPLIT>def sys_prefix ( self ) : command = [ self . python , "-c" "import sys; print(sys.prefix)" ] c = vistir . misc . run ( command , return_object = True , block = True , nospin = True , write_to_stdout = False ) sys_prefix = vistir . compat . Path ( vistir . misc . to_text ( c . out ) . strip ( ) ) . as_posix ( ) return sys_prefix

0<CODESPLIT>https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/variation/cortex.py#L122-L157<CODESPLIT>_combine_variants<CODESPLIT>Run cortex on a specified chromosome start / end region .<CODESPLIT>def _combine_variants ( in_vcfs , out_file , ref_file , config ) : in_vcfs . sort ( ) wrote_header = False with open ( out_file , "w" ) as out_handle : for in_vcf in ( x [ - 1 ] for x in in_vcfs ) : with open ( in_vcf ) as in_handle : header = list ( itertools . takewhile ( lambda x : x . startswith ( "#" ) , in_handle ) ) if not header [ 0 ] . startswith ( "##fileformat=VCFv4" ) : raise ValueError ( "Unexpected VCF file: %s" % in_vcf ) for line in in_handle : if not wrote_header : wrote_header = True out_handle . write ( "" . join ( header ) ) out_handle . write ( line ) if not wrote_header : out_handle . write ( "" . join ( header ) ) return out_file

0<CODESPLIT>https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/basicpar.py#L722-L752<CODESPLIT>IrafPar._getField<CODESPLIT>Set a parameter field value<CODESPLIT>def _getField ( self , field , native = 0 , prompt = 1 ) : try : # expand field name using minimum match field = _getFieldDict [ field ] except KeyError as e : # re-raise the exception with a bit more info raise SyntaxError ( "Cannot get field " + field + " for parameter " + self . name + "\n" + str ( e ) ) if field == "p_value" : # return value of parameter # Note that IRAF returns the filename for list parameters # when p_value is used.  I consider this a bug, and it does # not appear to be used by any cl scripts or SPP programs # in either IRAF or STSDAS.  It is also in conflict with # the IRAF help documentation.  I am making p_value exactly # the same as just a simple CL parameter reference. return self . get ( native = native , prompt = prompt ) elif field == "p_name" : return self . name elif field == "p_xtype" : return self . type elif field == "p_type" : return self . _getPType ( ) elif field == "p_mode" : return self . mode elif field == "p_prompt" : return self . prompt elif field == "p_scope" : return self . scope elif field == "p_default" or field == "p_filename" : # these all appear to be equivalent -- they just return the # current PFilename of the parameter (which is the same as the value # for non-list parameters, and is the filename for list parameters) return self . _getPFilename ( native , prompt ) elif field == "p_maximum" : if native : return self . max else : return self . toString ( self . max ) elif field == "p_minimum" : if self . choice is not None : if native : return self . choice else : schoice = list ( map ( self . toString , self . choice ) ) return "|" + "|" . join ( schoice ) + "|" else : if native : return self . min else : return self . toString ( self . min ) else : # XXX unimplemented fields: # p_length: maximum string length in bytes -- what to do with it? raise RuntimeError ( "Program bug in IrafPar._getField()\n" + "Requested field " + field + " for parameter " + self . name )

1<CODESPLIT>https://github.com/jobovy/galpy/blob/9c5b9fe65d58835624dffe432be282060918ee08/galpy/util/__init__.py#L105-L125<CODESPLIT>fast_cholesky_invert<CODESPLIT>NAME : fast_cholesky_invert PURPOSE : invert a positive definite matrix by using its Cholesky decomposition INPUT : A - matrix to be inverted logdet - ( Bool ) if True return the logarithm of the determinant as well tiny - ( double ) tiny number to add to the covariance matrix to make the decomposition stable ( has a default ) OUTPUT : A^ { - 1 } REVISION HISTORY : 2009 - 10 - 07 - Written - Bovy ( NYU )<CODESPLIT>def fast_cholesky_invert ( A , logdet = False , tiny = _TINY ) : L = stable_cho_factor ( A , tiny = tiny ) if logdet : return ( linalg . cho_solve ( L , numpy . eye ( A . shape [ 0 ] ) ) , 2. * numpy . sum ( numpy . log ( numpy . diag ( L [ 0 ] ) ) ) ) else : return linalg . cho_solve ( L , numpy . eye ( A . shape [ 0 ] ) )

0<CODESPLIT>https://github.com/booktype/python-ooxml/blob/b56990a5bee2e1bc46839cec5161ff3726dc4d87/ooxml/serialize.py#L74-L85<CODESPLIT>_get_font_size<CODESPLIT>Returns type for the list .<CODESPLIT>def _get_font_size ( document , style ) : font_size = style . get_font_size ( ) if font_size == - 1 : if style . based_on : based_on = document . styles . get_by_id ( style . based_on ) if based_on : return _get_font_size ( document , based_on ) return font_size

0<CODESPLIT>https://github.com/thombashi/SimpleSQLite/blob/b16f212132b9b98773e68bf7395abc2f60f56fe5/simplesqlite/core.py#L625-L641<CODESPLIT>SimpleSQLite.fetch_value<CODESPLIT>Send a DELETE query to the database .<CODESPLIT>def fetch_value ( self , select , table_name , where = None , extra = None ) : try : self . verify_table_existence ( table_name ) except TableNotFoundError as e : logger . debug ( e ) return None result = self . execute_query ( Select ( select , table_name , where , extra ) , logging . getLogger ( ) . findCaller ( ) ) if result is None : return None fetch = result . fetchone ( ) if fetch is None : return None return fetch [ 0 ]

0<CODESPLIT>https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/ndarray/sparse.py#L978-L1017<CODESPLIT>csr_matrix<CODESPLIT>Create a CSRNDArray based on data indices and indptr<CODESPLIT>def csr_matrix ( arg1 , shape = None , ctx = None , dtype = None ) : # construct a csr matrix from (M, N) or (data, indices, indptr) if isinstance ( arg1 , tuple ) : arg_len = len ( arg1 ) if arg_len == 2 : # construct a sparse csr matrix from # scipy coo matrix if input format is coo if isinstance ( arg1 [ 1 ] , tuple ) and len ( arg1 [ 1 ] ) == 2 : data , ( row , col ) = arg1 if isinstance ( data , NDArray ) : data = data . asnumpy ( ) if isinstance ( row , NDArray ) : row = row . asnumpy ( ) if isinstance ( col , NDArray ) : col = col . asnumpy ( ) coo = spsp . coo_matrix ( ( data , ( row , col ) ) , shape = shape ) _check_shape ( coo . shape , shape ) csr = coo . tocsr ( ) return array ( csr , ctx = ctx , dtype = dtype ) else : # empty matrix with shape _check_shape ( arg1 , shape ) return empty ( 'csr' , arg1 , ctx = ctx , dtype = dtype ) elif arg_len == 3 : # data, indices, indptr return _csr_matrix_from_definition ( arg1 [ 0 ] , arg1 [ 1 ] , arg1 [ 2 ] , shape = shape , ctx = ctx , dtype = dtype ) else : raise ValueError ( "Unexpected length of input tuple: " + str ( arg_len ) ) else : # construct a csr matrix from a sparse / dense one if isinstance ( arg1 , CSRNDArray ) or ( spsp and isinstance ( arg1 , spsp . csr . csr_matrix ) ) : # construct a csr matrix from scipy or CSRNDArray _check_shape ( arg1 . shape , shape ) return array ( arg1 , ctx = ctx , dtype = dtype ) elif isinstance ( arg1 , RowSparseNDArray ) : raise ValueError ( "Unexpected input type: RowSparseNDArray" ) else : # construct a csr matrix from a dense one # prepare default ctx and dtype since mx.nd.array doesn't use default values # based on source_array dtype = _prepare_default_dtype ( arg1 , dtype ) # create dns array with provided dtype. ctx is not passed since copy across # ctx requires dtype to be the same dns = _array ( arg1 , dtype = dtype ) if ctx is not None and dns . context != ctx : dns = dns . as_in_context ( ctx ) _check_shape ( dns . shape , shape ) return dns . tostype ( 'csr' )

0<CODESPLIT>https://github.com/Gjum/agarnet/blob/63365ba32aa31c23a6d61438b556ceb8ed65631f/agarnet/gcommer.py#L9-L29<CODESPLIT>gcommer_donate<CODESPLIT>Try to get a token for this server address . address has to be ip : port e . g . 1 . 2 . 3 . 4 : 1234 Returns tuple ( address token )<CODESPLIT>def gcommer_donate ( address , token , * _ ) : token = urllib . request . quote ( token ) url = 'http://at.gcommer.com/donate?server=%s&token=%s' % ( address , token ) response = urllib . request . urlopen ( url ) . read ( ) . decode ( ) return json . loads ( response ) [ 'msg' ]

0<CODESPLIT>https://github.com/woolfson-group/isambard/blob/ebc33b48a28ad217e18f93b910dfba46e6e71e07/isambard/ampal/specifications/polymer_specs/helix.py#L189-L223<CODESPLIT>Helix.from_start_and_end<CODESPLIT>Build straight helix along z - axis starting with CA1 on x - axis<CODESPLIT>def from_start_and_end ( cls , start , end , aa = None , helix_type = 'alpha' ) : start = numpy . array ( start ) end = numpy . array ( end ) if aa is None : rise_per_residue = _helix_parameters [ helix_type ] [ 1 ] aa = int ( ( numpy . linalg . norm ( end - start ) / rise_per_residue ) + 1 ) instance = cls ( aa = aa , helix_type = helix_type ) instance . move_to ( start = start , end = end ) return instance

1<CODESPLIT>https://github.com/manahl/arctic/blob/57e110b6e182dbab00e7e214dc26f7d9ec47c120/arctic/arctic.py#L216-L229<CODESPLIT>Arctic._list_libraries_cached<CODESPLIT>Returns ------- List of Arctic library names from a cached collection ( global per mongo cluster ) in mongo . Long term list_libraries should have a use_cached argument .<CODESPLIT>def _list_libraries_cached ( self , newer_than_secs = - 1 ) : _ = self . _conn # Ensures the connection exists and cache is initialized with it. cache_data = self . _cache . get ( 'list_libraries' , newer_than_secs ) if cache_data : logger . debug ( 'Library names are in cache.' ) return cache_data return self . _list_libraries ( )

0<CODESPLIT>https://github.com/wandb/client/blob/7d08954ed5674fee223cd85ed0d8518fe47266b2/wandb/vendor/prompt_toolkit/buffer.py#L1349-L1370<CODESPLIT>reshape_text<CODESPLIT>Unindent text of a : class : . Buffer object .<CODESPLIT>def reshape_text ( buffer , from_row , to_row ) : lines = buffer . text . splitlines ( True ) lines_before = lines [ : from_row ] lines_after = lines [ to_row + 1 : ] lines_to_reformat = lines [ from_row : to_row + 1 ] if lines_to_reformat : # Take indentation from the first line. length = re . search ( r'^\s*' , lines_to_reformat [ 0 ] ) . end ( ) indent = lines_to_reformat [ 0 ] [ : length ] . replace ( '\n' , '' ) # Now, take all the 'words' from the lines to be reshaped. words = '' . join ( lines_to_reformat ) . split ( ) # And reshape. width = ( buffer . text_width or 80 ) - len ( indent ) reshaped_text = [ indent ] current_width = 0 for w in words : if current_width : if len ( w ) + current_width + 1 > width : reshaped_text . append ( '\n' ) reshaped_text . append ( indent ) current_width = 0 else : reshaped_text . append ( ' ' ) current_width += 1 reshaped_text . append ( w ) current_width += len ( w ) if reshaped_text [ - 1 ] != '\n' : reshaped_text . append ( '\n' ) # Apply result. buffer . document = Document ( text = '' . join ( lines_before + reshaped_text + lines_after ) , cursor_position = len ( '' . join ( lines_before + reshaped_text ) ) )

1<CODESPLIT>https://github.com/edx/XBlock/blob/368bf46e2c0ee69bbb21817f428c4684936e18ee/xblock/fields.py#L799-L809<CODESPLIT>Dict.to_string<CODESPLIT>In python3 json . dumps () cannot sort keys of different types so preconvert None to null .<CODESPLIT>def to_string ( self , value ) : self . enforce_type ( value ) if isinstance ( value , dict ) and None in value : value = value . copy ( ) value [ 'null' ] = value [ None ] del value [ None ] return super ( Dict , self ) . to_string ( value )

1<CODESPLIT>https://github.com/JdeRobot/base/blob/303b18992785b2fe802212f2d758a60873007f1f/src/drivers/MAVLinkServer/MAVProxy/pymavlink/dialects/v10/matrixpilot.py#L12954-L12973<CODESPLIT>MAVLink.adsb_vehicle_send<CODESPLIT>The location and information of an ADSB vehicle<CODESPLIT>def adsb_vehicle_send ( self , ICAO_address , lat , lon , altitude_type , altitude , heading , hor_velocity , ver_velocity , callsign , emitter_type , tslc , flags , squawk , force_mavlink1 = False ) : return self . send ( self . adsb_vehicle_encode ( ICAO_address , lat , lon , altitude_type , altitude , heading , hor_velocity , ver_velocity , callsign , emitter_type , tslc , flags , squawk ) , force_mavlink1 = force_mavlink1 )

0<CODESPLIT>https://github.com/marshmallow-code/webargs/blob/40cc2d25421d15d9630b1a819f1dcefbbf01ed95/src/webargs/tornadoparser.py#L34-L47<CODESPLIT>Parser.handle_error<CODESPLIT>Return the decoded JSON body from the request .<CODESPLIT>def handle_error ( self , error , req , schema , error_status_code = None , error_headers = None ) : logger . error ( error ) raise error

1<CODESPLIT>https://github.com/qiniu/python-sdk/blob/a69fbef4e3e6ea1ebe09f4610a5b18bb2c17de59/qiniu/services/compute/app.py#L43-L53<CODESPLIT>AccountClient.get_qcos_client<CODESPLIT>获得资源管理客户端 缓存，但不是线程安全的<CODESPLIT>def get_qcos_client ( self , app_uri ) : client = self . qcos_clients . get ( app_uri ) if ( client is None ) : client = self . create_qcos_client ( app_uri ) self . qcos_clients [ app_uri ] = client return client

0<CODESPLIT>https://github.com/marcomusy/vtkplotter/blob/692c3396782722ec525bc1346a26999868c650c6/vtkplotter/plotter.py#L515-L547<CODESPLIT>plotMatrix<CODESPLIT>Returns a vtkActor from reading a file directory or vtkPolyData .<CODESPLIT>def plotMatrix ( M , title = 'matrix' , continuous = True , cmap = 'Greys' ) : import matplotlib . pyplot as plt import matplotlib as mpl from mpl_toolkits . axes_grid1 import make_axes_locatable M = numpy . array ( M ) m , n = numpy . shape ( M ) M = M . round ( decimals = 2 ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) cmap = mpl . cm . get_cmap ( cmap ) if not continuous : unq = numpy . unique ( M ) im = ax . imshow ( M , cmap = cmap , interpolation = 'None' ) divider = make_axes_locatable ( ax ) cax = divider . append_axes ( "right" , size = "5%" , pad = 0.05 ) dim = r'$%i \times %i$ ' % ( m , n ) ax . set_title ( dim + title ) ax . axis ( 'off' ) cb = plt . colorbar ( im , cax = cax ) if not continuous : cb . set_ticks ( unq ) cb . set_ticklabels ( unq ) plt . show ( )

0<CODESPLIT>https://github.com/serhatbolsu/robotframework-appiumlibrary/blob/91c808cf0602af6be8135ac529fa488fded04a85/AppiumLibrary/keywords/_android_utils.py#L130-L146<CODESPLIT>_AndroidUtilsKeywords.start_activity<CODESPLIT>Wait for an activity : block until target activity presents or time out .<CODESPLIT>def start_activity ( self , appPackage , appActivity , * * opts ) : # Almost the same code as in appium's start activity, # just to keep the same keyword names as in open application arguments = { 'app_wait_package' : 'appWaitPackage' , 'app_wait_activity' : 'appWaitActivity' , 'intent_action' : 'intentAction' , 'intent_category' : 'intentCategory' , 'intent_flags' : 'intentFlags' , 'optional_intent_arguments' : 'optionalIntentArguments' , 'stop_app_on_reset' : 'stopAppOnReset' } data = { } for key , value in arguments . items ( ) : if value in opts : data [ key ] = opts [ value ] driver = self . _current_application ( ) driver . start_activity ( app_package = appPackage , app_activity = appActivity , * * data )

0<CODESPLIT>https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/distlib/database.py#L632-L641<CODESPLIT>InstalledDistribution.read_exports<CODESPLIT>Write a dictionary of exports to a file in . ini format . : param exports : A dictionary of exports mapping an export category to a list of : class : ExportEntry instances describing the individual export entries .<CODESPLIT>def read_exports ( self ) : result = { } r = self . get_distinfo_resource ( EXPORTS_FILENAME ) if r : with contextlib . closing ( r . as_stream ( ) ) as stream : result = read_exports ( stream ) return result

0<CODESPLIT>https://github.com/docker/docker-py/blob/613d6aad83acc9931ff2ecfd6a6c7bd8061dc125/docker/models/containers.py#L314-L330<CODESPLIT>Container.logs<CODESPLIT>Insert a file or folder in this container using a tar archive as source .<CODESPLIT>def logs ( self , * * kwargs ) : return self . client . api . logs ( self . id , * * kwargs )

1<CODESPLIT>https://github.com/cslarsen/lyn/blob/d615a6a9473083ffc7318a98fcc697cbc28ba5da/lyn/lyn.py#L100-L159<CODESPLIT>Lightning._set_signatures<CODESPLIT>Sets return and parameter types for the foreign C functions .<CODESPLIT>def _set_signatures ( self ) : # We currently pass structs as void pointers. code_t = ctypes . c_int gpr_t = ctypes . c_int32 int32_t = ctypes . c_int32 node_p = ctypes . c_void_p pointer_t = ctypes . c_void_p state_p = ctypes . c_void_p void = None def sig ( rettype , fname , * ptypes ) : func = getattr ( self . lib , fname ) func . restype = rettype func . argtypes = ptypes sig ( node_p , "_jit_arg" , state_p ) sig ( node_p , "_jit_finishi" , state_p , pointer_t ) sig ( node_p , "_jit_forward" , state_p ) sig ( node_p , "_jit_indirect" , state_p ) sig ( node_p , "_jit_label" , state_p ) sig ( node_p , "_jit_new_node_p" , state_p , code_t , pointer_t ) sig ( node_p , "_jit_new_node_pww" , state_p , code_t , pointer_t , word_t , word_t ) sig ( node_p , "_jit_new_node_qww" , state_p , code_t , int32_t , int32_t , word_t ) sig ( node_p , "_jit_new_node_w" , state_p , code_t , word_t ) sig ( node_p , "_jit_new_node_ww" , state_p , code_t , word_t , word_t ) sig ( node_p , "_jit_new_node_www" , state_p , code_t , word_t , word_t , word_t ) sig ( node_p , "_jit_note" , state_p , char_p , ctypes . c_int ) sig ( pointer_t , "_jit_address" , state_p , node_p ) sig ( pointer_t , "_jit_emit" , state_p ) sig ( state_p , "jit_new_state" ) sig ( void , "_jit_clear_state" , state_p ) sig ( void , "_jit_destroy_state" , state_p ) sig ( void , "_jit_ellipsis" , state_p ) sig ( void , "_jit_epilog" , state_p ) sig ( void , "_jit_finishr" , state_p , gpr_t ) sig ( void , "_jit_getarg_i" , state_p , gpr_t , node_p ) sig ( void , "_jit_getarg_l" , state_p , gpr_t , node_p ) sig ( void , "_jit_link" , state_p , node_p ) sig ( void , "_jit_patch" , state_p , node_p ) sig ( void , "_jit_patch_at" , state_p , node_p , node_p ) sig ( void , "_jit_prepare" , state_p ) sig ( void , "_jit_prolog" , state_p ) sig ( void , "_jit_pushargi" , state_p , word_t ) sig ( void , "_jit_pushargr" , state_p , gpr_t ) sig ( void , "_jit_ret" , state_p ) sig ( void , "_jit_reti" , state_p , word_t ) sig ( void , "_jit_retr" , state_p , gpr_t ) sig ( void , "_jit_retval_c" , state_p , gpr_t ) sig ( void , "_jit_retval_i" , state_p , gpr_t ) sig ( void , "_jit_retval_s" , state_p , gpr_t ) sig ( void , "_jit_retval_uc" , state_p , gpr_t ) sig ( void , "_jit_retval_us" , state_p , gpr_t ) sig ( void , "finish_jit" ) sig ( void , "init_jit" , ctypes . c_char_p ) # NOTE: Don't use char_p if wordsize == 64 : sig ( void , "_jit_retval_l" , state_p , gpr_t ) sig ( void , "_jit_retval_ui" , state_p , gpr_t )

0<CODESPLIT>https://github.com/flaper87/python-gerrit/blob/34fcd6d98f31b35280f29c353ad8098905ba4b39/gerrit/reviews.py#L31-L52<CODESPLIT>get_results<CODESPLIT>Executes the query and yields items .<CODESPLIT>def get_results ( ) : base = "http://www.smackjeeves.com/search.php?submit=Search+for+Webcomics&search_mode=webcomics&comic_title=&special=all&last_update=3&style_all=on&genre_all=on&format_all=on&sort_by=2&start=%d" session = requests . Session ( ) # store info in a dictionary {name -> url, number of comics, adult flag, bounce flag} res = { } # a search for an empty string returned 286 result pages result_pages = 286 print ( "Parsing" , result_pages , "search result pages..." , file = sys . stderr ) for i in range ( 0 , result_pages ) : print ( i + 1 , file = sys . stderr , end = " " ) handle_url ( base % ( i * 12 ) , session , res ) save_result ( res , json_file )

1<CODESPLIT>https://github.com/crypto101/arthur/blob/c32e693fb5af17eac010e3b20f7653ed6e11eb6a/arthur/ui.py#L330-L340<CODESPLIT>_runPopUp<CODESPLIT>Displays the pop - up on the workbench and gets a completion notification deferred . When that fires undisplay the pop - up and return the result of the notification deferred verbatim .<CODESPLIT>def _runPopUp ( workbench , popUp ) : workbench . display ( popUp ) d = popUp . notifyCompleted ( ) d . addCallback ( _popUpCompleted , workbench ) return d

0<CODESPLIT>https://github.com/TrafficSenseMSD/SumoTools/blob/8607b4f885f1d1798e43240be643efe6dccccdaa/traci/_person.py#L145-L161<CODESPLIT>DefaultCompleter.update_model<CODESPLIT>getEdges ( string int ) - > list ( string )<CODESPLIT>def update_model ( self , words ) : extended_words = DefaultCompleter . _DefaultCompleter__tokens [ self . __language ] [ : ] extended_words . extend ( ( word for word in set ( words ) if word not in DefaultCompleter . _DefaultCompleter__tokens [ self . __language ] ) ) self . setModel ( QStringListModel ( extended_words ) ) return True

0<CODESPLIT>https://github.com/RockFeng0/rtsf/blob/fbc0d57edaeca86418af3942472fcc6d3e9ce591/rtsf/p_common.py#L245-L256<CODESPLIT>FileUtils._load_csv_file<CODESPLIT>load json file and check file content format<CODESPLIT>def _load_csv_file ( csv_file ) : csv_content_list = [ ] with io . open ( csv_file , encoding = 'utf-8' ) as csvfile : reader = csv . DictReader ( csvfile ) for row in reader : csv_content_list . append ( row ) return csv_content_list

1<CODESPLIT>https://github.com/ejeschke/ginga/blob/a78c893ec6f37a837de851947e9bb4625c597915/ginga/mockw/ImageViewMock.py#L309-L319<CODESPLIT>ImageViewEvent.key_press_event<CODESPLIT>Called when a key is pressed and the window has the focus . Adjust method signature as appropriate for callback .<CODESPLIT>def key_press_event ( self , widget , event ) : # get keyname or keycode and translate to ginga standard # keyname = # keycode = keyname = '' # self.transkey(keyname, keycode) self . logger . debug ( "key press event, key=%s" % ( keyname ) ) return self . make_ui_callback ( 'key-press' , keyname )

0<CODESPLIT>https://github.com/dstufft/crust/blob/5d4011ecace12fd3f68a03a17dbefb78390a9fc0/crust/resources.py#L129-L164<CODESPLIT>CloudFilesDatastore.query<CODESPLIT>Saves the current instance . Override this in a subclass if you want to control the saving process .<CODESPLIT>def query ( self , query ) : return query ( ( self . _deserialised_value ( x ) for x in self . container . get_objects ( prefix = query . key ) ) )

1<CODESPLIT>https://github.com/facelessuser/wcmatch/blob/d153e7007cc73b994ae1ba553dc4584039f5c212/wcmatch/_wcparse.py#L702-L713<CODESPLIT>WcParse._restrict_sequence<CODESPLIT>Restrict sequence .<CODESPLIT>def _restrict_sequence ( self ) : if self . pathname : value = self . seq_path_dot if self . after_start and not self . dot else self . seq_path if self . after_start : value = self . no_dir + value else : value = _NO_DOT if self . after_start and not self . dot else "" self . reset_dir_track ( ) return value

1<CODESPLIT>https://github.com/SKA-ScienceDataProcessor/integration-prototype/blob/8c8006de6ad71dcd44114b0338780738079c87d4/sip/examples/flask_processing_controller/app/api/subarray.py#L57-L61<CODESPLIT>create<CODESPLIT>Create / register a Scheduling Block instance with SDP .<CODESPLIT>def create ( sub_array_id ) : config = request . data config [ 'sub_array_id' ] = 'subarray-{:02d}' . format ( sub_array_id ) return add_scheduling_block ( config )

0<CODESPLIT>https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/click/core.py#L48-L58<CODESPLIT>fast_exit<CODESPLIT>Internal handler for the bash completion support .<CODESPLIT>def fast_exit ( code ) : sys . stdout . flush ( ) sys . stderr . flush ( ) os . _exit ( code )

1<CODESPLIT>https://github.com/andreycizov/python-xrpc/blob/4f916383cda7de3272962f3ba07a64f7ec451098/xrpc/runtime.py#L134-L164<CODESPLIT>_masquerade<CODESPLIT>build an origin URL such that the orig has all of the mappings to new defined by map<CODESPLIT>def _masquerade ( origin : str , orig : ServiceDefn , new : ServiceDefn , * * map : str ) -> str : origin : ParseResult = urlparse ( origin ) prev_maps = { } if origin . query : prev_maps = { k : v for k , v in parse_qsl ( origin . query ) } r_args = { } for new_k , orig_k in map . items ( ) : assert new_k in new . rpcs , [ new_k , new . rpcs ] assert orig_k in orig . rpcs , [ orig_k , orig . rpcs ] # todo: check if the definitions are the same new_v = new . rpcs [ new_k ] orig_v = orig . rpcs [ orig_k ] if orig_k in prev_maps : orig_k = prev_maps [ orig_k ] assert new_v . res == orig_v . res , [ new_v . res , orig_v . res ] assert new_v . req == orig_v . req , [ new_v . req , orig_v . req ] r_args [ new_k ] = orig_k return urlunparse ( origin . _replace ( query = urlencode ( r_args ) ) )

0<CODESPLIT>https://github.com/RI-imaging/qpsphere/blob/3cfa0e9fb8e81be8c820abbeccd47242e7972ac1/qpsphere/edgefit.py#L203-L224<CODESPLIT>circle_radii<CODESPLIT>Residuals for circle fitting<CODESPLIT>def circle_radii ( params , xedge , yedge ) : cx = params [ "cx" ] . value cy = params [ "cy" ] . value radii = np . sqrt ( ( cx - xedge ) ** 2 + ( cy - yedge ) ** 2 ) return radii

1<CODESPLIT>https://github.com/senaite/senaite.api/blob/c79c53abcbe6e3a5ab3ced86d2f455275efa20cf/src/senaite/api/__init__.py#L394-L417<CODESPLIT>get_icon<CODESPLIT>Get the icon of the content object<CODESPLIT>def get_icon ( brain_or_object , html_tag = True ) : # Manual approach, because `plone.app.layout.getIcon` does not reliable # work for Bika Contents coming from other catalogs than the # `portal_catalog` portal_types = get_tool ( "portal_types" ) fti = portal_types . getTypeInfo ( brain_or_object . portal_type ) icon = fti . getIcon ( ) if not icon : return "" url = "%s/%s" % ( get_url ( get_portal ( ) ) , icon ) if not html_tag : return url tag = '<img width="16" height="16" src="{url}" title="{title}" />' . format ( url = url , title = get_title ( brain_or_object ) ) return tag

1<CODESPLIT>https://github.com/widdowquinn/pyani/blob/2b24ec971401e04024bba896e4011984fe3f53f0/pyani/pyani_graphics.py#L138-L175<CODESPLIT>heatmap_seaborn<CODESPLIT>Returns seaborn heatmap with cluster dendrograms .<CODESPLIT>def heatmap_seaborn ( dfr , outfilename = None , title = None , params = None ) : # Decide on figure layout size: a minimum size is required for # aesthetics, and a maximum to avoid core dumps on rendering. # If we hit the maximum size, we should modify font size. maxfigsize = 120 calcfigsize = dfr . shape [ 0 ] * 1.1 figsize = min ( max ( 8 , calcfigsize ) , maxfigsize ) if figsize == maxfigsize : scale = maxfigsize / calcfigsize sns . set_context ( "notebook" , font_scale = scale ) # Add a colorbar? if params . classes is None : col_cb = None else : col_cb = get_seaborn_colorbar ( dfr , params . classes ) # Labels are defined before we build the clustering # If a label mapping is missing, use the key text as fall back params . labels = get_safe_seaborn_labels ( dfr , params . labels ) # Add attributes to parameter object, and draw heatmap params . colorbar = col_cb params . figsize = figsize params . linewidths = 0.25 fig = get_seaborn_clustermap ( dfr , params , title = title ) # Save to file if outfilename : fig . savefig ( outfilename ) # Return clustermap return fig

1<CODESPLIT>https://github.com/brocade/pynos/blob/bd8a34e98f322de3fc06750827d8bbc3a0c00380/pynos/utilities.py#L196-L246<CODESPLIT>merge_xml<CODESPLIT>Merges two XML documents .<CODESPLIT>def merge_xml ( first_doc , second_doc ) : # Adapted from: # http://stackoverflow.com/questions/27258013/merge-two-xml-files-python # Maps each elements tag to the element from the first document if isinstance ( first_doc , lxml . etree . _Element ) : first_doc = ET . fromstring ( lxml . etree . tostring ( first_doc ) ) if isinstance ( second_doc , lxml . etree . _Element ) : second_doc = ET . fromstring ( lxml . etree . tostring ( second_doc ) ) mapping = { element . tag : element for element in first_doc } for element in second_doc : if not len ( element ) : # Recursed fully.  This element has no children. try : # Update the first document's element's text mapping [ element . tag ] . text = element . text except KeyError : # The element doesn't exist # add it to the mapping and the root document mapping [ element . tag ] = element first_doc . append ( element ) else : # This element has children.  Recurse. try : merge_xml ( mapping [ element . tag ] , element ) except KeyError : # The element doesn't exist # add it to the mapping and the root document mapping [ element . tag ] = element first_doc . append ( element ) return lxml . etree . fromstring ( ET . tostring ( first_doc ) )

0<CODESPLIT>https://github.com/fracpete/python-weka-wrapper3/blob/d850ab1bdb25fbd5a8d86e99f34a397975425838/python/weka/flow/transformer.py#L1023-L1038<CODESPLIT>CrossValidate.fix_config<CODESPLIT>The actual execution of the actor .<CODESPLIT>def fix_config ( self , options ) : options = super ( CrossValidate , self ) . fix_config ( options ) opt = "setup" if opt not in options : options [ opt ] = Classifier ( classname = "weka.classifiers.rules.ZeroR" ) if opt not in self . help : self . help [ opt ] = "The classifier/clusterer to train (Classifier/Clusterer)." opt = "folds" if opt not in options : options [ opt ] = 10 if opt not in self . help : self . help [ opt ] = "The number of folds for CV (int)." opt = "seed" if opt not in options : options [ opt ] = 1 if opt not in self . help : self . help [ opt ] = "The seed value for randomizing the data (int)." opt = "discard_predictions" if opt not in options : options [ opt ] = False if opt not in self . help : self . help [ opt ] = "Discard classifier predictions to save memory (bool)." opt = "output" if opt not in options : options [ opt ] = None if opt not in self . help : self . help [ opt ] = "For capturing the classifier's prediction output (PredictionOutput)." return options

1<CODESPLIT>https://github.com/glue-viz/glue-vispy-viewers/blob/54a4351d98c1f90dfb1a557d1b447c1f57470eea/glue_vispy_viewers/extern/vispy/scene/canvas.py#L393-L423<CODESPLIT>SceneCanvas._visual_bounds_at<CODESPLIT>Find a visual whose bounding rect encompasses * pos * .<CODESPLIT>def _visual_bounds_at ( self , pos , node = None ) : if node is None : node = self . scene for ch in node . children : hit = self . _visual_bounds_at ( pos , ch ) if hit is not None : return hit if ( not isinstance ( node , VisualNode ) or not node . visible or not node . interactive ) : return None bounds = [ node . bounds ( axis = i ) for i in range ( 2 ) ] if None in bounds : return None tr = self . scene . node_transform ( node ) . inverse corners = np . array ( [ [ bounds [ 0 ] [ 0 ] , bounds [ 1 ] [ 0 ] ] , [ bounds [ 0 ] [ 0 ] , bounds [ 1 ] [ 1 ] ] , [ bounds [ 0 ] [ 1 ] , bounds [ 1 ] [ 0 ] ] , [ bounds [ 0 ] [ 1 ] , bounds [ 1 ] [ 1 ] ] ] ) bounds = tr . map ( corners ) xhit = bounds [ : , 0 ] . min ( ) < pos [ 0 ] < bounds [ : , 0 ] . max ( ) yhit = bounds [ : , 1 ] . min ( ) < pos [ 1 ] < bounds [ : , 1 ] . max ( ) if xhit and yhit : return node

1<CODESPLIT>https://github.com/Azure/azure-storage-python/blob/52327354b192cbcf6b7905118ec6b5d57fa46275/azure-storage-file/azure/storage/file/fileservice.py#L1041-L1070<CODESPLIT>FileService.get_directory_properties<CODESPLIT>Returns all user - defined metadata and system properties for the specified directory . The data returned does not include the directory s list of files .<CODESPLIT>def get_directory_properties ( self , share_name , directory_name , timeout = None , snapshot = None ) : _validate_not_none ( 'share_name' , share_name ) _validate_not_none ( 'directory_name' , directory_name ) request = HTTPRequest ( ) request . method = 'GET' request . host_locations = self . _get_host_locations ( ) request . path = _get_path ( share_name , directory_name ) request . query = { 'restype' : 'directory' , 'timeout' : _int_to_str ( timeout ) , 'sharesnapshot' : _to_str ( snapshot ) } return self . _perform_request ( request , _parse_directory , [ directory_name ] )

0<CODESPLIT>https://github.com/nuagenetworks/bambou/blob/d334fea23e384d3df8e552fe1849ad707941c666/bambou/nurest_login_controller.py#L223-L256<CODESPLIT>NURESTLoginController.reset<CODESPLIT>Return authenication string to place in Authorization Header<CODESPLIT>def reset ( self ) : self . _is_impersonating = False self . _impersonation = None self . user = None self . password = None self . api_key = None self . enterprise = None self . url = None

1<CODESPLIT>https://github.com/greenape/mktheapidocs/blob/a45e8b43ddd80ed360fe1e98d4f73dc11c4e7bf7/mktheapidocs/mkapi.py#L429-L472<CODESPLIT>get_source_link<CODESPLIT>Get a link to the line number a module / class / function is defined at .<CODESPLIT>def get_source_link ( thing , source_location ) : try : lineno = get_line ( thing ) try : owner_module = inspect . getmodule ( thing ) assert owner_module is not None except ( TypeError , AssertionError ) : owner_module = inspect . getmodule ( thing . fget ) thing_file = "/" . join ( owner_module . __name__ . split ( "." ) ) if owner_module . __file__ . endswith ( "__init__.py" ) : thing_file += "/__init__.py" else : thing_file += ".py" return ( f"Source: [{escape(thing_file)}]({source_location}/{thing_file}#L{lineno})" + "\n\n" ) except Exception as e : # print("Failed to find source file.") # print(e) # print(lineno) # print(thing) # print(owner_module) # print(thing_file) # print(source_location) pass return ""

0<CODESPLIT>https://github.com/kstaniek/condoor/blob/77c054b29d4e286c1d7aca2c74dff86b805e1fae/condoor/actions.py#L40-L51<CODESPLIT>a_send_username<CODESPLIT>Send the password text .<CODESPLIT>def a_send_username ( username , ctx ) : if username : ctx . ctrl . sendline ( username ) return True else : ctx . ctrl . disconnect ( ) raise ConnectionAuthenticationError ( "Username not provided" , ctx . ctrl . hostname )

1<CODESPLIT>https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/assessment_authoring/objects.py#L431-L441<CODESPLIT>AssessmentPartForm.get_allocated_time_metadata<CODESPLIT>Gets the metadata for the allocated time .<CODESPLIT>def get_allocated_time_metadata ( self ) : # Implemented from template for osid.resource.ResourceForm.get_group_metadata_template metadata = dict ( self . _mdata [ 'allocated_time' ] ) metadata . update ( { 'existing_duration_values' : self . _my_map [ 'allocatedTime' ] } ) return Metadata ( * * metadata )

0<CODESPLIT>https://github.com/afg984/pyardrone/blob/2c39263d5fdea91070c20f0ae9c9f355a9b5fe17/pyardrone/utils/__init__.py#L42-L57<CODESPLIT>get_grouped_translations<CODESPLIT>Generator that yields for every * secs * seconds .<CODESPLIT>def get_grouped_translations ( instances , * * kwargs ) : grouped_translations = collections . defaultdict ( list ) if not instances : return grouped_translations if not isinstance ( instances , collections . Iterable ) : instances = [ instances ] if isinstance ( instances , QuerySet ) : model = instances . model else : model = instances [ 0 ] . _meta . model instances_ids = [ ] for instance in instances : instances_ids . append ( instance . pk ) if instance . _meta . model != model : raise Exception ( "You cannot use different model instances, only one authorized." ) from . models import Translation from . mixins import ModelMixin decider = model . _meta . linguist . get ( "decider" , Translation ) identifier = model . _meta . linguist . get ( "identifier" , None ) chunks_length = kwargs . get ( "chunks_length" , None ) populate_missing = kwargs . get ( "populate_missing" , True ) if identifier is None : raise Exception ( 'You must define Linguist "identifier" meta option' ) lookup = dict ( identifier = identifier ) for kwarg in ( "field_names" , "languages" ) : value = kwargs . get ( kwarg , None ) if value is not None : if not isinstance ( value , ( list , tuple ) ) : value = [ value ] lookup [ "%s__in" % kwarg [ : - 1 ] ] = value if chunks_length is not None : translations_qs = [ ] for ids in utils . chunks ( instances_ids , chunks_length ) : ids_lookup = copy . copy ( lookup ) ids_lookup [ "object_id__in" ] = ids translations_qs . append ( decider . objects . filter ( * * ids_lookup ) ) translations = itertools . chain . from_iterable ( translations_qs ) else : lookup [ "object_id__in" ] = instances_ids translations = decider . objects . filter ( * * lookup ) for translation in translations : grouped_translations [ translation . object_id ] . append ( translation ) return grouped_translations

0<CODESPLIT>https://github.com/mlenzen/collections-extended/blob/ee9e86f6bbef442dbebcb3a5970642c5c969e2cf/collections_extended/__init__.py#L27-L59<CODESPLIT>version_parser<CODESPLIT>Return a : class : Collection with the specified properties .<CODESPLIT>def version_parser ( version_string ) : version , commit , branch = version_string . split ( '-' , 3 ) # Remove the "v" version_number = version [ 1 : ] version_number = int ( version_number ) return Version ( version_number , commit , branch )

0<CODESPLIT>https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/chunked/core.py#L205-L210<CODESPLIT>amin<CODESPLIT>Compute the sum .<CODESPLIT>def amin ( data , axis = None , mapper = None , blen = None , storage = None , create = 'array' , * * kwargs ) : return reduce_axis ( data , axis = axis , reducer = np . amin , block_reducer = np . minimum , mapper = mapper , blen = blen , storage = storage , create = create , * * kwargs )

0<CODESPLIT>https://github.com/jantman/awslimitchecker/blob/e50197f70f3d0abcc5cfc7fde6336f548b790e34/awslimitchecker/trustedadvisor.py#L336-L363<CODESPLIT>TrustedAdvisor._get_check_result<CODESPLIT>Determine if the given check_id can be refreshed yet .<CODESPLIT>def _get_check_result ( self , check_id ) : checks = self . conn . describe_trusted_advisor_check_result ( checkId = check_id , language = 'en' ) try : check_datetime = parser . parse ( checks [ 'result' ] [ 'timestamp' ] ) logger . debug ( "Got TrustedAdvisor data for check %s as of %s" , check_id , check_datetime ) except KeyError : check_datetime = None logger . debug ( "Got TrustedAdvisor data for check %s but unable to " "parse timestamp" , check_id ) return checks , check_datetime

1<CODESPLIT>https://github.com/yyuu/botornado/blob/fffb056f5ff2324d1d5c1304014cfb1d899f602e/boto/dynamodb/layer2.py#L342-L365<CODESPLIT>Layer2.create_table<CODESPLIT>Create a new Amazon DynamoDB table . : type name : str : param name : The name of the desired table .<CODESPLIT>def create_table ( self , name , schema , read_units , write_units ) : response = self . layer1 . create_table ( name , schema . dict , { 'ReadCapacityUnits' : read_units , 'WriteCapacityUnits' : write_units } ) return Table ( self , response )

0<CODESPLIT>https://github.com/gtalarico/airtable-python-wrapper/blob/48b2d806178085b52a31817571e5a1fc3dce4045/airtable/airtable.py#L496-L510<CODESPLIT>Airtable.replace_by_field<CODESPLIT>Deletes a record by its id >>> record = airtable . match ( Employee Id DD13332454 ) >>> airtable . delete ( record [ id ] ) Args : record_id ( str ) : Airtable record id Returns : record ( dict ) : Deleted Record<CODESPLIT>def replace_by_field ( self , field_name , field_value , fields , typecast = False , * * options ) : record = self . match ( field_name , field_value , * * options ) return { } if not record else self . replace ( record [ 'id' ] , fields , typecast )

1<CODESPLIT>https://github.com/jeremymcrae/denovonear/blob/feaab0fc77e89d70b31e8092899e4f0e68bac9fe/scripts/run_batch.py#L114-L156<CODESPLIT>submit_bsub_job<CODESPLIT>construct a bsub job submission command Args : command : list of strings that forma unix command job_id : string for job ID for submission dependent_id : job ID or list of job IDs which the current command needs to have finished before the current command will start . Note that the list can be empty in which case there are no dependencies . memory : minimum memory requirements ( in megabytes ) Returns : nothing<CODESPLIT>def submit_bsub_job ( command , job_id = None , dependent_id = None , memory = None , requeue_code = None , logfile = None ) : if job_id is None : job_id = get_random_string ( ) job = "-J \"{0}\"" . format ( job_id ) mem = "" if memory is not None : mem = "-R 'select[mem>{0}] rusage[mem={0}]' -M {0}" . format ( memory ) requeue = "" if requeue_code is not None : requeue = "-Q 'EXCLUDE({0})'" . format ( requeue_code ) dependent = "" if dependent_id is not None : if type ( dependent_id ) == list : dependent_id = " && " . join ( dependent_id ) dependent = "-w '{0}'" . format ( dependent_id ) log = "bjob_output.txt" if logfile is not None : log = logfile preamble = [ "bsub" , job , dependent , requeue , "-q" , "normal" , "-o" , log , mem ] command = [ "bash" , "-c" , "\"" ] + command + [ "\"" ] command = " " . join ( preamble + command ) subprocess . call ( command , shell = True )

0<CODESPLIT>https://github.com/Roastero/freshroastsr700/blob/49cf4961444c0f56d051d5ac5088ace480b54f02/examples/pid_tune_aid.py#L136-L169<CODESPLIT>Roaster.update_data<CODESPLIT>This is a method that will be called when the time remaining ends . The current state can be : roasting cooling idle sleeping connecting or unkown .<CODESPLIT>def update_data ( self ) : time_elapsed = datetime . datetime . now ( ) - self . start_time crntTemp = self . roaster . current_temp targetTemp = self . roaster . target_temp heaterLevel = self . roaster . heater_level # print( #     "Time: %4.6f, crntTemp: %d, targetTemp: %d, heaterLevel: %d" % #     (time_elapsed.total_seconds(), crntTemp, targetTemp, heaterLevel)) self . file . write ( "%4.6f,%d,%d,%d\n" % ( time_elapsed . total_seconds ( ) , crntTemp , targetTemp , heaterLevel ) )

1<CODESPLIT>https://github.com/twilio/twilio-python/blob/c867895f55dcc29f522e6e8b8868d0d18483132f/twilio/rest/preview/deployed_devices/fleet/deployment.py#L38-L56<CODESPLIT>DeploymentList.create<CODESPLIT>Create a new DeploymentInstance<CODESPLIT>def create ( self , friendly_name = values . unset , sync_service_sid = values . unset ) : data = values . of ( { 'FriendlyName' : friendly_name , 'SyncServiceSid' : sync_service_sid , } ) payload = self . _version . create ( 'POST' , self . _uri , data = data , ) return DeploymentInstance ( self . _version , payload , fleet_sid = self . _solution [ 'fleet_sid' ] , )

0<CODESPLIT>https://github.com/goshuirc/irc/blob/d6a5e3e04d337566c009b087f108cd76f9e122cc/girc/client.py#L548-L559<CODESPLIT>ServerConnection.sasl_plain<CODESPLIT>Identify to NickServ ( legacy ) .<CODESPLIT>def sasl_plain ( self , name , password , identity = None ) : if identity is None : identity = name self . sasl ( 'plain' , name , password , identity )

1<CODESPLIT>https://github.com/onicagroup/runway/blob/3f3549ec3bf6e39b9f27d9738a1847f3a4369e7f/runway/commands/runway_command.py#L107-L111<CODESPLIT>RunwayCommand.runway_config<CODESPLIT>Return parsed runway . yml .<CODESPLIT>def runway_config ( self ) : if not self . _runway_config : self . _runway_config = self . parse_runway_config ( ) return self . _runway_config

0<CODESPLIT>https://github.com/NoneGG/aredis/blob/204caad740ac13e5760d46444a2ba7632982a046/aredis/utils.py#L151-L176<CODESPLIT>parse_debug_object<CODESPLIT>Wrapper for CLUSTERDOWN error handling .<CODESPLIT>def parse_debug_object ( response ) : # The 'type' of the object is the first item in the response, but isn't # prefixed with a name response = nativestr ( response ) response = 'type:' + response response = dict ( [ kv . split ( ':' ) for kv in response . split ( ) ] ) # parse some expected int values from the string response # note: this cmd isn't spec'd so these may not appear in all redis versions int_fields = ( 'refcount' , 'serializedlength' , 'lru' , 'lru_seconds_idle' ) for field in int_fields : if field in response : response [ field ] = int ( response [ field ] ) return response

0<CODESPLIT>https://github.com/jtwhite79/pyemu/blob/c504d8e7a4097cec07655a6318d275739bd8148a/pyemu/sc.py#L187-L215<CODESPLIT>Schur.map_forecast_estimate<CODESPLIT>get posterior forecast ( prediction ) variances<CODESPLIT>def map_forecast_estimate ( self ) : assert self . forecasts is not None islog = self . pst . parameter_data . partrans == "log" par_map = self . map_parameter_estimate par_map . loc [ islog , : ] = np . log10 ( par_map . loc [ islog , : ] ) par_map = Matrix . from_dataframe ( par_map . loc [ : , [ "post_expt" ] ] ) posts , priors = [ ] , [ ] post_expt = ( self . predictions . T * par_map ) . to_dataframe ( ) for fname in self . forecast_names : #fname = forecast.col_names[0] pr = self . pst . res . loc [ fname , "modelled" ] priors . append ( pr ) posts . append ( pr + post_expt . loc [ fname , "post_expt" ] ) return pd . DataFrame ( data = np . array ( [ priors , posts ] ) . transpose ( ) , columns = [ "prior_expt" , "post_expt" ] , index = self . forecast_names )

1<CODESPLIT>https://github.com/ktdreyer/txkoji/blob/a7de380f29f745bf11730b27217208f6d4da7733/txkoji/estimates.py#L11-L21<CODESPLIT>average_build_duration<CODESPLIT>Return the average build duration for a package ( or container ) .<CODESPLIT>def average_build_duration ( connection , package ) : if isinstance ( package , str ) and package . endswith ( '-container' ) : return average_last_builds ( connection , package ) return connection . getAverageBuildDuration ( package )

1<CODESPLIT>https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/src/unity/python/turicreate/toolkits/_private_utils.py#L152-L217<CODESPLIT>_validate_lists<CODESPLIT>For a list - typed SArray check whether the first elements are lists that - contain only the provided types - all have the same lengths ( optionally )<CODESPLIT>def _validate_lists ( sa , allowed_types = [ str ] , require_same_type = True , require_equal_length = False , num_to_check = 10 ) : if len ( sa ) == 0 : return True first_elements = sa . head ( num_to_check ) if first_elements . dtype != list : raise ValueError ( "Expected an SArray of lists when type-checking lists." ) # Check list lengths list_lengths = list ( first_elements . item_length ( ) ) same_length = _check_elements_equal ( list_lengths ) if require_equal_length and not same_length : return False # If list lengths are all zero, return True. if len ( first_elements [ 0 ] ) == 0 : return True # Check for matching types within each list types = first_elements . apply ( lambda xs : [ str ( type ( x ) ) for x in xs ] ) same_type = [ _check_elements_equal ( x ) for x in types ] all_same_type = _check_elements_equal ( same_type ) if require_same_type and not all_same_type : return False # Check for matching types across lists first_types = [ t [ 0 ] for t in types if t ] all_same_type = _check_elements_equal ( first_types ) if require_same_type and not all_same_type : return False # Check to make sure all elements have types that are allowed allowed_type_strs = [ str ( x ) for x in allowed_types ] for list_element_types in types : for t in list_element_types : if t not in allowed_type_strs : return False return True

1<CODESPLIT>https://github.com/hazelcast/hazelcast-python-client/blob/3f6639443c23d6d036aa343f8e094f052250d2c1/hazelcast/proxy/atomic_long.py#L96-L107<CODESPLIT>AtomicLong.get_and_alter<CODESPLIT>Alters the currently stored value by applying a function on it on and gets the old value .<CODESPLIT>def get_and_alter ( self , function ) : check_not_none ( function , "function can't be None" ) return self . _encode_invoke ( atomic_long_get_and_alter_codec , function = self . _to_data ( function ) )

0<CODESPLIT>https://github.com/klmitch/cli_tools/blob/3f9b5fd8d7458a402b3999618644ffa419d8a946/cli_tools.py#L822-L833<CODESPLIT>argument_group<CODESPLIT>Decorator used to specify alternate keyword arguments to pass to the argparse . ArgumentParser . add_subparsers () call .<CODESPLIT>def argument_group ( group , * * kwargs ) : def decorator ( func ) : adaptor = ScriptAdaptor . _get_adaptor ( func ) adaptor . _add_group ( group , 'group' , kwargs ) return func return decorator

0<CODESPLIT>https://github.com/python-visualization/folium/blob/8595240517135d1637ca4cf7cc624045f1d911b3/folium/features.py#L512-L521<CODESPLIT>GeoJson.convert_to_feature_collection<CODESPLIT>Tests self . style_function and self . highlight_function to ensure they are functions returning dictionaries .<CODESPLIT>def convert_to_feature_collection ( self ) : if self . data [ 'type' ] == 'FeatureCollection' : return if not self . embed : raise ValueError ( 'Data is not a FeatureCollection, but it should be to apply ' 'style or highlight. Because `embed=False` it cannot be ' 'converted into one.\nEither change your geojson data to a ' 'FeatureCollection, set `embed=True` or disable styling.' ) # Catch case when GeoJSON is just a single Feature or a geometry. if 'geometry' not in self . data . keys ( ) : # Catch case when GeoJSON is just a geometry. self . data = { 'type' : 'Feature' , 'geometry' : self . data } self . data = { 'type' : 'FeatureCollection' , 'features' : [ self . data ] }

1<CODESPLIT>https://github.com/airspeed-velocity/asv/blob/d23bb8b74e8adacbfa3cf5724bda55fb39d56ba6/asv/extern/asizeof.py#L2022-L2091<CODESPLIT>asizeof<CODESPLIT>Return the combined size in bytes of all objects passed as positional argments .<CODESPLIT>def asizeof ( * objs , * * opts ) : t , p = _objs_opts ( objs , * * opts ) if t : _asizer . reset ( * * p ) s = _asizer . asizeof ( * t ) _asizer . print_stats ( objs = t , opts = opts ) # show opts as _kwdstr _asizer . _clear ( ) else : s = 0 return s

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/docker/translate/container.py#L694-L704<CODESPLIT>volumes<CODESPLIT>Should be a list of absolute paths<CODESPLIT>def volumes ( val , * * kwargs ) : # pylint: disable=unused-argument val = helpers . translate_stringlist ( val ) for item in val : if not os . path . isabs ( item ) : raise SaltInvocationError ( '\'{0}\' is not an absolute path' . format ( item ) ) return val

1<CODESPLIT>https://github.com/Brightmd/TxPx/blob/403c18b3006fc68842ec05b259e8611fe80763aa/txpx/process.py#L86-L96<CODESPLIT>EchoProcess.processEnded<CODESPLIT>Connected process shut down<CODESPLIT>def processEnded ( self , reason ) : log_debug ( "{name} process exited" , name = self . name ) if self . deferred : if reason . type == ProcessDone : self . deferred . callback ( reason . value . exitCode ) elif reason . type == ProcessTerminated : self . deferred . errback ( reason ) return self . deferred

0<CODESPLIT>https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/hydpytools.py#L517-L528<CODESPLIT>HydPy.variables<CODESPLIT>|Nodes| object containing all |Node| objects currently handled by the |HydPy| object which define a downstream end point of a network .<CODESPLIT>def variables ( self ) : variables = set ( [ ] ) for node in self . nodes : variables . add ( node . variable ) return sorted ( variables )

1<CODESPLIT>https://github.com/pystorm/pystorm/blob/0f853e007c79e03cefdb4a0794423f84dce4c2f3/pystorm/serializers/json_serializer.py#L39-L91<CODESPLIT>JSONSerializer.read_message<CODESPLIT>The Storm multilang protocol consists of JSON messages followed by a newline and end \ n .<CODESPLIT>def read_message ( self ) : msg = "" num_blank_lines = 0 while True : # readline will return trailing \n so that output is unambigious, we # should only have line == '' if we're at EOF with self . _reader_lock : line = self . input_stream . readline ( ) if line == "end\n" : break elif line == "" : raise StormWentAwayError ( ) elif line == "\n" : num_blank_lines += 1 if num_blank_lines % 1000 == 0 : log . warn ( "While trying to read a command or pending task " "ID, Storm has instead sent %s '\\n' messages." , num_blank_lines , ) continue msg = "{}{}\n" . format ( msg , line [ 0 : - 1 ] ) try : return json . loads ( msg ) except Exception : log . error ( "JSON decode error for message: %r" , msg , exc_info = True ) raise

1<CODESPLIT>https://github.com/mapnik/Cascadenik/blob/82f66859340a31dfcb24af127274f262d4f3ad85/cascadenik/compile.py#L963-L1062<CODESPLIT>get_text_rule_groups<CODESPLIT>Given a list of declarations return a list of output . Rule objects .<CODESPLIT>def get_text_rule_groups ( declarations ) : property_map = { 'text-anchor-dx' : 'anchor_dx' , # does nothing 'text-anchor-dy' : 'anchor_dy' , # does nothing 'text-align' : 'horizontal_alignment' , 'text-allow-overlap' : 'allow_overlap' , 'text-avoid-edges' : 'avoid_edges' , 'text-character-spacing' : 'character_spacing' , 'text-dx' : 'dx' , 'text-dy' : 'dy' , 'text-face-name' : 'face_name' , 'text-fill' : 'fill' , 'text-fontset' : 'fontset' , 'text-halo-fill' : 'halo_fill' , 'text-halo-radius' : 'halo_radius' , 'text-justify-align' : 'justify_alignment' , 'text-label-position-tolerance' : 'label_position_tolerance' , 'text-line-spacing' : 'line_spacing' , 'text-max-char-angle-delta' : 'max_char_angle_delta' , 'text-min-distance' : 'minimum_distance' , 'text-placement' : 'label_placement' , 'text-ratio' : 'text_ratio' , 'text-size' : 'size' , 'text-spacing' : 'spacing' , 'text-transform' : 'text_convert' , 'text-vertical-align' : 'vertical_alignment' , 'text-wrap-width' : 'wrap_width' , 'text-meta-output' : 'meta-output' , 'text-meta-writer' : 'meta-writer' } property_names = property_map . keys ( ) # pull out all the names text_names = [ dec . selector . elements [ 1 ] . names [ 0 ] for dec in declarations if len ( dec . selector . elements ) is 2 and len ( dec . selector . elements [ 1 ] . names ) is 1 ] # a place to put groups groups = [ ] # a separate style element for each text name for text_name in set ( text_names ) : # just the ones we care about here. # the complicated conditional means: get all declarations that # apply to this text_name specifically, or text in general. name_declarations = [ dec for dec in declarations if dec . property . name in property_map and ( len ( dec . selector . elements ) == 1 or ( len ( dec . selector . elements ) == 2 and dec . selector . elements [ 1 ] . names [ 0 ] in ( text_name , '*' ) ) ) ] # a place to put rules rules = [ ] for ( filter , values ) in filtered_property_declarations ( name_declarations , property_names ) : face_name = values . has_key ( 'text-face-name' ) and values [ 'text-face-name' ] . value or None fontset = values . has_key ( 'text-fontset' ) and values [ 'text-fontset' ] . value or None size = values . has_key ( 'text-size' ) and values [ 'text-size' ] . value color = values . has_key ( 'text-fill' ) and values [ 'text-fill' ] . value ratio = values . has_key ( 'text-ratio' ) and values [ 'text-ratio' ] . value or None wrap_width = values . has_key ( 'text-wrap-width' ) and values [ 'text-wrap-width' ] . value or None label_spacing = values . has_key ( 'text-spacing' ) and values [ 'text-spacing' ] . value or None label_position_tolerance = values . has_key ( 'text-label-position-tolerance' ) and values [ 'text-label-position-tolerance' ] . value or None max_char_angle_delta = values . has_key ( 'text-max-char-angle-delta' ) and values [ 'text-max-char-angle-delta' ] . value or None halo_color = values . has_key ( 'text-halo-fill' ) and values [ 'text-halo-fill' ] . value or None halo_radius = values . has_key ( 'text-halo-radius' ) and values [ 'text-halo-radius' ] . value or None dx = values . has_key ( 'text-dx' ) and values [ 'text-dx' ] . value or None dy = values . has_key ( 'text-dy' ) and values [ 'text-dy' ] . value or None avoid_edges = values . has_key ( 'text-avoid-edges' ) and values [ 'text-avoid-edges' ] . value or None minimum_distance = values . has_key ( 'text-min-distance' ) and values [ 'text-min-distance' ] . value or None allow_overlap = values . has_key ( 'text-allow-overlap' ) and values [ 'text-allow-overlap' ] . value or None label_placement = values . has_key ( 'text-placement' ) and values [ 'text-placement' ] . value or None text_transform = values . has_key ( 'text-transform' ) and values [ 'text-transform' ] . value or None anchor_dx = values . has_key ( 'text-anchor-dx' ) and values [ 'text-anchor-dx' ] . value or None anchor_dy = values . has_key ( 'text-anchor-dy' ) and values [ 'text-anchor-dy' ] . value or None horizontal_alignment = values . has_key ( 'text-horizontal-align' ) and values [ 'text-horizontal-align' ] . value or None vertical_alignment = values . has_key ( 'text-vertical-align' ) and values [ 'text-vertical-align' ] . value or None justify_alignment = values . has_key ( 'text-justify-align' ) and values [ 'text-justify-align' ] . value or None line_spacing = values . has_key ( 'text-line-spacing' ) and values [ 'text-line-spacing' ] . value or None character_spacing = values . has_key ( 'text-character-spacing' ) and values [ 'text-character-spacing' ] . value or None if ( face_name or fontset ) and size and color : symbolizer = output . TextSymbolizer ( text_name , face_name , size , color , wrap_width , label_spacing , label_position_tolerance , max_char_angle_delta , halo_color , halo_radius , dx , dy , avoid_edges , minimum_distance , allow_overlap , label_placement , line_spacing , character_spacing , text_transform , fontset , anchor_dx , anchor_dy , horizontal_alignment , vertical_alignment , justify_alignment ) rules . append ( make_rule ( filter , symbolizer ) ) groups . append ( ( text_name , rules ) ) return dict ( groups )

1<CODESPLIT>https://github.com/JukeboxPipeline/jukebox-core/blob/bac2280ca49940355270e4b69400ce9976ab2e6f/src/jukeboxcore/gui/reftrackitemdata.py#L378-L401<CODESPLIT>ReftrackSortFilterModel.filterAcceptsRow<CODESPLIT>Return True if the filter accepts the given row of the parent<CODESPLIT>def filterAcceptsRow ( self , row , parentindex ) : if not super ( ReftrackSortFilterModel , self ) . filterAcceptsRow ( row , parentindex ) : return False if parentindex . isValid ( ) : m = parentindex . model ( ) else : m = self . sourceModel ( ) i = m . index ( row , 18 , parentindex ) reftrack = i . data ( REFTRACK_OBJECT_ROLE ) if not reftrack : return True else : return self . filter_accept_reftrack ( reftrack )

0<CODESPLIT>https://github.com/orb-framework/orb/blob/575be2689cb269e65a0a2678232ff940acc19e5a/orb/core/column_types/dtime.py#L507-L523<CODESPLIT>is_mobile<CODESPLIT>Converts the inputted string text to a value that matches the type from this column type .<CODESPLIT>def is_mobile ( user_agent ) : if user_agent : b = reg_b . search ( user_agent ) v = reg_v . search ( user_agent [ 0 : 4 ] ) return b or v return False

1<CODESPLIT>https://github.com/timothyb0912/pylogit/blob/f83b0fd6debaa7358d87c3828428f6d4ead71357/pylogit/bootstrap_abc.py#L478-L505<CODESPLIT>calc_bias_abc<CODESPLIT>Calculates the approximate bias of the MLE estimates for use in calculating the approximate bootstrap confidence ( ABC ) intervals .<CODESPLIT>def calc_bias_abc ( second_order_influence ) : num_obs = second_order_influence . shape [ 0 ] constant = 2.0 * num_obs ** 2 bias = second_order_influence . sum ( axis = 0 ) / constant return bias

1<CODESPLIT>https://github.com/AndrewAnnex/SpiceyPy/blob/fc20a9b9de68b58eed5b332f0c051fb343a6e335/spiceypy/spiceypy.py#L3283-L3309<CODESPLIT>dskx02<CODESPLIT>Determine the plate ID and body - fixed coordinates of the intersection of a specified ray with the surface defined by a type 2 DSK plate model .<CODESPLIT>def dskx02 ( handle , dladsc , vertex , raydir ) : handle = ctypes . c_int ( handle ) vertex = stypes . toDoubleVector ( vertex ) raydir = stypes . toDoubleVector ( raydir ) plid = ctypes . c_int ( ) xpt = stypes . emptyDoubleVector ( 3 ) found = ctypes . c_int ( ) libspice . dskx02_c ( handle , ctypes . byref ( dladsc ) , vertex , raydir , ctypes . byref ( plid ) , xpt , ctypes . byref ( found ) ) return plid . value , stypes . cVectorToPython ( xpt ) , bool ( found . value )

0<CODESPLIT>https://github.com/rochacbruno/python-pagseguro/blob/18a9ca3301783cb323e838574b59f9ddffa9a593/pagseguro/__init__.py#L244-L248<CODESPLIT>PagSeguro.check_pre_approval_notification<CODESPLIT>ask form a subscribe payment<CODESPLIT>def check_pre_approval_notification ( self , code ) : response = self . get ( url = self . config . PRE_APPROVAL_NOTIFICATION_URL % code ) return PagSeguroPreApprovalNotificationResponse ( response . content , self . config )

1<CODESPLIT>https://github.com/bpython/curtsies/blob/223e42b97fbf6c86b479ed4f0963a067333c5a63/curtsies/input.py#L276-L283<CODESPLIT>Input.scheduled_event_trigger<CODESPLIT>Returns a callback that schedules events for the future .<CODESPLIT>def scheduled_event_trigger ( self , event_type ) : def callback ( when , * * kwargs ) : self . queued_scheduled_events . append ( ( when , event_type ( when = when , * * kwargs ) ) ) return callback

1<CODESPLIT>https://github.com/cherrypy/cheroot/blob/2af3b1798d66da697957480d3a8b4831a405770b/cheroot/ssl/pyopenssl.py#L258-L264<CODESPLIT>pyOpenSSLAdapter.bind<CODESPLIT>Wrap and return the given socket .<CODESPLIT>def bind ( self , sock ) : if self . context is None : self . context = self . get_context ( ) conn = SSLConnection ( self . context , sock ) self . _environ = self . get_environ ( ) return conn

1<CODESPLIT>https://github.com/hydraplatform/hydra-base/blob/9251ff7946505f7a272c87837390acd1c435bc6e/hydra_base/lib/sharing.py#L268-L287<CODESPLIT>unhide_dataset<CODESPLIT>Hide a particular piece of data so it can only be seen by its owner . Only an owner can hide ( and unhide ) data . Data with no owner cannot be hidden .<CODESPLIT>def unhide_dataset ( dataset_id , * * kwargs ) : user_id = kwargs . get ( 'user_id' ) dataset_i = _get_dataset ( dataset_id ) #check that I can unhide the dataset if dataset_i . created_by != int ( user_id ) : raise HydraError ( 'Permission denied. ' 'User %s is not the owner of dataset %s' % ( user_id , dataset_i . name ) ) dataset_i . hidden = 'N' db . DBSession . flush ( )

0<CODESPLIT>https://github.com/GNS3/gns3-server/blob/a221678448fb5d24e977ef562f81d56aacc89ab1/gns3server/compute/base_manager.py#L229-L257<CODESPLIT>BaseManager.convert_old_project<CODESPLIT>Create a new node<CODESPLIT>def convert_old_project ( self , project , legacy_id , name ) : new_id = str ( uuid4 ( ) ) legacy_project_files_path = os . path . join ( project . path , "{}-files" . format ( project . name ) ) new_project_files_path = os . path . join ( project . path , "project-files" ) if os . path . exists ( legacy_project_files_path ) and not os . path . exists ( new_project_files_path ) : # move the project files log . info ( "Converting old project..." ) try : log . info ( 'Moving "{}" to "{}"' . format ( legacy_project_files_path , new_project_files_path ) ) yield from wait_run_in_executor ( shutil . move , legacy_project_files_path , new_project_files_path ) except OSError as e : raise aiohttp . web . HTTPInternalServerError ( text = "Could not move project files directory: {} to {} {}" . format ( legacy_project_files_path , new_project_files_path , e ) ) if project . is_local ( ) is False : legacy_remote_project_path = os . path . join ( project . location , project . name , self . module_name . lower ( ) ) new_remote_project_path = os . path . join ( project . path , "project-files" , self . module_name . lower ( ) ) if os . path . exists ( legacy_remote_project_path ) and not os . path . exists ( new_remote_project_path ) : # move the legacy remote project (remote servers only) log . info ( "Converting old remote project..." ) try : log . info ( 'Moving "{}" to "{}"' . format ( legacy_remote_project_path , new_remote_project_path ) ) yield from wait_run_in_executor ( shutil . move , legacy_remote_project_path , new_remote_project_path ) except OSError as e : raise aiohttp . web . HTTPInternalServerError ( text = "Could not move directory: {} to {} {}" . format ( legacy_remote_project_path , new_remote_project_path , e ) ) if hasattr ( self , "get_legacy_vm_workdir" ) : # rename old project node working dir log . info ( "Converting old node working directory..." ) legacy_vm_dir = self . get_legacy_vm_workdir ( legacy_id , name ) legacy_vm_working_path = os . path . join ( new_project_files_path , legacy_vm_dir ) new_vm_working_path = os . path . join ( new_project_files_path , self . module_name . lower ( ) , new_id ) if os . path . exists ( legacy_vm_working_path ) and not os . path . exists ( new_vm_working_path ) : try : log . info ( 'Moving "{}" to "{}"' . format ( legacy_vm_working_path , new_vm_working_path ) ) yield from wait_run_in_executor ( shutil . move , legacy_vm_working_path , new_vm_working_path ) except OSError as e : raise aiohttp . web . HTTPInternalServerError ( text = "Could not move vm working directory: {} to {} {}" . format ( legacy_vm_working_path , new_vm_working_path , e ) ) return new_id

0<CODESPLIT>https://github.com/DataBiosphere/toil/blob/a8252277ff814e7bee0971139c2344f88e44b644/src/toil/cwl/cwltoil.py#L618-L641<CODESPLIT>remove_pickle_problems<CODESPLIT>Create the correct Toil Job object for the CWL tool ( workflow job or job wrapper for dynamic resource requirements . )<CODESPLIT>def remove_pickle_problems ( obj ) : if hasattr ( obj , "doc_loader" ) : obj . doc_loader = None if hasattr ( obj , "embedded_tool" ) : obj . embedded_tool = remove_pickle_problems ( obj . embedded_tool ) if hasattr ( obj , "steps" ) : obj . steps = [ remove_pickle_problems ( s ) for s in obj . steps ] return obj

1<CODESPLIT>https://github.com/CI-WATER/gsshapy/blob/00fd4af0fd65f1614d75a52fe950a04fb0867f4c/gsshapy/orm/spn.py#L65-L103<CODESPLIT>StormPipeNetworkFile._read<CODESPLIT>Storm Pipe Network File Read from File Method<CODESPLIT>def _read ( self , directory , filename , session , path , name , extension , spatial , spatialReferenceID , replaceParamFile ) : # Set file extension property self . fileExtension = extension # Dictionary of keywords/cards and parse function names KEYWORDS = { 'CONNECT' : spc . connectChunk , 'SJUNC' : spc . sjuncChunk , 'SLINK' : spc . slinkChunk } sjuncs = [ ] slinks = [ ] connections = [ ] # Parse file into chunks associated with keywords/cards with open ( path , 'r' ) as f : chunks = pt . chunk ( KEYWORDS , f ) # Parse chunks associated with each key for key , chunkList in iteritems ( chunks ) : # Parse each chunk in the chunk list for chunk in chunkList : # Call chunk specific parsers for each chunk result = KEYWORDS [ key ] ( key , chunk ) # Cases if key == 'CONNECT' : connections . append ( result ) elif key == 'SJUNC' : sjuncs . append ( result ) elif key == 'SLINK' : slinks . append ( result ) # Create GSSHAPY objects self . _createConnection ( connections ) self . _createSjunc ( sjuncs ) self . _createSlink ( slinks )

1<CODESPLIT>https://github.com/projectatomic/atomic-reactor/blob/fd31c01b964097210bf169960d051e5f04019a80/atomic_reactor/odcs_util.py#L111-L128<CODESPLIT>ODCSClient.renew_compose<CODESPLIT>Renew or extend existing compose<CODESPLIT>def renew_compose ( self , compose_id ) : logger . info ( "Renewing compose %d" , compose_id ) response = self . session . patch ( '{}composes/{}' . format ( self . url , compose_id ) ) response . raise_for_status ( ) response_json = response . json ( ) compose_id = response_json [ 'id' ] logger . info ( "Renewed compose is %d" , compose_id ) return response_json

0<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/keystone.py#L950-L985<CODESPLIT>user_list<CODESPLIT>Return a specific users ( keystone user - get )<CODESPLIT>def user_list ( profile = None , * * connection_args ) : kstone = auth ( profile , * * connection_args ) ret = { } for user in kstone . users . list ( ) : ret [ user . name ] = dict ( ( value , getattr ( user , value , None ) ) for value in dir ( user ) if not value . startswith ( '_' ) and isinstance ( getattr ( user , value , None ) , ( six . string_types , dict , bool ) ) ) tenant_id = getattr ( user , 'tenantId' , None ) if tenant_id : ret [ user . name ] [ 'tenant_id' ] = tenant_id return ret

1<CODESPLIT>https://github.com/grundprinzip/pyxplorer/blob/34c1d166cfef4a94aeb6d5fcb3cbb726d48146e2/pyxplorer/types.py#L91-L105<CODESPLIT>Column.distribution<CODESPLIT>Build the distribution of distinct values<CODESPLIT>def distribution ( self , limit = 1024 ) : res = self . _qexec ( "%s, count(*) as __cnt" % self . name ( ) , group = "%s" % self . name ( ) , order = "__cnt DESC LIMIT %d" % limit ) dist = [ ] cnt = self . _table . size ( ) for i , r in enumerate ( res ) : dist . append ( list ( r ) + [ i , r [ 1 ] / float ( cnt ) ] ) self . _distribution = pd . DataFrame ( dist , columns = [ "value" , "cnt" , "r" , "fraction" ] ) self . _distribution . index = self . _distribution . r return self . _distribution

0<CODESPLIT>https://github.com/MIT-LCP/wfdb-python/blob/cc8c9e9e44f10af961b7a9d8ae03708b31ac8a8c/wfdb/io/_signal.py#L1798-L1824<CODESPLIT>describe_list_indices<CODESPLIT>Infer the length of a signal from a dat file .<CODESPLIT>def describe_list_indices ( full_list ) : unique_elements = [ ] element_indices = { } for i in range ( len ( full_list ) ) : item = full_list [ i ] # new item if item not in unique_elements : unique_elements . append ( item ) element_indices [ item ] = [ i ] # previously seen item else : element_indices [ item ] . append ( i ) return unique_elements , element_indices

1<CODESPLIT>https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/aws_adapter/repository/managers.py#L1707-L1729<CODESPLIT>RepositoryProxyManager.get_asset_lookup_session_for_repository<CODESPLIT>Gets the OsidSession associated with the asset lookup service for the given repository .<CODESPLIT>def get_asset_lookup_session_for_repository ( self , repository_id = None , proxy = None ) : return AssetLookupSession ( self . _provider_manager . get_asset_lookup_session_for_repository ( repository_id , proxy ) , self . _config_map )

0<CODESPLIT>https://github.com/pypa/setuptools/blob/83c667e0b2a98193851c07115d1af65011ed0fb6/pkg_resources/__init__.py#L1484-L1556<CODESPLIT>Distribution._get_metadata_path_for_display<CODESPLIT>Validate the resource paths according to the docs . https : // setuptools . readthedocs . io / en / latest / pkg_resources . html#basic - resource - access<CODESPLIT>def _get_metadata_path_for_display ( self , name ) : try : # We need to access _get_metadata_path() on the provider object # directly rather than through this class's __getattr__() # since _get_metadata_path() is marked private. path = self . _provider . _get_metadata_path ( name ) # Handle exceptions e.g. in case the distribution's metadata # provider doesn't support _get_metadata_path(). except Exception : return '[could not detect]' return path

0<CODESPLIT>https://github.com/emory-libraries/eulxml/blob/17d71c7d98c0cebda9932b7f13e72093805e1fe2/eulxml/forms/xmlobject.py#L44-L57<CODESPLIT>_collect_fields<CODESPLIT>Parse a list of field names possibly including dot - separated subform fields into an internal ParsedFieldList object representing the base fields and subform listed .<CODESPLIT>def _collect_fields ( field_parts_list , include_parents ) : fields = [ ] subpart_lists = defaultdict ( list ) for parts in field_parts_list : field , subparts = parts [ 0 ] , parts [ 1 : ] if subparts : if include_parents and field not in fields : fields . append ( field ) subpart_lists [ field ] . append ( subparts ) else : fields . append ( field ) subfields = dict ( ( field , _collect_fields ( subparts , include_parents ) ) for field , subparts in six . iteritems ( subpart_lists ) ) return ParsedFieldList ( fields , subfields )

1<CODESPLIT>https://github.com/ckan/ckan-service-provider/blob/83a42b027dba8a0b3ca7e5f689f990b7bc2cd7fa/ckanserviceprovider/web.py#L525-L546<CODESPLIT>job_data<CODESPLIT>Get the raw data that the job returned . The mimetype will be the value provided in the metdata for the key mimetype .<CODESPLIT>def job_data ( job_id ) : job_dict = db . get_job ( job_id ) if not job_dict : return json . dumps ( { 'error' : 'job_id not found' } ) , 404 , headers if not is_authorized ( job_dict ) : return json . dumps ( { 'error' : 'not authorized' } ) , 403 , headers if job_dict [ 'error' ] : return json . dumps ( { 'error' : job_dict [ 'error' ] } ) , 409 , headers content_type = job_dict [ 'metadata' ] . get ( 'mimetype' ) return flask . Response ( job_dict [ 'data' ] , mimetype = content_type )

1<CODESPLIT>https://github.com/svasilev94/GraphLibrary/blob/bf979a80bdea17eeb25955f0c119ca8f711ef62b/graphlibrary/prim.py#L8-L29<CODESPLIT>connected_components<CODESPLIT>Check if G is connected and return list of sets . Every set contains all vertices in one connected component .<CODESPLIT>def connected_components ( G ) : result = [ ] vertices = set ( G . vertices ) while vertices : n = vertices . pop ( ) group = { n } queue = Queue ( ) queue . put ( n ) while not queue . empty ( ) : n = queue . get ( ) neighbors = set ( G . vertices [ n ] ) neighbors . difference_update ( group ) vertices . difference_update ( neighbors ) group . update ( neighbors ) for element in neighbors : queue . put ( element ) result . append ( group ) return result

0<CODESPLIT>https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/command_line/aconvasp_caller.py#L37-L45<CODESPLIT>run_aconvasp_command<CODESPLIT>Get kpoint divisions for a given k - point density ( per reciprocal - atom ) : kppa and a given structure<CODESPLIT>def run_aconvasp_command ( command , structure ) : poscar = Poscar ( structure ) p = subprocess . Popen ( command , stdout = subprocess . PIPE , stdin = subprocess . PIPE , stderr = subprocess . PIPE ) output = p . communicate ( input = poscar . get_string ( ) ) return output

1<CODESPLIT>https://github.com/simonvh/genomepy/blob/abace2366511dbe855fe1430b1f7d9ec4cbf6d29/genomepy/plugin.py#L40-L54<CODESPLIT>convert<CODESPLIT>Convert CamelCase to underscore<CODESPLIT>def convert ( name ) : s1 = re . sub ( '(.)([A-Z][a-z]+)' , r'\1_\2' , name ) return re . sub ( '([a-z0-9])([A-Z])' , r'\1_\2' , s1 ) . lower ( )

0<CODESPLIT>https://github.com/google/python-gflags/blob/4f06c3d0d6cbe9b1fb90ee9fb1c082b3bf9285f6/gflags/flagvalues.py#L210-L222<CODESPLIT>FlagValues._FlagIsRegistered<CODESPLIT>Specifies that a flag is a key flag for a module .<CODESPLIT>def _FlagIsRegistered ( self , flag_obj ) : flag_dict = self . FlagDict ( ) # Check whether flag_obj is registered under its long name. name = flag_obj . name if flag_dict . get ( name , None ) == flag_obj : return True # Check whether flag_obj is registered under its short name. short_name = flag_obj . short_name if ( short_name is not None and flag_dict . get ( short_name , None ) == flag_obj ) : return True return False

1<CODESPLIT>https://github.com/python-diamond/Diamond/blob/0f3eb04327d6d3ed5e53a9967d6c9d2c09714a47/src/collectors/filestat/filestat.py#L231-L243<CODESPLIT>FilestatCollector.process_lsof<CODESPLIT>Get the list of users and file types to collect for and collect the data from lsof<CODESPLIT>def process_lsof ( self , users , types ) : d = { } for u in users : d [ u ] = { } tmp = os . popen ( "lsof -wbu %s | awk '{ print $5 }'" % ( u ) ) . read ( ) . split ( ) for t in types : d [ u ] [ t ] = tmp . count ( t ) return d

1<CODESPLIT>https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/cwl/hpc.py#L8-L40<CODESPLIT>create_cromwell_config<CODESPLIT>Prepare a cromwell configuration within the current working directory .<CODESPLIT>def create_cromwell_config ( args , work_dir , sample_file ) : docker_attrs = [ "String? docker" , "String? docker_user" ] cwl_attrs = [ "Int? cpuMin" , "Int? cpuMax" , "Int? memoryMin" , "Int? memoryMax" , "String? outDirMin" , "String? outDirMax" , "String? tmpDirMin" , "String? tmpDirMax" ] out_file = os . path . join ( work_dir , "bcbio-cromwell.conf" ) run_config = _load_custom_config ( args . runconfig ) if args . runconfig else { } # Avoid overscheduling jobs for local runs by limiting concurrent jobs # Longer term would like to keep these within defined core window joblimit = args . joblimit if joblimit == 0 and not args . scheduler : joblimit = 1 file_types = _get_filesystem_types ( args , sample_file ) std_args = { "docker_attrs" : "" if args . no_container else "\n        " . join ( docker_attrs ) , "submit_docker" : 'submit-docker: ""' if args . no_container else "" , "joblimit" : "concurrent-job-limit = %s" % ( joblimit ) if joblimit > 0 else "" , "cwl_attrs" : "\n        " . join ( cwl_attrs ) , "filesystem" : _get_filesystem_config ( file_types ) , "database" : run_config . get ( "database" , DATABASE_CONFIG % { "work_dir" : work_dir } ) } cl_args , conf_args , scheduler , cloud_type = _args_to_cromwell ( args ) std_args [ "engine" ] = _get_engine_filesystem_config ( file_types , args , conf_args ) conf_args . update ( std_args ) main_config = { "hpc" : ( HPC_CONFIGS [ scheduler ] % conf_args ) if scheduler else "" , "cloud" : ( CLOUD_CONFIGS [ cloud_type ] % conf_args ) if cloud_type else "" , "work_dir" : work_dir } main_config . update ( std_args ) # Local run always seems to need docker set because of submit-docker in default configuration # Can we unset submit-docker based on configuration so it doesn't inherit? # main_config["docker_attrs"] = "\n        ".join(docker_attrs) with open ( out_file , "w" ) as out_handle : out_handle . write ( CROMWELL_CONFIG % main_config ) return out_file

0<CODESPLIT>https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/snap.py#L104-L115<CODESPLIT>list_files<CODESPLIT>Restores a snapshot . : param res_id : the LUN number of primary LUN or snapshot mount point to be restored . : param backup_snap : the name of a backup snapshot to be created before restoring .<CODESPLIT>def list_files ( tag = '' , sat_id = None , data_path = None , format_str = None ) : if format_str is None and data_path is not None : file_base = 'supermag_magnetometer' if tag == "indices" or tag == "all" : file_base += '_all' # Can't just download indices if tag == "indices" : psplit = path . split ( data_path [ : - 1 ] ) data_path = path . join ( psplit [ 0 ] , "all" , "" ) if tag == "stations" : min_fmt = '_' . join ( [ file_base , '{year:4d}.???' ] ) doff = pds . DateOffset ( years = 1 ) else : min_fmt = '_' . join ( [ file_base , '{year:4d}{month:02d}{day:02d}.???' ] ) doff = pds . DateOffset ( days = 1 ) files = pysat . Files . from_os ( data_path = data_path , format_str = min_fmt ) # station files are once per year but we need to # create the illusion there is a file per year         if not files . empty : files = files . sort_index ( ) if tag == "stations" : orig_files = files . copy ( ) new_files = [ ] # Assigns the validity of each station file to be 1 year for orig in orig_files . iteritems ( ) : files . ix [ orig [ 0 ] + doff - pds . DateOffset ( days = 1 ) ] = orig [ 1 ] files = files . sort_index ( ) new_files . append ( files . ix [ orig [ 0 ] : orig [ 0 ] + doff - pds . DateOffset ( days = 1 ) ] . asfreq ( 'D' , method = 'pad' ) ) files = pds . concat ( new_files ) files = files . dropna ( ) files = files . sort_index ( ) # add the date to the filename files = files + '_' + files . index . strftime ( '%Y-%m-%d' ) return files elif format_str is None : estr = 'A directory must be passed to the loading routine for SuperMAG' raise ValueError ( estr ) else : return pysat . Files . from_os ( data_path = data_path , format_str = format_str )

1<CODESPLIT>https://github.com/harabchuk/kibana-dashboard-api/blob/8a13d5078fa92fb73f06498757ba9f51632e8a23/kibana_dashboard_api/paneltools.py#L28-L46<CODESPLIT>longest_lines<CODESPLIT>Creates lines from shape : param shape : : return : list of dictionaries with col row len fields<CODESPLIT>def longest_lines ( shape ) : lines = [ ] for level in set ( shape ) : count = 0 for i in range ( len ( shape ) ) : if shape [ i ] <= level : count += 1 elif count : lines . append ( { 'row' : level , 'col' : i - count + 1 , 'len' : count } ) count = 0 if count : lines . append ( { 'row' : level , 'col' : i - count + 2 , 'len' : count } ) return sorted ( lines , key = lambda l : l [ 'row' ] )

1<CODESPLIT>https://github.com/staticdev/django-sorting-bootstrap/blob/cfdc6e671b1b57aad04e44b041b9df10ee8288d3/sorting_bootstrap/templatetags/sorting_tags.py#L112-L168<CODESPLIT>sort_link<CODESPLIT>Usage : { % sort_link text field_name % } Usage : { % sort_link text field_name Visible name % }<CODESPLIT>def sort_link ( context , text , sort_field , visible_name = None ) : sorted_fields = False ascending = None class_attrib = 'sortable' orig_sort_field = sort_field if context . get ( 'current_sort_field' ) == sort_field : sort_field = '-%s' % sort_field visible_name = '-%s' % ( visible_name or orig_sort_field ) sorted_fields = True ascending = False class_attrib += ' sorted descending' elif context . get ( 'current_sort_field' ) == '-' + sort_field : visible_name = '%s' % ( visible_name or orig_sort_field ) sorted_fields = True ascending = True class_attrib += ' sorted ascending' if visible_name : if 'request' in context : request = context [ 'request' ] request . session [ visible_name ] = sort_field # builds url if 'request' in context : url = context [ 'request' ] . path else : url = "./" url += "?sort_by=" if visible_name is None : url += sort_field else : url += visible_name if 'getsortvars' in context : extra_vars = context [ 'getsortvars' ] else : if 'request' in context : request = context [ 'request' ] getvars = request . GET . copy ( ) if 'sort_by' in getvars : del getvars [ 'sort_by' ] if len ( getvars . keys ( ) ) > 0 : context [ 'getsortvars' ] = "&%s" % getvars . urlencode ( ) else : context [ 'getsortvars' ] = '' extra_vars = context [ 'getsortvars' ] # append other vars to url url += extra_vars return { 'text' : text , 'url' : url , 'ascending' : ascending , 'sorted_fields' : sorted_fields , 'class_attrib' : class_attrib }

0<CODESPLIT>https://github.com/pgjones/quart/blob/7cb2d3bd98e8746025764f2b933abc12041fa175/quart/app.py#L951-L970<CODESPLIT>Quart.handle_websocket_exception<CODESPLIT>Handle an uncaught exception .<CODESPLIT>async def handle_websocket_exception ( self , error : Exception ) -> Optional [ Response ] : await got_websocket_exception . send ( self , exception = error ) self . log_exception ( sys . exc_info ( ) ) internal_server_error = all_http_exceptions [ 500 ] ( ) handler = self . _find_exception_handler ( internal_server_error ) if handler is None : return internal_server_error . get_response ( ) else : return await self . finalize_websocket ( await handler ( error ) , from_error_handler = True )

0<CODESPLIT>https://github.com/yougov/vr.common/blob/ca8ed0c50ba873fc51fdfeeaa25d3b8ec1b54eb4/vr/common/utils.py#L139-L152<CODESPLIT>lock_file<CODESPLIT>Given a url like redis : // localhost : 6379 / 0 return a dict with host port and db members .<CODESPLIT>def lock_file ( f , block = False ) : try : flags = fcntl . LOCK_EX if not block : flags |= fcntl . LOCK_NB fcntl . flock ( f . fileno ( ) , flags ) except IOError as e : if e . errno in ( errno . EACCES , errno . EAGAIN ) : raise SystemExit ( "ERROR: %s is locked by another process." % f . name ) raise

1<CODESPLIT>https://github.com/glitchassassin/lackey/blob/7adadfacd7f45d81186710be992f5668b15399fe/lackey/RegionMatching.py#L804-L827<CODESPLIT>Region.drag<CODESPLIT>Starts a dragDrop operation .<CODESPLIT>def drag ( self , dragFrom = None ) : if dragFrom is None : dragFrom = self . _lastMatch or self # Whichever one is not None dragFromLocation = None if isinstance ( dragFrom , Pattern ) : dragFromLocation = self . find ( dragFrom ) . getTarget ( ) elif isinstance ( dragFrom , basestring ) : dragFromLocation = self . find ( dragFrom ) . getTarget ( ) elif isinstance ( dragFrom , Match ) : dragFromLocation = dragFrom . getTarget ( ) elif isinstance ( dragFrom , Region ) : dragFromLocation = dragFrom . getCenter ( ) elif isinstance ( dragFrom , Location ) : dragFromLocation = dragFrom else : raise TypeError ( "drag expected dragFrom to be Pattern, String, Match, Region, or Location object" ) Mouse . moveSpeed ( dragFromLocation , Settings . MoveMouseDelay ) time . sleep ( Settings . DelayBeforeMouseDown ) Mouse . buttonDown ( ) Debug . history ( "Began drag at {}" . format ( dragFromLocation ) )

1<CODESPLIT>https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/kern/src/todo/eq_ode1.py#L134-L184<CODESPLIT>Eq_ode1._dK_ode_dtheta<CODESPLIT>Do all the computations for the ode parts of the covariance function .<CODESPLIT>def _dK_ode_dtheta ( self , target ) : t_ode = self . _t [ self . _index > 0 ] dL_dK_ode = self . _dL_dK [ self . _index > 0 , : ] index_ode = self . _index [ self . _index > 0 ] - 1 if self . _t2 is None : if t_ode . size == 0 : return t2_ode = t_ode dL_dK_ode = dL_dK_ode [ : , self . _index > 0 ] index2_ode = index_ode else : t2_ode = self . _t2 [ self . _index2 > 0 ] dL_dK_ode = dL_dK_ode [ : , self . _index2 > 0 ] if t_ode . size == 0 or t2_ode . size == 0 : return index2_ode = self . _index2 [ self . _index2 > 0 ] - 1 h1 = self . _compute_H ( t_ode , index_ode , t2_ode , index2_ode , stationary = self . is_stationary , update_derivatives = True ) #self._dK_ddelay = self._dh_ddelay self . _dK_dsigma = self . _dh_dsigma if self . _t2 is None : h2 = h1 else : h2 = self . _compute_H ( t2_ode , index2_ode , t_ode , index_ode , stationary = self . is_stationary , update_derivatives = True ) #self._dK_ddelay += self._dh_ddelay.T self . _dK_dsigma += self . _dh_dsigma . T # C1 = self.sensitivity # C2 = self.sensitivity # K = 0.5 * (h1 + h2.T) # var2 = C1*C2 # if self.is_normalized: #     dk_dD1 = (sum(sum(dL_dK.*dh1_dD1)) + sum(sum(dL_dK.*dh2_dD1.T)))*0.5*var2 #     dk_dD2 = (sum(sum(dL_dK.*dh1_dD2)) + sum(sum(dL_dK.*dh2_dD2.T)))*0.5*var2 #     dk_dsigma = 0.5 * var2 * sum(sum(dL_dK.*dK_dsigma)) #     dk_dC1 = C2 * sum(sum(dL_dK.*K)) #     dk_dC2 = C1 * sum(sum(dL_dK.*K)) # else: #     K = np.sqrt(np.pi) * K #     dk_dD1 = (sum(sum(dL_dK.*dh1_dD1)) + * sum(sum(dL_dK.*K)) #     dk_dC2 = self.sigma * C1 * sum(sum(dL_dK.*K)) # dk_dSim1Variance = dk_dC1 # Last element is the length scale. ( dL_dK_ode [ : , : , None ] * self . _dh_ddelay [ : , None , : ] ) . sum ( 2 ) target [ - 1 ] += ( dL_dK_ode * self . _dK_dsigma / np . sqrt ( 2 ) ) . sum ( )

1<CODESPLIT>https://github.com/moliware/dicts/blob/0e8258cc3dc00fe929685cae9cda062222722715/dicts/dict.py#L21-L28<CODESPLIT>Dict.join<CODESPLIT>Add dic pairs to self . data<CODESPLIT>def join ( self , dic ) : for k , v in dic . iteritems ( ) : if k in self . data : self [ k ] += dic [ k ] else : self [ k ] = deepcopy ( v ) return self

0<CODESPLIT>https://github.com/androguard/androguard/blob/984c0d981be2950cf0451e484f7b0d4d53bc4911/androguard/core/androconf.py#L243-L255<CODESPLIT>rrmdir<CODESPLIT>enable log messages on stdout<CODESPLIT>def rrmdir ( directory ) : for root , dirs , files in os . walk ( directory , topdown = False ) : for name in files : os . remove ( os . path . join ( root , name ) ) for name in dirs : os . rmdir ( os . path . join ( root , name ) ) os . rmdir ( directory )

1<CODESPLIT>https://github.com/cltk/cltk/blob/ed9c025b7ec43c949481173251b70e05e4dffd27/cltk/prosody/latin/string_utils.py#L65-L79<CODESPLIT>mark_list<CODESPLIT>Given a string return a list of index positions where a character / non blank space exists .<CODESPLIT>def mark_list ( line : str ) -> List [ int ] : marks = [ ] for idx , car in enumerate ( list ( line ) ) : if car != " " : marks . append ( idx ) return marks

0<CODESPLIT>https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/stats/admixture.py#L14-L43<CODESPLIT>randomized_pca<CODESPLIT>Unbiased estimator for h where 2 * h is the heterozygosity of the population .<CODESPLIT>def randomized_pca ( gn , n_components = 10 , copy = True , iterated_power = 3 , random_state = None , scaler = 'patterson' , ploidy = 2 ) : # set up the model model = GenotypeRandomizedPCA ( n_components , copy = copy , iterated_power = iterated_power , random_state = random_state , scaler = scaler , ploidy = ploidy ) # fit the model and project the input data onto the new dimensions coords = model . fit_transform ( gn ) return coords , model

0<CODESPLIT>https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/broad/__init__.py#L40-L57<CODESPLIT>_clean_java_out<CODESPLIT>Retrieve GATK memory specifications moving down a list of potential specifications .<CODESPLIT>def _clean_java_out ( version_str ) : out = [ ] for line in version_str . decode ( ) . split ( "\n" ) : if line . startswith ( "Picked up" ) : pass if line . find ( "setlocale" ) > 0 : pass else : out . append ( line ) return "\n" . join ( out )

0<CODESPLIT>https://github.com/Alignak-monitoring/alignak/blob/f3c145207e83159b799d3714e4241399c7740a64/alignak/objects/schedulingitem.py#L3233-L3241<CODESPLIT>SchedulingItem.set_unreachable<CODESPLIT>Unset impact only if impact state change is set in configuration<CODESPLIT>def set_unreachable ( self ) : self . state_id = 4 self . state = u'UNREACHABLE' self . last_time_unreachable = int ( time . time ( ) )

0<CODESPLIT>https://github.com/juju/charm-helpers/blob/aa785c40c3b7a8c69dbfbc7921d6b9f30142e171/charmhelpers/contrib/database/mysql.py#L265-L293<CODESPLIT>MySQLHelper.set_mysql_password<CODESPLIT>Retrieve generate or store a mysql password for the provided username using peer relation cluster .<CODESPLIT>def set_mysql_password ( self , username , password ) : if username is None : username = 'root' # get root password via leader-get, it may be that in the past (when # changes to root-password were not supported) the user changed the # password, so leader-get is more reliable source than # config.previous('root-password'). rel_username = None if username == 'root' else username cur_passwd = self . get_mysql_password ( rel_username ) # password that needs to be set new_passwd = password # update password for all users (e.g. root@localhost, root@::1, etc) try : self . connect ( user = username , password = cur_passwd ) cursor = self . connection . cursor ( ) except MySQLdb . OperationalError as ex : raise MySQLSetPasswordError ( ( 'Cannot connect using password in ' 'leader settings (%s)' ) % ex , ex ) try : # NOTE(freyes): Due to skip-name-resolve root@$HOSTNAME account # fails when using SET PASSWORD so using UPDATE against the # mysql.user table is needed, but changes to this table are not # replicated across the cluster, so this update needs to run in # all the nodes. More info at # http://galeracluster.com/documentation-webpages/userchanges.html release = CompareHostReleases ( lsb_release ( ) [ 'DISTRIB_CODENAME' ] ) if release < 'bionic' : SQL_UPDATE_PASSWD = ( "UPDATE mysql.user SET password = " "PASSWORD( %s ) WHERE user = %s;" ) else : # PXC 5.7 (introduced in Bionic) uses authentication_string SQL_UPDATE_PASSWD = ( "UPDATE mysql.user SET " "authentication_string = " "PASSWORD( %s ) WHERE user = %s;" ) cursor . execute ( SQL_UPDATE_PASSWD , ( new_passwd , username ) ) cursor . execute ( 'FLUSH PRIVILEGES;' ) self . connection . commit ( ) except MySQLdb . OperationalError as ex : raise MySQLSetPasswordError ( 'Cannot update password: %s' % str ( ex ) , ex ) finally : cursor . close ( ) # check the password was changed try : self . connect ( user = username , password = new_passwd ) self . execute ( 'select 1;' ) except MySQLdb . OperationalError as ex : raise MySQLSetPasswordError ( ( 'Cannot connect using new password: ' '%s' ) % str ( ex ) , ex ) if not is_leader ( ) : log ( 'Only the leader can set a new password in the relation' , level = DEBUG ) return for key in self . passwd_keys ( rel_username ) : _password = leader_get ( key ) if _password : log ( 'Updating password for %s (%s)' % ( key , rel_username ) , level = DEBUG ) leader_set ( settings = { key : new_passwd } )

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/nftables.py#L522-L548<CODESPLIT>check_table<CODESPLIT>Check for the existence of a table<CODESPLIT>def check_table ( table = None , family = 'ipv4' ) : ret = { 'comment' : '' , 'result' : False } if not table : ret [ 'comment' ] = 'Table needs to be specified' return ret nft_family = _NFTABLES_FAMILIES [ family ] cmd = '{0} list tables {1}' . format ( _nftables_cmd ( ) , nft_family ) out = __salt__ [ 'cmd.run' ] ( cmd , python_shell = False ) . find ( 'table {0} {1}' . format ( nft_family , table ) ) if out == - 1 : ret [ 'comment' ] = 'Table {0} in family {1} does not exist' . format ( table , family ) else : ret [ 'comment' ] = 'Table {0} in family {1} exists' . format ( table , family ) ret [ 'result' ] = True return ret

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/data.py#L89-L102<CODESPLIT>update<CODESPLIT>Update a key with a value in the minion datastore<CODESPLIT>def update ( key , value ) : store = load ( ) store [ key ] = value dump ( store ) return True

1<CODESPLIT>https://github.com/pyapi-gitlab/pyapi-gitlab/blob/f74b6fb5c13cecae9524997847e928905cc60acf/gitlab/__init__.py#L2175-L2193<CODESPLIT>Gitlab.editlabel<CODESPLIT>Updates an existing label with new name or now color . At least one parameter is required to update the label .<CODESPLIT>def editlabel ( self , project_id , name , new_name = None , color = None ) : data = { 'name' : name , 'new_name' : new_name , 'color' : color } request = requests . put ( '{0}/{1}/labels' . format ( self . projects_url , project_id ) , data = data , verify = self . verify_ssl , auth = self . auth , headers = self . headers , timeout = self . timeout ) if request . status_code == 200 : return request . json ( ) else : return False

0<CODESPLIT>https://github.com/tkem/uritools/blob/e77ba4acd937b68da9850138563debd4c925ef9f/uritools/split.py#L248-L251<CODESPLIT>SplitResult.getquerylist<CODESPLIT>Return : const : True if this is an absolute - path reference .<CODESPLIT>def getquerylist ( self , sep = '&' , encoding = 'utf-8' , errors = 'strict' ) : if not self . query : return [ ] elif isinstance ( sep , type ( self . query ) ) : qsl = self . query . split ( sep ) elif isinstance ( sep , bytes ) : qsl = self . query . split ( sep . decode ( 'ascii' ) ) else : qsl = self . query . split ( sep . encode ( 'ascii' ) ) items = [ ] for parts in [ qs . partition ( self . EQ ) for qs in qsl if qs ] : name = uridecode ( parts [ 0 ] , encoding , errors ) if parts [ 1 ] : value = uridecode ( parts [ 2 ] , encoding , errors ) else : value = None items . append ( ( name , value ) ) return items

1<CODESPLIT>https://github.com/manns/pyspread/blob/0e2fd44c2e0f06605efc3058c20a43a8c1f9e7e0/pyspread/src/lib/_grid_cairo_renderer.py#L1051-L1061<CODESPLIT>Cell.get_right_key_rect<CODESPLIT>Returns tuple key rect of right cell<CODESPLIT>def get_right_key_rect ( self ) : key_right = self . row , self . col + 1 , self . tab border_width_right = float ( self . cell_attributes [ self . key ] [ "borderwidth_right" ] ) / 2.0 rect_right = ( self . x + self . width , self . y , border_width_right , self . height ) return key_right , rect_right

0<CODESPLIT>https://github.com/aiogram/aiogram/blob/2af930149ce2482547721e2c8755c10307295e48/aiogram/dispatcher/webhook.py#L535-L546<CODESPLIT>ForwardMessage.message<CODESPLIT>Write line<CODESPLIT>def message ( self , message : types . Message ) : setattr ( self , 'from_chat_id' , message . chat . id ) setattr ( self , 'message_id' , message . message_id ) return self

1<CODESPLIT>https://github.com/dailymotion/dailymotion-sdk-python/blob/944a8a474119c87d351504b0865e21216e286559/dailymotion.py#L164-L215<CODESPLIT>Dailymotion.set_grant_type<CODESPLIT>Grant types : - token : An authorization is requested to the end - user by redirecting it to an authorization page hosted on Dailymotion . Once authorized a refresh token is requested by the API client to the token server and stored in the end - user s cookie ( or other storage technique implemented by subclasses ) . The refresh token is then used to request time limited access token to the token server .<CODESPLIT>def set_grant_type ( self , grant_type = 'client_credentials' , api_key = None , api_secret = None , scope = None , info = None ) : self . access_token = None if api_key and api_secret : self . _grant_info [ 'key' ] = api_key self . _grant_info [ 'secret' ] = api_secret else : raise DailymotionClientError ( 'Missing API key/secret' ) if isinstance ( info , dict ) : self . _grant_info . update ( info ) else : info = { } if self . _session_store_enabled and isinstance ( info , dict ) and info . get ( 'username' ) is not None : self . _session_store . set_user ( info . get ( 'username' ) ) if grant_type in ( 'authorization' , 'token' ) : grant_type = 'authorization' if 'redirect_uri' not in info : raise DailymotionClientError ( 'Missing redirect_uri in grant info for token grant type.' ) elif grant_type in ( 'client_credentials' , 'none' ) : grant_type = 'client_credentials' elif grant_type == 'password' : if 'username' not in info or 'password' not in info : raise DailymotionClientError ( 'Missing username or password in grant info for password grant type.' ) self . _grant_type = grant_type if scope : if not isinstance ( scope , ( list , tuple ) ) : raise DailymotionClientError ( 'Invalid scope type: must be a list of valid scopes' ) self . _grant_info [ 'scope' ] = scope

0<CODESPLIT>https://github.com/lehins/django-smartfields/blob/23d4b0b18352f4f40ce8c429735e673ba5191502/smartfields/managers.py#L132-L161<CODESPLIT>all_phrase_translations<CODESPLIT>Processing is triggered by field s pre_save method . It will be executed if field s value has been changed ( known through descriptor and stashing logic ) or if model instance has never been saved before i . e . no pk set because there is a chance that field was initialized through model s __init__ hence default value was stashed with pre_init handler .<CODESPLIT>def all_phrase_translations ( phrase ) : if not trees : init ( ) phrase = phrase . split ( string . whitespace ) for word in phrase : for x in range ( len ( word ) ) : for translation in _words_at_the_beginning ( word [ x + 1 : ] , trees [ 'simplified' ] [ word [ x ] ] , prefix = word [ x ] ) : yield translation

0<CODESPLIT>https://github.com/kissmetrics/py-KISSmetrics/blob/705bf3fe26dd440abdf37bf213396b68af385523/KISSmetrics/client_compat.py#L57-L81<CODESPLIT>ClientCompat.set<CODESPLIT>Record event for identity with any properties .<CODESPLIT>def set ( self , data , path = KISSmetrics . SET_PATH , resp = False ) : self . check_id_key ( ) timestamp = None response = self . client . set ( person = self . identity , properties = data , timestamp = timestamp , path = path ) if resp : return response

1<CODESPLIT>https://github.com/FNNDSC/pftree/blob/b841e337c976bce151735f9d5dd95eded62aa094/pftree/pftree.py#L729-L769<CODESPLIT>pftree.stats_compute<CODESPLIT>Simply loop over the internal dictionary and echo the list size at each key ( i . e . the number of files ) .<CODESPLIT>def stats_compute ( self , * args , * * kwargs ) : totalElements = 0 totalKeys = 0 totalSize = 0 l_stats = [ ] d_report = { } for k , v in sorted ( self . d_inputTreeCallback . items ( ) , key = lambda kv : ( kv [ 1 ] [ 'diskUsage_raw' ] ) , reverse = self . b_statsReverse ) : str_report = "files: %5d; raw size: %12d; human size: %8s; %s" % ( len ( self . d_inputTree [ k ] ) , self . d_inputTreeCallback [ k ] [ 'diskUsage_raw' ] , self . d_inputTreeCallback [ k ] [ 'diskUsage_human' ] , k ) d_report = { 'files' : len ( self . d_inputTree [ k ] ) , 'diskUsage_raw' : self . d_inputTreeCallback [ k ] [ 'diskUsage_raw' ] , 'diskUsage_human' : self . d_inputTreeCallback [ k ] [ 'diskUsage_human' ] , 'path' : k } self . dp . qprint ( str_report , level = 1 ) l_stats . append ( d_report ) totalElements += len ( v ) totalKeys += 1 totalSize += self . d_inputTreeCallback [ k ] [ 'diskUsage_raw' ] str_totalSize_human = self . sizeof_fmt ( totalSize ) return { 'status' : True , 'dirs' : totalKeys , 'files' : totalElements , 'totalSize' : totalSize , 'totalSize_human' : str_totalSize_human , 'l_stats' : l_stats , 'runTime' : other . toc ( ) }

0<CODESPLIT>https://github.com/astrocatalogs/astrocats/blob/11abc3131c6366ecd23964369e55ff264add7805/astrocats/catalog/entry.py#L803-L831<CODESPLIT>Entry.add_model<CODESPLIT>Add a Spectrum instance to this entry .<CODESPLIT>def add_model ( self , allow_alias = False , * * kwargs ) : if not allow_alias and MODEL . ALIAS in kwargs : err_str = "`{}` passed in kwargs, this shouldn't happen!" . format ( SOURCE . ALIAS ) self . _log . error ( err_str ) raise RuntimeError ( err_str ) # Set alias number to be +1 of current number of models if MODEL . ALIAS not in kwargs : kwargs [ MODEL . ALIAS ] = str ( self . num_models ( ) + 1 ) model_obj = self . _init_cat_dict ( Model , self . _KEYS . MODELS , * * kwargs ) if model_obj is None : return None for item in self . get ( self . _KEYS . MODELS , '' ) : if model_obj . is_duplicate_of ( item ) : return item [ item . _KEYS . ALIAS ] self . setdefault ( self . _KEYS . MODELS , [ ] ) . append ( model_obj ) return model_obj [ model_obj . _KEYS . ALIAS ]

1<CODESPLIT>https://github.com/PureStorage-OpenConnect/rest-client/blob/097d5f2bc6facf607d7e4a92567b09fb8cf5cb34/purestorage/purestorage.py#L3234-L3260<CODESPLIT>FlashArray.list_certificates<CODESPLIT>Get the attributes of the current array certificate .<CODESPLIT>def list_certificates ( self ) : # This call takes no parameters. if self . _rest_version >= LooseVersion ( "1.12" ) : return self . _request ( "GET" , "cert" ) else : # If someone tries to call this against a too-early api version, # do the best we can to provide expected behavior. cert = self . _request ( "GET" , "cert" ) out = ResponseList ( [ cert ] ) out . headers = cert . headers return out

1<CODESPLIT>https://github.com/HazyResearch/metal/blob/c24e3772e25ac6d0917b8b7af4c1bcb92928f84a/metal/contrib/modules/sparse_linear_module.py#L25-L33<CODESPLIT>SparseLinearModule.forward<CODESPLIT>Execute sparse linear layer<CODESPLIT>def forward ( self , X ) : return self . W ( X ) . sum ( dim = 1 ) + self . b

0<CODESPLIT>https://github.com/tomnor/channelpack/blob/9ad3cd11c698aed4c0fc178385b2ba38a7d0efae/channelpack/pack.py#L1255-L1298<CODESPLIT>dbfpack<CODESPLIT>Return a ChannelPack instance loaded with data from the spread sheet file fn ( xls xlsx ) .<CODESPLIT>def dbfpack ( fn , usecols = None ) : loadfunc = pulldbf . dbf_asdict cp = ChannelPack ( loadfunc ) cp . load ( fn , usecols ) names = pulldbf . channel_names ( fn , usecols ) cp . set_channel_names ( names ) # cp.set_basefilemtime() return cp

1<CODESPLIT>https://github.com/pantsbuild/pants/blob/b72e650da0df685824ffdcc71988b8c282d0962d/src/python/pants/base/mustache.py#L88-L98<CODESPLIT>MustacheRenderer._get_template_text_from_package<CODESPLIT>Load the named template embedded in our package .<CODESPLIT>def _get_template_text_from_package ( self , template_name ) : if self . _package_name is None : raise self . MustacheError ( 'No package specified for template loading.' ) path = os . path . join ( 'templates' , template_name + '.mustache' ) template_text = pkgutil . get_data ( self . _package_name , path ) if template_text is None : raise self . MustacheError ( 'could not find template {} in package {}' . format ( path , self . _package_name ) ) return template_text . decode ( 'utf8' )

0<CODESPLIT>https://github.com/jefflovejapan/drench/blob/e99a8bf844a61d909d2d57629937ac672810469c/drench/drench.py#L153-L172<CODESPLIT>Torrent.tracker_request<CODESPLIT>Builds the payload that will be sent in tracker_request<CODESPLIT>def tracker_request ( self ) : assert self . torrent_dict [ 'info' ] payload = self . build_payload ( ) if self . torrent_dict [ 'announce' ] . startswith ( 'udp' ) : raise Exception ( 'need to deal with UDP' ) else : self . r = requests . get ( self . torrent_dict [ 'announce' ] , params = payload ) # Decoding response from tracker self . tracker_response = tparser . bdecode ( self . r . content ) self . get_peer_ips ( )

0<CODESPLIT>https://github.com/oasiswork/zimsoap/blob/d1ea2eb4d50f263c9a16e5549af03f1eff3e295e/zimsoap/client.py#L1570-L1575<CODESPLIT>ZimbraMailClient.modify_folder_grant<CODESPLIT>: param folder_ids : list of ids : param path : list of folder s paths<CODESPLIT>def modify_folder_grant ( self , folder_ids , perm , zid = None , grantee_name = None , gt = 'usr' , flags = None ) : f_ids = self . _return_comma_list ( folder_ids ) params = { 'action' : { 'id' : f_ids , 'op' : 'grant' , 'grant' : { 'perm' : perm , 'gt' : gt } } } if perm == 'none' : params [ 'action' ] [ 'op' ] = '!grant' params [ 'action' ] [ 'zid' ] = zid # Remove key to raise Zimsoap exception if no zid provided if not zid : params [ 'action' ] . pop ( 'zid' , None ) if grantee_name : params [ 'action' ] [ 'grant' ] [ 'd' ] = grantee_name elif zid : params [ 'action' ] [ 'grant' ] [ 'zid' ] = zid else : raise TypeError ( 'missing zid or grantee_name' ) self . request ( 'FolderAction' , params )

0<CODESPLIT>https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/tasks.py#L1804-L1815<CODESPLIT>Task.set_status<CODESPLIT>Unlock the task set its status to S_READY so that the scheduler can submit it . source_node is the : class : Node that removed the lock Call task . check_status if check_status is True .<CODESPLIT>def set_status ( self , status , msg ) : # truncate string if it's long. msg will be logged in the object and we don't want to waste memory. if len ( msg ) > 2000 : msg = msg [ : 2000 ] msg += "\n... snip ...\n" # Locked files must be explicitly unlocked if self . status == self . S_LOCKED or status == self . S_LOCKED : err_msg = ( "Locked files must be explicitly unlocked before calling set_status but\n" "task.status = %s, input status = %s" % ( self . status , status ) ) raise RuntimeError ( err_msg ) status = Status . as_status ( status ) changed = True if hasattr ( self , "_status" ) : changed = ( status != self . _status ) self . _status = status if status == self . S_RUN : # Set datetimes.start when the task enters S_RUN if self . datetimes . start is None : self . datetimes . start = datetime . datetime . now ( ) # Add new entry to history only if the status has changed. if changed : if status == self . S_SUB : self . datetimes . submission = datetime . datetime . now ( ) self . history . info ( "Submitted with MPI=%s, Omp=%s, Memproc=%.1f [Gb] %s " % ( self . mpi_procs , self . omp_threads , self . mem_per_proc . to ( "Gb" ) , msg ) ) elif status == self . S_OK : self . history . info ( "Task completed %s" , msg ) elif status == self . S_ABICRITICAL : self . history . info ( "Status set to S_ABI_CRITICAL due to: %s" , msg ) else : self . history . info ( "Status changed to %s. msg: %s" , status , msg ) ####################################################### # The section belows contains callbacks that should not # be executed if we are in spectator_mode ####################################################### if status == self . S_DONE : # Execute the callback self . _on_done ( ) if status == self . S_OK : # Finalize the task. if not self . finalized : self . _on_ok ( ) # here we remove the output files of the task and of its parents. if self . gc is not None and self . gc . policy == "task" : self . clean_output_files ( ) if self . status == self . S_OK : # Because _on_ok might have changed the status. self . send_signal ( self . S_OK ) return status

1<CODESPLIT>https://github.com/Parquery/sphinx-icontract/blob/92918f23a8ea1873112e9b7446c64cd6f12ee04b/sphinx_icontract/__init__.py#L303-L343<CODESPLIT>_capture_as_text<CODESPLIT>Convert the capture function into its text representation by parsing the source code of the decorator .<CODESPLIT>def _capture_as_text ( capture : Callable [ ... , Any ] ) -> str : if not icontract . _represent . _is_lambda ( a_function = capture ) : signature = inspect . signature ( capture ) param_names = list ( signature . parameters . keys ( ) ) return "{}({})" . format ( capture . __qualname__ , ", " . join ( param_names ) ) lines , lineno = inspect . findsource ( capture ) filename = inspect . getsourcefile ( capture ) decorator_inspection = icontract . _represent . inspect_decorator ( lines = lines , lineno = lineno , filename = filename ) call_node = decorator_inspection . node capture_node = None # type: Optional[ast.Lambda] if len ( call_node . args ) > 0 : assert isinstance ( call_node . args [ 0 ] , ast . Lambda ) , ( "Expected the first argument to the snapshot decorator to be a condition as lambda AST node, " "but got: {}" ) . format ( type ( call_node . args [ 0 ] ) ) capture_node = call_node . args [ 0 ] elif len ( call_node . keywords ) > 0 : for keyword in call_node . keywords : if keyword . arg == "capture" : assert isinstance ( keyword . value , ast . Lambda ) , "Expected lambda node as value of the 'capture' argument to the decorator." capture_node = keyword . value break assert capture_node is not None , "Expected to find a keyword AST node with 'capture' arg, but found none" else : raise AssertionError ( "Expected a call AST node of a snapshot decorator to have either args or keywords, but got: {}" . format ( ast . dump ( call_node ) ) ) capture_text = decorator_inspection . atok . get_text ( capture_node . body ) return capture_text

0<CODESPLIT>https://github.com/miquelo/resort/blob/097a25d3257c91a75c194fd44c2797ab356f85dd/packages/resort/engine/__init__.py#L409-L427<CODESPLIT>ComponentStub.delete<CODESPLIT>Get the specified component . : param str comp_name : Specified component name . : rtype : ComponentStub : return : Specified component stub .<CODESPLIT>def delete ( self , context , plan ) : op = execution . Delete ( self . __comp_name , self . __comp ( ) ) if op not in plan and self . available ( context ) != False : for dep_stub in self . __dependents ( self . __comp_stub_reg . get ( None ) ) : dep_stub . delete ( context , plan ) plan . append ( op )

1<CODESPLIT>https://github.com/neovim/pynvim/blob/5e577188e6d7133f597ad0ce60dc6a4b1314064a/scripts/logging_statement_modifier.py#L281-L309<CODESPLIT>disable_logging<CODESPLIT>Disables logging statements in these lines whose logging level falls between the specified minimum and maximum levels .<CODESPLIT>def disable_logging ( lines , min_level_value , max_level_value ) : output = '' while lines : line = lines [ 0 ] ret = RE_LOGGING_START . match ( line ) if not ret : # no logging statement here, so just leave the line as-is and keep going output += line lines = lines [ 1 : ] else : # a logging call has started: find all the lines it includes and those it does not logging_lines , remaining_lines = split_call ( lines ) lines = remaining_lines logging_stmt = '' . join ( logging_lines ) # replace the logging statement if its level falls b/w min and max if not check_level ( logging_stmt , False , min_level_value , max_level_value ) : output += logging_stmt else : # comment out this logging statement and replace it with pass prefix_ws = ret . group ( 1 ) pass_stmt = prefix_ws + PASS_LINE_CONTENTS commented_out_logging_lines = comment_lines ( logging_lines ) new_lines = pass_stmt + commented_out_logging_lines logging . info ( 'replacing:\n%s\nwith this:\n%s' % ( logging_stmt . rstrip ( ) , new_lines . rstrip ( ) ) ) output += new_lines return output

1<CODESPLIT>https://github.com/budacom/trading-bots/blob/8cb68bb8d0b5f822108db1cc5dae336e3d3c3452/trading_bots/contrib/clients.py#L448-L450<CODESPLIT>TradingClient.fetch_closed_orders_since<CODESPLIT>Fetch closed orders since the given timestamp .<CODESPLIT>def fetch_closed_orders_since ( self , since : int ) -> List [ Order ] : return self . _fetch_orders_since ( self . _closed_orders_since , since )

0<CODESPLIT>https://github.com/galaxy-genome-annotation/python-apollo/blob/2bc9991302abe4402ec2885dcaac35915475b387/arrow/commands/annotations/merge_exons.py#L22-L29<CODESPLIT>cli<CODESPLIT>Merge two exons<CODESPLIT>def cli ( ctx , feature_id , organism = "" , sequence = "" ) : return ctx . gi . annotations . set_readthrough_stop_codon ( feature_id , organism = organism , sequence = sequence )

1<CODESPLIT>https://github.com/JasonKessler/scattertext/blob/cacf1f687d218ee8cae3fc05cc901db824bb1b81/scattertext/__init__.py#L1183-L1236<CODESPLIT>produce_projection_explorer<CODESPLIT>Parameters ---------- corpus : ParsedCorpus It is highly recommended to use a stoplisted unigram corpus -- corpus . get_stoplisted_unigram_corpus () category : str word2vec_model : Word2Vec A gensim word2vec model . A default model will be used instead . See Word2VecFromParsedCorpus for the default model . projection_model : sklearn - style dimensionality reduction model . By default : umap . UMAP ( min_dist = 0 . 5 metric = cosine ) You could also use e . g . sklearn . manifold . TSNE ( perplexity = 10 n_components = 2 init = pca n_iter = 2500 random_state = 23 ) embeddings : array [ len ( corpus . get_terms () ) X ] Word embeddings . If None ( default ) wil train them using word2vec Model term_acceptance_re : SRE_Pattern Regular expression to identify valid terms show_axes : bool default False Show the ticked axes on the plot . If false show inner axes as a crosshair . kwargs : dict Remaining produce_scattertext_explorer keywords get_tooltip_content<CODESPLIT>def produce_projection_explorer ( corpus , category , word2vec_model = None , projection_model = None , embeddings = None , term_acceptance_re = re . compile ( '[a-z]{3,}' ) , show_axes = False , * * kwargs ) : embeddings_resolover = EmbeddingsResolver ( corpus ) if embeddings is not None : embeddings_resolover . set_embeddings ( embeddings ) else : embeddings_resolover . set_embeddings_model ( word2vec_model , term_acceptance_re ) corpus , word_axes = embeddings_resolover . project_embeddings ( projection_model , x_dim = 0 , y_dim = 1 ) html = produce_scattertext_explorer ( corpus = corpus , category = category , minimum_term_frequency = 0 , sort_by_dist = False , x_coords = scale ( word_axes [ 'x' ] ) , y_coords = scale ( word_axes [ 'y' ] ) , y_label = '' , x_label = '' , show_axes = show_axes , * * kwargs ) return html

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/eselect.py#L159-L201<CODESPLIT>set_target<CODESPLIT>Set the target for the given module . Target can be specified by index or name .<CODESPLIT>def set_target ( module , target , module_parameter = None , action_parameter = None ) : if action_parameter : action_parameter = '{0} {1}' . format ( action_parameter , target ) else : action_parameter = target # get list of available modules if module not in get_modules ( ) : log . error ( 'Module %s not available' , module ) return False exec_result = exec_action ( module , 'set' , module_parameter = module_parameter , action_parameter = action_parameter , state_only = True ) if exec_result : return exec_result return False

1<CODESPLIT>https://github.com/joshspeagle/dynesty/blob/9e482aafeb5cf84bedb896fa6f07a761d917983e/dynesty/sampler.py#L242-L259<CODESPLIT>Sampler._beyond_unit_bound<CODESPLIT>Check whether we should update our bound beyond the initial unit cube .<CODESPLIT>def _beyond_unit_bound ( self , loglstar ) : if self . logl_first_update is None : # If we haven't already updated our bounds, check if we satisfy # the provided criteria for establishing the first bounding update. check = ( self . ncall > self . ubound_ncall and self . eff < self . ubound_eff ) if check : # Save the log-likelihood where our first update took place. self . logl_first_update = loglstar return check else : # If we've already update our bounds, check if we've exceeded the # saved log-likelihood threshold. (This is useful when sampling # within `dynamicsampler`). return loglstar >= self . logl_first_update

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/cloud/clouds/linode.py#L1275-L1315<CODESPLIT>show_instance<CODESPLIT>Displays details about a particular Linode VM . Either a name or a linode_id must be provided .<CODESPLIT>def show_instance ( name , call = None ) : if call != 'action' : raise SaltCloudException ( 'The show_instance action must be called with -a or --action.' ) node_id = get_linode_id_from_name ( name ) node_data = get_linode ( kwargs = { 'linode_id' : node_id } ) ips = get_ips ( node_id ) state = int ( node_data [ 'STATUS' ] ) ret = { 'id' : node_data [ 'LINODEID' ] , 'image' : node_data [ 'DISTRIBUTIONVENDOR' ] , 'name' : node_data [ 'LABEL' ] , 'size' : node_data [ 'TOTALRAM' ] , 'state' : _get_status_descr_by_id ( state ) , 'private_ips' : ips [ 'private_ips' ] , 'public_ips' : ips [ 'public_ips' ] } return ret

1<CODESPLIT>https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/storage/memory.py#L173-L182<CODESPLIT>RegionMap.unmap_by_address<CODESPLIT>Removes a mapping based on its absolute address .<CODESPLIT>def unmap_by_address ( self , absolute_address ) : desc = self . _address_to_region_id [ absolute_address ] del self . _address_to_region_id [ absolute_address ] del self . _region_id_to_address [ desc . region_id ]

0<CODESPLIT>https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/namespacebrowser.py#L278-L344<CODESPLIT>NamespaceBrowser.set_data<CODESPLIT>Import data from text file .<CODESPLIT>def set_data ( self , data ) : if data != self . editor . model . get_data ( ) : self . editor . set_data ( data ) self . editor . adjust_columns ( )

1<CODESPLIT>https://github.com/ethereum/web3.py/blob/71b8bf03dc6d332dd97d8902a38ffab6f8b5a5ab/web3/contract.py#L1427-L1454<CODESPLIT>build_transaction_for_function<CODESPLIT>Builds a dictionary with the fields required to make the given transaction<CODESPLIT>def build_transaction_for_function ( address , web3 , function_name = None , transaction = None , contract_abi = None , fn_abi = None , * args , * * kwargs ) : prepared_transaction = prepare_transaction ( address , web3 , fn_identifier = function_name , contract_abi = contract_abi , fn_abi = fn_abi , transaction = transaction , fn_args = args , fn_kwargs = kwargs , ) prepared_transaction = fill_transaction_defaults ( web3 , prepared_transaction ) return prepared_transaction

0<CODESPLIT>https://github.com/quantumlib/Cirq/blob/0827da80dd7880e5b923eb69407e980ed9bc0bd2/cirq/circuits/circuit_dag.py#L49-L51<CODESPLIT>Moment._approx_eq_<CODESPLIT>Returns true only if the operations have qubits in common .<CODESPLIT>def _approx_eq_ ( self , other : Any , atol : Union [ int , float ] ) -> bool : if not isinstance ( other , type ( self ) ) : return NotImplemented return approx_eq ( self . operations , other . operations , atol = atol )

0<CODESPLIT>https://github.com/developmentseed/landsat-util/blob/92dc81771ddaa64a8a9124a89a6516b52485374b/landsat/landsat.py#L276-L432<CODESPLIT>args_options<CODESPLIT>Main function - launches the program .<CODESPLIT>def args_options ( ) : parser = argparse . ArgumentParser ( prog = 'landsat' , formatter_class = argparse . RawDescriptionHelpFormatter , description = textwrap . dedent ( DESCRIPTION ) ) subparsers = parser . add_subparsers ( help = 'Landsat Utility' , dest = 'subs' ) parser . add_argument ( '--version' , action = 'version' , version = '%(prog)s version ' + __version__ ) # Search Logic parser_search = subparsers . add_parser ( 'search' , help = 'Search Landsat metadata' ) # Global search options parser_search . add_argument ( '-l' , '--limit' , default = 10 , type = int , help = 'Search return results limit\n' 'default is 10' ) parser_search . add_argument ( '-s' , '--start' , help = 'Start Date - Most formats are accepted ' 'e.g. Jun 12 2014 OR 06/12/2014' ) parser_search . add_argument ( '-e' , '--end' , help = 'End Date - Most formats are accepted ' 'e.g. Jun 12 2014 OR 06/12/2014' ) parser_search . add_argument ( '--latest' , default = - 1 , type = int , help = 'returns the N latest images within the last 365 days' ) parser_search . add_argument ( '-c' , '--cloud' , type = float , default = 100.0 , help = 'Maximum cloud percentage ' 'default is 100 perct' ) parser_search . add_argument ( '-p' , '--pathrow' , help = 'Paths and Rows in order separated by comma. Use quotes ("001").' 'Example: path,row,path,row 001,001,190,204' ) parser_search . add_argument ( '--lat' , type = float , help = 'The latitude' ) parser_search . add_argument ( '--lon' , type = float , help = 'The longitude' ) parser_search . add_argument ( '--address' , type = str , help = 'The address' ) parser_search . add_argument ( '--json' , action = 'store_true' , help = 'Returns a bare JSON response' ) parser_search . add_argument ( '--geojson' , action = 'store_true' , help = 'Returns a geojson response' ) parser_download = subparsers . add_parser ( 'download' , help = 'Download images from Google Storage' ) parser_download . add_argument ( 'scenes' , metavar = 'sceneID' , nargs = "+" , help = "Provide Full sceneID, e.g. LC81660392014196LGN00" ) parser_download . add_argument ( '-b' , '--bands' , help = 'If you specify bands, landsat-util will try to download ' 'the band from S3. If the band does not exist, an error is returned' , default = None ) parser_download . add_argument ( '-d' , '--dest' , help = 'Destination path' ) parser_download . add_argument ( '-p' , '--process' , help = 'Process the image after download' , action = 'store_true' ) parser_download . add_argument ( '--pansharpen' , action = 'store_true' , help = 'Whether to also pansharpen the process ' 'image. Pansharpening requires larger memory' ) parser_download . add_argument ( '--ndvi' , action = 'store_true' , help = 'Whether to run the NDVI process. If used, bands parameter is disregarded' ) parser_download . add_argument ( '--ndvigrey' , action = 'store_true' , help = 'Create an NDVI map in grayscale (grey)' ) parser_download . add_argument ( '--clip' , help = 'Clip the image with the bounding box provided. Values must be in ' + 'WGS84 datum, and with longitude and latitude units of decimal degrees ' + 'separated by comma.' + 'Example: --clip=-346.06658935546875,49.93531194616915,-345.4595947265625,' + '50.2682767372753' ) parser_download . add_argument ( '-u' , '--upload' , action = 'store_true' , help = 'Upload to S3 after the image processing completed' ) parser_download . add_argument ( '--username' , help = 'USGS Eros account Username (only works if the account has' + ' special inventory access). Username and password as a fallback if the image' + 'is not found on AWS S3 or Google Storage' ) parser_download . add_argument ( '--password' , help = 'USGS Eros username, used as a fallback' ) parser_download . add_argument ( '--key' , help = 'Amazon S3 Access Key (You can also be set AWS_ACCESS_KEY_ID as ' 'Environment Variables)' ) parser_download . add_argument ( '--secret' , help = 'Amazon S3 Secret Key (You can also be set AWS_SECRET_ACCESS_KEY ' 'as Environment Variables)' ) parser_download . add_argument ( '--bucket' , help = 'Bucket name (required if uploading to s3)' ) parser_download . add_argument ( '--region' , help = 'URL to S3 region e.g. s3-us-west-2.amazonaws.com' ) parser_download . add_argument ( '--force-unzip' , help = 'Force unzip tar file' , action = 'store_true' ) parser_process = subparsers . add_parser ( 'process' , help = 'Process Landsat imagery' ) parser_process . add_argument ( 'path' , help = 'Path to the compressed image file' ) parser_process . add_argument ( '--pansharpen' , action = 'store_true' , help = 'Whether to also pansharpen the process ' 'image. Pansharpening requires larger memory' ) parser_process . add_argument ( '--ndvi' , action = 'store_true' , help = 'Create an NDVI map in color.' ) parser_process . add_argument ( '--ndvigrey' , action = 'store_true' , help = 'Create an NDVI map in grayscale (grey)' ) parser_process . add_argument ( '--clip' , help = 'Clip the image with the bounding box provided. Values must be in ' + 'WGS84 datum, and with longitude and latitude units of decimal degrees ' + 'separated by comma.' + 'Example: --clip=-346.06658935546875,49.93531194616915,-345.4595947265625,' + '50.2682767372753' ) parser_process . add_argument ( '-b' , '--bands' , help = 'specify band combinations. Default is 432' 'Example: --bands 321' , default = '432' ) parser_process . add_argument ( '-v' , '--verbose' , action = 'store_true' , help = 'Turn on verbosity' ) parser_process . add_argument ( '-u' , '--upload' , action = 'store_true' , help = 'Upload to S3 after the image processing completed' ) parser_process . add_argument ( '--key' , help = 'Amazon S3 Access Key (You can also be set AWS_ACCESS_KEY_ID as ' 'Environment Variables)' ) parser_process . add_argument ( '--secret' , help = 'Amazon S3 Secret Key (You can also be set AWS_SECRET_ACCESS_KEY ' 'as Environment Variables)' ) parser_process . add_argument ( '--bucket' , help = 'Bucket name (required if uploading to s3)' ) parser_process . add_argument ( '--region' , help = 'URL to S3 region e.g. s3-us-west-2.amazonaws.com' ) parser_process . add_argument ( '--force-unzip' , help = 'Force unzip tar file' , action = 'store_true' ) return parser

1<CODESPLIT>https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/hardware.py#L404-L408<CODESPLIT>Detector.get_pmt<CODESPLIT>Return PMT with DOM ID and DAQ channel ID<CODESPLIT>def get_pmt ( self , dom_id , channel_id ) : du , floor , _ = self . doms [ dom_id ] pmt = self . pmts [ self . _pmt_index_by_omkey [ ( du , floor , channel_id ) ] ] return pmt

1<CODESPLIT>https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/plist_plugins/dtfabric_plugin.py#L82-L99<CODESPLIT>DtFabricBasePlistPlugin._GetDataTypeMap<CODESPLIT>Retrieves a data type map defined by the definition file .<CODESPLIT>def _GetDataTypeMap ( self , name ) : data_type_map = self . _data_type_maps . get ( name , None ) if not data_type_map : data_type_map = self . _fabric . CreateDataTypeMap ( name ) self . _data_type_maps [ name ] = data_type_map return data_type_map

1<CODESPLIT>https://github.com/lpantano/seqcluster/blob/774e23add8cd4fdc83d626cea3bd1f458e7d060d/seqcluster/libs/simulator.py#L70-L85<CODESPLIT>_write_reads<CODESPLIT>Write fasta file ma file and real position<CODESPLIT>def _write_reads ( reads , prefix ) : out_ma = prefix + ".ma" out_fasta = prefix + ".fasta" out_real = prefix + ".txt" with open ( out_ma , 'w' ) as ma_handle : print ( "id\tseq\tsample" , file = ma_handle , end = "" ) with open ( out_fasta , 'w' ) as fa_handle : with open ( out_real , 'w' ) as read_handle : for idx , r in enumerate ( reads ) : info = r . split ( "_" ) print ( "seq_%s\t%s\t%s" % ( idx , reads [ r ] [ 0 ] , reads [ r ] [ 1 ] ) , file = ma_handle , end = "" ) print ( ">seq_%s\n%s" % ( idx , reads [ r ] [ 0 ] ) , file = fa_handle , end = "" ) print ( "%s\t%s\t%s\t%s\t%s\t%s\t%s" % ( idx , r , reads [ r ] [ 0 ] , reads [ r ] [ 1 ] , info [ 1 ] , info [ 2 ] , info [ 3 ] ) , file = read_handle , end = "" )

1<CODESPLIT>https://github.com/JdeRobot/base/blob/303b18992785b2fe802212f2d758a60873007f1f/src/drivers/MAVLinkServer/MAVProxy/pymavlink/tools/mavplayback.py#L150-L155<CODESPLIT>App.slew<CODESPLIT>move to a given position in the file<CODESPLIT>def slew ( self , value ) : if float ( value ) != self . filepos : pos = float ( value ) * self . filesize self . mlog . f . seek ( int ( pos ) ) self . find_message ( )

0<CODESPLIT>https://github.com/PyCQA/pydocstyle/blob/2549847f9efad225789f931e83dfe782418ca13e/src/pydocstyle/checker.py#L322-L336<CODESPLIT>ConventionChecker.check_backslashes<CODESPLIT>r D302 : Use u for docstrings with Unicode .<CODESPLIT>def check_backslashes ( self , definition , docstring ) : # Just check that docstring is raw, check_triple_double_quotes # ensures the correct quotes. if docstring and '\\' in docstring and not docstring . startswith ( ( 'r' , 'ur' ) ) : return violations . D301 ( )

1<CODESPLIT>https://github.com/genialis/resolwe/blob/f7bb54932c81ec0cfc5b5e80d238fceaeaa48d86/resolwe/elastic/builder.py#L126-L129<CODESPLIT>ElasticSignal.disconnect<CODESPLIT>Disconnect all connected signal types from this receiver .<CODESPLIT>def disconnect ( self ) : for signal , kwargs in self . connections : signal . disconnect ( self , * * kwargs )

1<CODESPLIT>https://github.com/klen/graphite-beacon/blob/c1f071e9f557693bc90f6acbc314994985dc3b77/graphite_beacon/units.py#L148-L155<CODESPLIT>TimeUnit.convert<CODESPLIT>Convert a value from one time unit to another .<CODESPLIT>def convert ( cls , value , from_unit , to_unit ) : value_ms = value * cls . UNITS_IN_MILLISECONDS [ from_unit ] return value_ms / cls . UNITS_IN_MILLISECONDS [ to_unit ]

0<CODESPLIT>https://github.com/jaraco/path.py/blob/bbe7d99e7a64a004f866ace9ec12bd9b296908f5/path/__init__.py#L350-L360<CODESPLIT>Path.splitpath<CODESPLIT>p . splitdrive () - > Return ( p . drive <the rest of p > ) .<CODESPLIT>def splitpath ( self ) : parent , child = self . module . split ( self ) return self . _next_class ( parent ) , child

1<CODESPLIT>https://github.com/neo4j/neo4j-python-driver/blob/0c641e826765e86ff5454dae57c99521db8ca45c/neo4j/types/temporal.py#L123-L151<CODESPLIT>dehydrate_datetime<CODESPLIT>Dehydrator for datetime values .<CODESPLIT>def dehydrate_datetime ( value ) : def seconds_and_nanoseconds ( dt ) : if isinstance ( dt , datetime ) : dt = DateTime . from_native ( dt ) zone_epoch = DateTime ( 1970 , 1 , 1 , tzinfo = dt . tzinfo ) t = dt . to_clock_time ( ) - zone_epoch . to_clock_time ( ) return t . seconds , t . nanoseconds tz = value . tzinfo if tz is None : # without time zone value = utc . localize ( value ) seconds , nanoseconds = seconds_and_nanoseconds ( value ) return Structure ( b"d" , seconds , nanoseconds ) elif hasattr ( tz , "zone" ) and tz . zone : # with named time zone seconds , nanoseconds = seconds_and_nanoseconds ( value ) return Structure ( b"f" , seconds , nanoseconds , tz . zone ) else : # with time offset seconds , nanoseconds = seconds_and_nanoseconds ( value ) return Structure ( b"F" , seconds , nanoseconds , tz . utcoffset ( value ) . seconds )

0<CODESPLIT>https://github.com/numenta/htmresearch/blob/70c096b09a577ea0432c3f3bfff4442d4871b7aa/htmresearch/algorithms/faulty_spatial_pooler.py#L117-L178<CODESPLIT>normalizeSequence<CODESPLIT>This is the primary public method of the SpatialPooler class . This function takes a input vector and outputs the indices of the active columns . If learn is set to True this method also updates the permanences of the columns .<CODESPLIT>def normalizeSequence ( sequence ) : seq = np . array ( sequence ) . astype ( 'float64' ) meanSeq = np . mean ( seq ) stdSeq = np . std ( seq ) seq = ( seq - np . mean ( seq ) ) / np . std ( seq ) sequence = seq . tolist ( ) return sequence , meanSeq , stdSeq

1<CODESPLIT>https://github.com/nmdp-bioinformatics/SeqAnn/blob/5ce91559b0a4fbe4fb7758e034eb258202632463/seqann/feature_client/apis/features_api.py#L607-L634<CODESPLIT>FeaturesApi.list_features_1<CODESPLIT>List the enumerated sequence features matching a term and rank at a locus<CODESPLIT>def list_features_1 ( self , locus , term , rank , * * kwargs ) : kwargs [ '_return_http_data_only' ] = True if kwargs . get ( 'callback' ) : return self . list_features_1_with_http_info ( locus , term , rank , * * kwargs ) else : ( data ) = self . list_features_1_with_http_info ( locus , term , rank , * * kwargs ) return data

1<CODESPLIT>https://github.com/brocade/pynos/blob/bd8a34e98f322de3fc06750827d8bbc3a0c00380/pynos/versions/ver_7/ver_7_1_0/yang/brocade_vswitch.py#L632-L643<CODESPLIT>brocade_vswitch.get_vnetwork_vswitches_input_name<CODESPLIT>Auto Generated Code<CODESPLIT>def get_vnetwork_vswitches_input_name ( self , * * kwargs ) : config = ET . Element ( "config" ) get_vnetwork_vswitches = ET . Element ( "get_vnetwork_vswitches" ) config = get_vnetwork_vswitches input = ET . SubElement ( get_vnetwork_vswitches , "input" ) name = ET . SubElement ( input , "name" ) name . text = kwargs . pop ( 'name' ) callback = kwargs . pop ( 'callback' , self . _callback ) return callback ( config )

0<CODESPLIT>https://github.com/btr1975/ipaddresstools/blob/759e91b3fdf1e8a8eea55337b09f53a94ad2016e/ipaddresstools/ipaddresstools.py#L536-L562<CODESPLIT>all_ip_address_in_subnet<CODESPLIT>Function to return every subnet a ip can belong to with a shorter prefix Args : ip_net : Unicast or Multicast IP address or subnet in the following format 192 . 168 . 1 . 1 239 . 1 . 1 . 1 cidr : CIDR value of 0 to 32 include_default : If you want the list to inlclude the default route set to True<CODESPLIT>def all_ip_address_in_subnet ( ip_net , cidr ) : ip_address_list = list ( ) if not ip_mask ( '{ip_net}/{cidr}' . format ( ip_net = ip_net , cidr = cidr ) , return_tuple = False ) : LOGGER . critical ( '{network} is not a valid IPv4 network' . format ( network = '{ip_net}/{cidr}' . format ( ip_net = ip_net , cidr = cidr ) ) ) raise ValueError ( '{network} is not a valid IPv4 network' . format ( network = '{ip_net}/{cidr}' . format ( ip_net = ip_net , cidr = cidr ) ) ) else : ip_net = whole_subnet_maker ( ip_net , cidr ) net = __ipaddress . ip_network ( '{ip_net}/{cidr}' . format ( ip_net = ip_net , cidr = cidr ) ) for single_ip in net : ip_address_list . append ( str ( single_ip ) ) return ip_address_list

0<CODESPLIT>https://github.com/lk-geimfari/mimesis/blob/4b16ee7a8dba6281a904654a88dbb4b052869fc5/mimesis/providers/business.py#L104-L117<CODESPLIT>Business.price<CODESPLIT>Generate random price in BTC .<CODESPLIT>def price ( self , minimum : float = 10.00 , maximum : float = 1000.00 ) -> str : price = self . random . uniform ( minimum , maximum , precision = 2 ) return '{0} {1}' . format ( price , self . currency_symbol ( ) )

0<CODESPLIT>https://github.com/iblancasa/GitHubCity/blob/c5299c6859dbefbd869e2ac6ff2faff2a39cf32f/src/githubcity/ghuser.py#L262-L274<CODESPLIT>GitHubUser.__getOrganizations<CODESPLIT>Get data of the GitHub user .<CODESPLIT>def __getOrganizations ( self , web ) : orgsElements = web . find_all ( "a" , { "class" : "avatar-group-item" } ) self . organizations = len ( orgsElements )

1<CODESPLIT>https://github.com/limpyd/redis-limpyd/blob/3c745dde1390a0bd09690b77a089dcc08c6c7e43/limpyd/indexes.py#L1334-L1372<CODESPLIT>NumberRangeIndex.get_boundaries<CODESPLIT>Compute the boundaries to pass to the sorted - set command depending of the filter type<CODESPLIT>def get_boundaries ( self , filter_type , value ) : assert filter_type in self . handled_suffixes start = '-inf' end = '+inf' exclude = None if filter_type in ( None , 'eq' ) : # only one score start = end = value elif filter_type == 'gt' : start = '(%s' % value elif filter_type == 'gte' : start = value elif filter_type == 'lt' : end = '(%s' % value elif filter_type == 'lte' : end = value return start , end , exclude

0<CODESPLIT>https://github.com/prompt-toolkit/ptpython/blob/b1bba26a491324cd65e0ef46c7b818c4b88fd993/ptpython/style.py#L15-L21<CODESPLIT>initialize_extensions<CODESPLIT>Return a mapping from style names to their classes .<CODESPLIT>def initialize_extensions ( shell , extensions ) : try : iter ( extensions ) except TypeError : pass # no extensions found else : for ext in extensions : try : shell . extension_manager . load_extension ( ext ) except : ipy_utils . warn . warn ( "Error in loading extension: %s" % ext + "\nCheck your config files in %s" % ipy_utils . path . get_ipython_dir ( ) ) shell . showtraceback ( )

0<CODESPLIT>https://github.com/jay-johnson/network-pipeline/blob/4e53ae13fe12085e0cf2e5e1aff947368f4f1ffa/network_pipeline/record_packets_to_csv.py#L318-L353<CODESPLIT>RecordPacketsToCSV.process_udp_frame<CODESPLIT>process_dns_frame<CODESPLIT>def process_udp_frame ( self , id = None , msg = None ) : # normalize into a dataframe df = json_normalize ( msg ) # convert to a flattened dictionary dt = json . loads ( df . to_json ( ) ) flat_msg = { } for k in dt : new_key = "udp_{}" . format ( k ) flat_msg [ new_key ] = dt [ k ] [ "0" ] if new_key not in self . udp_keys : self . udp_keys [ new_key ] = k # end of capturing all unique keys dt [ "udp_id" ] = id self . all_udp . append ( dt ) log . debug ( "UDP data updated:" ) log . debug ( self . udp_keys ) log . debug ( self . all_udp ) log . debug ( "" ) return flat_msg

0<CODESPLIT>https://github.com/sci-bots/svg-model/blob/2d119650f995e62b29ce0b3151a23f3b957cb072/svg_model/__init__.py#L367-L405<CODESPLIT>remove_layer<CODESPLIT>Return offset and scale factor to scale x y columns of : data : df_points to fill : data : bounding_box while maintaining aspect ratio .<CODESPLIT>def remove_layer ( svg_source , layer_name ) : # Parse input file. xml_root = lxml . etree . parse ( svg_source ) svg_root = xml_root . xpath ( '/svg:svg' , namespaces = INKSCAPE_NSMAP ) [ 0 ] if isinstance ( layer_name , str ) : layer_name = [ layer_name ] for layer_name_i in layer_name : # Remove existing layer from source, in-memory XML (source file remains # unmodified). layer_xpath = '//svg:g[@inkscape:label="%s"]' % layer_name_i layer_groups = svg_root . xpath ( layer_xpath , namespaces = INKSCAPE_NSMAP ) if layer_groups : for g in layer_groups : g . getparent ( ) . remove ( g ) # Write result to `StringIO`. output = StringIO . StringIO ( ) xml_root . write ( output ) output . seek ( 0 ) return output

1<CODESPLIT>https://github.com/cloudera/impyla/blob/547fa2ba3b6151e2a98b3544301471a643212dc3/impala/_thrift_gen/hive_metastore/ThriftHiveMetastore.py#L4838-L4845<CODESPLIT>Client.drop_function<CODESPLIT>Parameters : - dbName - funcName<CODESPLIT>def drop_function ( self , dbName , funcName ) : self . send_drop_function ( dbName , funcName ) self . recv_drop_function ( )

0<CODESPLIT>https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/tools/caffe_converter/compare_layers.py#L149-L173<CODESPLIT>convert_and_compare_caffe_to_mxnet<CODESPLIT>Implementation of Breadth - first search ( BFS ) on caffe network DAG : param root_node : root node of caffe network DAG : param process_node : function to run on each node<CODESPLIT>def convert_and_compare_caffe_to_mxnet ( image_url , gpu , caffe_prototxt_path , caffe_model_path , caffe_mean , mean_diff_allowed , max_diff_allowed ) : import caffe from caffe_proto_utils import read_network_dag , process_network_proto , read_caffe_mean from convert_model import convert_model if isinstance ( caffe_mean , str ) : caffe_mean = read_caffe_mean ( caffe_mean ) elif caffe_mean is None : pass elif len ( caffe_mean ) == 3 : # swap channels from Caffe BGR to RGB caffe_mean = caffe_mean [ : : - 1 ] # get caffe root location, this is needed to run the upgrade network utility, so we only need # to support parsing of latest caffe caffe_root = os . path . dirname ( os . path . dirname ( caffe . __path__ [ 0 ] ) ) caffe_prototxt_path = process_network_proto ( caffe_root , caffe_prototxt_path ) _ , layer_name_to_record , top_to_layers = read_network_dag ( caffe_prototxt_path ) caffe . set_mode_cpu ( ) caffe_net = caffe . Net ( caffe_prototxt_path , caffe_model_path , caffe . TEST ) image_dims = tuple ( caffe_net . blobs [ 'data' ] . shape ) [ 2 : 4 ] logging . info ( 'getting image %s' , image_url ) img_rgb = read_image ( image_url , image_dims , caffe_mean ) img_bgr = img_rgb [ : , : : - 1 , : , : ] caffe_net . blobs [ 'data' ] . reshape ( * img_bgr . shape ) caffe_net . blobs [ 'data' ] . data [ ... ] = img_bgr _ = caffe_net . forward ( ) # read sym and add all outputs sym , arg_params , aux_params , _ = convert_model ( caffe_prototxt_path , caffe_model_path ) sym = sym . get_internals ( ) # now mxnet if gpu < 0 : ctx = mx . cpu ( 0 ) else : ctx = mx . gpu ( gpu ) arg_params , aux_params = _ch_dev ( arg_params , aux_params , ctx ) arg_params [ "data" ] = mx . nd . array ( img_rgb , ctx ) arg_params [ "prob_label" ] = mx . nd . empty ( ( 1 , ) , ctx ) exe = sym . bind ( ctx , arg_params , args_grad = None , grad_req = "null" , aux_states = aux_params ) exe . forward ( is_train = False ) compare_layers_from_nets ( caffe_net , arg_params , aux_params , exe , layer_name_to_record , top_to_layers , mean_diff_allowed , max_diff_allowed ) return

1<CODESPLIT>https://github.com/ramses-tech/nefertari/blob/c7caffe11576c11aa111adbdbadeff70ce66b1dd/nefertari/wrappers.py#L300-L319<CODESPLIT>add_object_url._set_object_self<CODESPLIT>Add _self key value to : obj : dict .<CODESPLIT>def _set_object_self ( self , obj ) : from nefertari . elasticsearch import ES location = self . request . path_url route_kwargs = { } """ Check for parents """ if self . request . matchdict : route_kwargs . update ( self . request . matchdict ) try : type_ , obj_pk = obj [ '_type' ] , obj [ '_pk' ] except KeyError : return resource = ( self . model_collections . get ( type_ ) or self . model_collections . get ( ES . src2type ( type_ ) ) ) if resource is not None : route_kwargs . update ( { resource . id_name : obj_pk } ) location = self . request . route_url ( resource . uid , * * route_kwargs ) obj . setdefault ( '_self' , location )

0<CODESPLIT>https://github.com/grundic/yagocd/blob/4c75336ae6f107c8723d37b15e52169151822127/yagocd/resources/artifact.py#L515-L526<CODESPLIT>BaseNode.get_descendants<CODESPLIT>Artifact tree generator - analogue of os . walk .<CODESPLIT>def get_descendants ( self , transitive = False ) : result = self . _descendants if transitive : return YagocdUtil . graph_depth_walk ( result , lambda v : v . descendants ) return result

0<CODESPLIT>https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/build/lib/pylabcontrol/gui/windows_and_widgets/main_window.py#L1049-L1079<CODESPLIT>MainWindow.fill_treewidget<CODESPLIT>updates the script based on the information provided in item<CODESPLIT>def fill_treewidget ( self , tree , parameters ) : tree . clear ( ) assert isinstance ( parameters , ( dict , Parameter ) ) for key , value in parameters . items ( ) : if isinstance ( value , Parameter ) : B26QTreeItem ( tree , key , value , parameters . valid_values [ key ] , parameters . info [ key ] ) else : B26QTreeItem ( tree , key , value , type ( value ) , '' )

0<CODESPLIT>https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/logging_/objects.py#L284-L294<CODESPLIT>LogEntryForm.set_agent<CODESPLIT>Gets the metadata for the agent .<CODESPLIT>def set_agent ( self , agent_id ) : # Implemented from template for osid.resource.ResourceForm.set_avatar_template if self . get_agent_metadata ( ) . is_read_only ( ) : raise errors . NoAccess ( ) if not self . _is_valid_id ( agent_id ) : raise errors . InvalidArgument ( ) self . _my_map [ 'agentId' ] = str ( agent_id )

0<CODESPLIT>https://github.com/Esri/ArcREST/blob/ab240fde2b0200f61d4a5f6df033516e53f2f416/src/arcrest/common/geometry.py#L493-L513<CODESPLIT>Polygon.__geomToPointList<CODESPLIT>returns the object as a python dictionary<CODESPLIT>def __geomToPointList ( self , geom ) : sr = geom . spatialReference wkid = None wkt = None if sr is None : if self . _wkid is None and self . _wkt is not None : wkt = self . _wkt else : wkid = self . _wkid else : wkid = sr . factoryCode g = json . loads ( geom . JSON ) top = [ ] for gring in g [ 'rings' ] : ring = [ ] for g in gring : ring . append ( Point ( coord = g , wkid = wkid , wkt = wkt , z = None , m = None ) ) top . append ( ring ) return top

0<CODESPLIT>https://github.com/occrp-attic/exactitude/blob/9fe13aa70f1aac644dbc999e0b21683db507f02d/exactitude/name.py#L8-L12<CODESPLIT>CountryType.clean_text<CODESPLIT>Basic clean - up .<CODESPLIT>def clean_text ( self , country , guess = False , * * kwargs ) : code = country . lower ( ) . strip ( ) if code in self . names : return code country = countrynames . to_code ( country , fuzzy = guess ) if country is not None : return country . lower ( )

0<CODESPLIT>https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/symmetry/bandstructure.py#L208-L236<CODESPLIT>WorkResults.from_node<CODESPLIT>Returns : the kpoints along the paths in cartesian coordinates together with the labels for symmetry points - Wei<CODESPLIT>def from_node ( cls , work ) : new = super ( ) . from_node ( work ) # Will put all files found in outdir in GridFs # Warning: assuming binary files. d = { os . path . basename ( f ) : f for f in work . outdir . list_filepaths ( ) } new . register_gridfs_files ( * * d ) return new

1<CODESPLIT>https://github.com/ynop/audiomate/blob/61727920b23a708293c3d526fa3000d4de9c6c21/audiomate/processing/base.py#L358-L384<CODESPLIT>Processor._process_corpus<CODESPLIT>Utility function for processing a corpus with a separate processing function .<CODESPLIT>def _process_corpus ( self , corpus , output_path , processing_func , frame_size = 400 , hop_size = 160 , sr = None ) : feat_container = containers . FeatureContainer ( output_path ) feat_container . open ( ) sampling_rate = - 1 for utterance in corpus . utterances . values ( ) : utt_sampling_rate = utterance . sampling_rate if sr is None : if sampling_rate > 0 and sampling_rate != utt_sampling_rate : raise ValueError ( 'File {} has a different sampling-rate than the previous ones!' . format ( utterance . track . idx ) ) sampling_rate = utt_sampling_rate processing_func ( utterance , feat_container , frame_size , hop_size , sr , corpus ) tf_frame_size , tf_hop_size = self . frame_transform ( frame_size , hop_size ) feat_container . frame_size = tf_frame_size feat_container . hop_size = tf_hop_size feat_container . sampling_rate = sr or sampling_rate feat_container . close ( ) return feat_container

0<CODESPLIT>https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/storage/google/cloud/storage/bucket.py#L943-L977<CODESPLIT>Bucket.delete_blob<CODESPLIT>Deletes a list of blobs from the current bucket .<CODESPLIT>def delete_blob ( self , blob_name , client = None , generation = None ) : client = self . _require_client ( client ) blob = Blob ( blob_name , bucket = self , generation = generation ) # We intentionally pass `_target_object=None` since a DELETE # request has no response value (whether in a standard request or # in a batch request). client . _connection . api_request ( method = "DELETE" , path = blob . path , query_params = blob . _query_params , _target_object = None , )

0<CODESPLIT>https://github.com/BernardFW/bernard/blob/9c55703e5ffe5717c9fa39793df59dbfa5b4c5ab/src/bernard/utils.py#L150-L162<CODESPLIT>make_ro<CODESPLIT>Copy a RO object into a RW structure made with standard Python classes .<CODESPLIT>def make_ro ( obj : Any , forgive_type = False ) : if isinstance ( obj , ( str , bytes , int , float , bool , RoDict , RoList ) ) or obj is None : return obj elif isinstance ( obj , Mapping ) : return RoDict ( obj , forgive_type ) elif isinstance ( obj , Sequence ) : return RoList ( obj , forgive_type ) elif forgive_type : return obj else : raise ValueError ( 'Trying to make read-only an object of type "{}"' . format ( obj . __class__ . __name__ ) )

1<CODESPLIT>https://github.com/tumblr/pytumblr/blob/4a5cd7c4b8ae78d12811d9fd52620afa1692a415/pytumblr/helpers.py#L3-L29<CODESPLIT>validate_params<CODESPLIT>Helps us validate the parameters for the request<CODESPLIT>def validate_params ( valid_options , params ) : #crazy little if statement hanging by himself :( if not params : return #We only allow one version of the data parameter to be passed data_filter = [ 'data' , 'source' , 'external_url' , 'embed' ] multiple_data = [ key for key in params . keys ( ) if key in data_filter ] if len ( multiple_data ) > 1 : raise Exception ( "You can't mix and match data parameters" ) #No bad fields which are not in valid options can pass disallowed_fields = [ key for key in params . keys ( ) if key not in valid_options ] if disallowed_fields : field_strings = "," . join ( disallowed_fields ) raise Exception ( "{} are not allowed fields" . format ( field_strings ) )

1<CODESPLIT>https://github.com/django-parler/django-parler/blob/11ae4af5e8faddb74c69c848870122df4006a54e/parler/cache.py#L112-L144<CODESPLIT>_get_cached_values<CODESPLIT>Fetch an cached field .<CODESPLIT>def _get_cached_values ( instance , translated_model , language_code , use_fallback = False ) : if not appsettings . PARLER_ENABLE_CACHING or not instance . pk or instance . _state . adding : return None key = get_translation_cache_key ( translated_model , instance . pk , language_code ) values = cache . get ( key ) if not values : return None # Check for a stored fallback marker if values . get ( '__FALLBACK__' , False ) : # Internal trick, already set the fallback marker, so no query will be performed. instance . _translations_cache [ translated_model ] [ language_code ] = MISSING # Allow to return the fallback language instead. if use_fallback : lang_dict = get_language_settings ( language_code ) # iterate over list of fallback languages, which should be already # in proper order for fallback_lang in lang_dict [ 'fallbacks' ] : if fallback_lang != language_code : return _get_cached_values ( instance , translated_model , fallback_lang , use_fallback = False ) return None values [ 'master' ] = instance values [ 'language_code' ] = language_code return values

0<CODESPLIT>https://github.com/observermedia/django-wordpress-rest/blob/f0d96891d8ac5a69c8ba90e044876e756fad1bfe/wordpress/loading.py#L864-L879<CODESPLIT>WPAPILoader.process_new_post<CODESPLIT>Sync data for a many - to - many field related to a post using set differences .<CODESPLIT>def process_new_post ( self , bulk_mode , api_post , posts , author , post_categories , post_tags , post_media_attachments ) : post = Post ( site_id = self . site_id , wp_id = api_post [ "ID" ] , author = author , post_date = api_post [ "date" ] , modified = api_post [ "modified" ] , title = api_post [ "title" ] , url = api_post [ "URL" ] , short_url = api_post [ "short_URL" ] , content = api_post [ "content" ] , excerpt = api_post [ "excerpt" ] , slug = api_post [ "slug" ] , guid = api_post [ "guid" ] , status = api_post [ "status" ] , sticky = api_post [ "sticky" ] , password = api_post [ "password" ] , parent = api_post [ "parent" ] , post_type = api_post [ "type" ] , likes_enabled = api_post [ "likes_enabled" ] , sharing_enabled = api_post [ "sharing_enabled" ] , like_count = api_post [ "like_count" ] , global_ID = api_post [ "global_ID" ] , featured_image = api_post [ "featured_image" ] , format = api_post [ "format" ] , menu_order = api_post [ "menu_order" ] , metadata = api_post [ "metadata" ] , post_thumbnail = api_post [ "post_thumbnail" ] ) posts . append ( post ) # if we're not in bulk mode, go ahead and create the post in the db now # otherwise this happens after all API posts are processed if not bulk_mode : self . bulk_create_posts ( posts , post_categories , post_tags , post_media_attachments )

1<CODESPLIT>https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilecore/iotile/core/hw/reports/utc_assigner.py#L427-L452<CODESPLIT>UTCAssigner._fix_left<CODESPLIT>Fix a reading by looking for the nearest anchor point before it .<CODESPLIT>def _fix_left ( self , reading_id , last , start , found_id ) : accum_delta = 0 exact = True crossed_break = False if start == 0 : return None for curr in self . _anchor_points . islice ( None , start - 1 , reverse = True ) : if curr . uptime is None or last . uptime is None : exact = False elif curr . is_break or last . uptime < curr . uptime : exact = False crossed_break = True else : accum_delta += last . uptime - curr . uptime if curr . utc is not None : time_delta = datetime . timedelta ( seconds = accum_delta ) return UTCAssignment ( reading_id , curr . utc + time_delta , found_id , exact , crossed_break ) last = curr return None

0<CODESPLIT>https://github.com/rouk1/django-image-renderer/blob/6a4326b77709601e18ee04f5626cf475c5ea0bb5/renderer/models.py#L49-L64<CODESPLIT>photo_post_delete_handler<CODESPLIT>returns real rendition URL<CODESPLIT>def photo_post_delete_handler ( sender , * * kwargs ) : instance = kwargs . get ( 'instance' ) instance . master . delete ( save = False ) instance . delete_all_renditions ( )

1<CODESPLIT>https://github.com/artefactual-labs/agentarchives/blob/af19ade56a90c64069cf46b50972fe72b6f10a45/agentarchives/atom/client.py#L749-L757<CODESPLIT>AtomClient.delete_record<CODESPLIT>Delete a record with record_id .<CODESPLIT>def delete_record ( self , record_id ) : self . _delete ( urljoin ( self . base_url , "informationobjects/{}" . format ( record_id ) ) , expected_response = 204 , ) return { "status" : "Deleted" }

1<CODESPLIT>https://github.com/tornadoweb/tornado/blob/b8b481770bcdb333a69afde5cce7eaa449128326/tornado/web.py#L2092-L2113<CODESPLIT>Application.listen<CODESPLIT>Starts an HTTP server for this application on the given port .<CODESPLIT>def listen ( self , port : int , address : str = "" , * * kwargs : Any ) -> HTTPServer : server = HTTPServer ( self , * * kwargs ) server . listen ( port , address ) return server

0<CODESPLIT>https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/util.py#L124-L142<CODESPLIT>consumer<CODESPLIT>Method decorator caching a method s returned values .<CODESPLIT>def consumer ( function ) : @ wraps ( function ) def wrapper ( * args , * * kwargs ) : generator = function ( * args , * * kwargs ) next ( generator ) return generator return wrapper

0<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/azurearm_network.py#L1789-L1828<CODESPLIT>public_ip_address_delete<CODESPLIT>.. versionadded :: 2019 . 2 . 0<CODESPLIT>def public_ip_address_delete ( name , resource_group , * * kwargs ) : result = False netconn = __utils__ [ 'azurearm.get_client' ] ( 'network' , * * kwargs ) try : pub_ip = netconn . public_ip_addresses . delete ( public_ip_address_name = name , resource_group_name = resource_group ) pub_ip . wait ( ) result = True except CloudError as exc : __utils__ [ 'azurearm.log_cloud_error' ] ( 'network' , str ( exc ) , * * kwargs ) return result

1<CODESPLIT>https://github.com/openstack/networking-cisco/blob/aa58a30aec25b86f9aa5952b0863045975debfa9/networking_cisco/ml2_drivers/nexus/mech_cisco_nexus.py#L148-L203<CODESPLIT>CiscoNexusCfgMonitor.replay_config<CODESPLIT>Sends pending config data in OpenStack to Nexus .<CODESPLIT>def replay_config ( self , switch_ip ) : LOG . debug ( "Replaying config for switch ip %(switch_ip)s" , { 'switch_ip' : switch_ip } ) # Before replaying all config, initialize trunk interfaces # to none as required.  If this fails, the switch may not # be up all the way.  Quit and retry later. try : self . _initialize_trunk_interfaces_to_none ( switch_ip ) except Exception : return nve_bindings = nxos_db . get_nve_switch_bindings ( switch_ip ) # If configured to set global VXLAN values and # there exists VXLAN data base entries, then configure # the "interface nve" entry on the switch. if ( len ( nve_bindings ) > 0 and cfg . CONF . ml2_cisco . vxlan_global_config ) : LOG . debug ( "Nexus: Replay NVE Interface" ) loopback = self . _mdriver . get_nve_loopback ( switch_ip ) self . _driver . enable_vxlan_feature ( switch_ip , const . NVE_INT_NUM , loopback ) for x in nve_bindings : try : self . _driver . create_nve_member ( switch_ip , const . NVE_INT_NUM , x . vni , x . mcast_group ) except Exception as e : LOG . error ( "Failed to configure nve_member for " "switch %(switch_ip)s, vni %(vni)s" "Reason:%(reason)s " , { 'switch_ip' : switch_ip , 'vni' : x . vni , 'reason' : e } ) self . _mdriver . register_switch_as_inactive ( switch_ip , 'replay create_nve_member' ) return try : port_bindings = nxos_db . get_nexusport_switch_bindings ( switch_ip ) except excep . NexusPortBindingNotFound : LOG . warning ( "No port entries found for switch ip " "%(switch_ip)s during replay." , { 'switch_ip' : switch_ip } ) return try : self . _mdriver . configure_switch_entries ( switch_ip , port_bindings ) except Exception as e : LOG . error ( "Unexpected exception while replaying " "entries for switch %(switch_ip)s, Reason:%(reason)s " , { 'switch_ip' : switch_ip , 'reason' : e } ) self . _mdriver . register_switch_as_inactive ( switch_ip , 'replay switch_entries' )

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/states/boto_apigateway.py#L815-L844<CODESPLIT>_Swagger._validate_swagger_file<CODESPLIT>High level check / validation of the input swagger file based on https : // github . com / swagger - api / swagger - spec / blob / master / versions / 2 . 0 . md<CODESPLIT>def _validate_swagger_file ( self ) : # check for any invalid fields for Swagger Object V2 for field in self . _cfg : if ( field not in _Swagger . SWAGGER_OBJ_V2_FIELDS and not _Swagger . VENDOR_EXT_PATTERN . match ( field ) ) : raise ValueError ( 'Invalid Swagger Object Field: {0}' . format ( field ) ) # check for Required Swagger fields by Saltstack boto apigateway state for field in _Swagger . SWAGGER_OBJ_V2_FIELDS_REQUIRED : if field not in self . _cfg : raise ValueError ( 'Missing Swagger Object Field: {0}' . format ( field ) ) # check for Swagger Version self . _swagger_version = self . _cfg . get ( 'swagger' ) if self . _swagger_version not in _Swagger . SWAGGER_VERSIONS_SUPPORTED : raise ValueError ( 'Unsupported Swagger version: {0},' 'Supported versions are {1}' . format ( self . _swagger_version , _Swagger . SWAGGER_VERSIONS_SUPPORTED ) ) log . info ( type ( self . _models ) ) self . _validate_error_response_model ( self . paths , self . _models ( ) )

0<CODESPLIT>https://github.com/sibirrer/lenstronomy/blob/4edb100a4f3f4fdc4fac9b0032d2b0283d0aa1d6/lenstronomy/LensModel/multi_plane.py#L359-L372<CODESPLIT>MultiPlane._add_deflection<CODESPLIT>ray propagation with small angle approximation<CODESPLIT>def _add_deflection ( self , x , y , alpha_x , alpha_y , kwargs_lens , idex ) : theta_x , theta_y = self . _co_moving2angle ( x , y , idex ) alpha_x_red , alpha_y_red = self . _lens_model . alpha ( theta_x , theta_y , kwargs_lens , k = self . _sorted_redshift_index [ idex ] ) alpha_x_phys = self . _reduced2physical_deflection ( alpha_x_red , idex ) alpha_y_phys = self . _reduced2physical_deflection ( alpha_y_red , idex ) alpha_x_new = alpha_x - alpha_x_phys alpha_y_new = alpha_y - alpha_y_phys return alpha_x_new , alpha_y_new

0<CODESPLIT>https://github.com/polyaxon/rhea/blob/f47b59777cd996d834a0497a1ab442541aaa8a62/rhea/manager.py#L147-L187<CODESPLIT>Rhea.get_float<CODESPLIT>Get a the value corresponding to the key and converts it to bool / list ( str ) .<CODESPLIT>def get_float ( self , key , is_list = False , is_optional = False , is_secret = False , is_local = False , default = None , options = None ) : if is_list : return self . _get_typed_list_value ( key = key , target_type = float , type_convert = float , is_optional = is_optional , is_secret = is_secret , is_local = is_local , default = default , options = options ) return self . _get_typed_value ( key = key , target_type = float , type_convert = float , is_optional = is_optional , is_secret = is_secret , is_local = is_local , default = default , options = options )

0<CODESPLIT>https://github.com/brocade/pynos/blob/bd8a34e98f322de3fc06750827d8bbc3a0c00380/pynos/versions/ver_7/ver_7_1_0/yang/brocade_firmware.py#L986-L999<CODESPLIT>brocade_firmware.logical_chassis_fwdl_status_output_cluster_fwdl_entries_fwdl_entries_blade_name<CODESPLIT>Auto Generated Code<CODESPLIT>def logical_chassis_fwdl_status_output_cluster_fwdl_entries_fwdl_entries_blade_name ( self , * * kwargs ) : config = ET . Element ( "config" ) logical_chassis_fwdl_status = ET . Element ( "logical_chassis_fwdl_status" ) config = logical_chassis_fwdl_status output = ET . SubElement ( logical_chassis_fwdl_status , "output" ) cluster_fwdl_entries = ET . SubElement ( output , "cluster-fwdl-entries" ) fwdl_entries = ET . SubElement ( cluster_fwdl_entries , "fwdl-entries" ) blade_name = ET . SubElement ( fwdl_entries , "blade-name" ) blade_name . text = kwargs . pop ( 'blade_name' ) callback = kwargs . pop ( 'callback' , self . _callback ) return callback ( config )

1<CODESPLIT>https://github.com/mbedmicro/pyOCD/blob/41a174718a9739f3cbe785c2ba21cb7fd1310c6f/pyocd/flash/flash_builder.py#L225-L312<CODESPLIT>FlashBuilder._build_sectors_and_pages<CODESPLIT>! @brief Converts the list of flash operations to flash sectors and pages . @param self @param keep_unwritten If true unwritten pages in an erased sector and unwritten contents of a modified page will be read from the target and added to the data to be programmed .<CODESPLIT>def _build_sectors_and_pages ( self , keep_unwritten ) : assert len ( self . flash_operation_list ) > 0 self . program_byte_count = 0 flash_addr = self . flash_operation_list [ 0 ] . addr sector_info = self . flash . get_sector_info ( flash_addr ) if sector_info is None : raise FlashFailure ( "Attempt to program flash at invalid address 0x%08x" % flash_addr ) page_info = self . flash . get_page_info ( flash_addr ) if page_info is None : raise FlashFailure ( "Attempt to program flash at invalid address 0x%08x" % flash_addr ) current_sector = _FlashSector ( sector_info ) self . sector_list . append ( current_sector ) current_page = _FlashPage ( page_info ) current_sector . add_page ( current_page ) self . page_list . append ( current_page ) for flash_operation in self . flash_operation_list : pos = 0 while pos < len ( flash_operation . data ) : flash_addr = flash_operation . addr + pos # Check if operation is in a different sector. if flash_addr >= current_sector . addr + current_sector . size : sector_info = self . flash . get_sector_info ( flash_addr ) if sector_info is None : raise FlashFailure ( "Attempt to program flash at invalid address 0x%08x" % flash_addr ) current_sector = _FlashSector ( sector_info ) self . sector_list . append ( current_sector ) # Check if operation is in a different page if flash_addr >= current_page . addr + current_page . size : page_info = self . flash . get_page_info ( flash_addr ) if page_info is None : raise FlashFailure ( "Attempt to program flash at invalid address 0x%08x" % flash_addr ) current_page = _FlashPage ( page_info ) current_sector . add_page ( current_page ) self . page_list . append ( current_page ) # Fill the page gap if there is one page_data_end = current_page . addr + len ( current_page . data ) if flash_addr != page_data_end : old_data_len = flash_addr - page_data_end if keep_unwritten : self . _enable_read_access ( ) old_data = self . flash . target . read_memory_block8 ( page_data_end , old_data_len ) else : old_data = [ self . flash . region . erased_byte_value ] * old_data_len current_page . data . extend ( old_data ) self . program_byte_count += old_data_len # Copy data to page and increment pos space_left_in_page = page_info . size - len ( current_page . data ) space_left_in_data = len ( flash_operation . data ) - pos amount = min ( space_left_in_page , space_left_in_data ) current_page . data . extend ( flash_operation . data [ pos : pos + amount ] ) self . program_byte_count += amount #increment position pos += amount # Fill the page gap at the end if there is one if len ( current_page . data ) != current_page . size : page_data_end = current_page . addr + len ( current_page . data ) old_data_len = current_page . size - len ( current_page . data ) if keep_unwritten and self . flash . region . is_readable : self . _enable_read_access ( ) old_data = self . flash . target . read_memory_block8 ( page_data_end , old_data_len ) else : old_data = [ self . flash . region . erased_byte_value ] * old_data_len current_page . data . extend ( old_data ) self . program_byte_count += old_data_len # Go back through sectors and fill any missing pages with existing data. if keep_unwritten and self . flash . region . is_readable : self . _fill_unwritten_sector_pages ( )

1<CODESPLIT>https://github.com/striglia/pyramid_swagger/blob/1dbc0b4f23e2e5f4ed575c116f3f7d0e83e30d45/pyramid_swagger/ingest.py#L195-L244<CODESPLIT>create_bravado_core_config<CODESPLIT>Create a configuration dict for bravado_core based on pyramid_swagger settings .<CODESPLIT>def create_bravado_core_config ( settings ) : # Map pyramid_swagger config key -> bravado_core config key config_keys = { 'pyramid_swagger.enable_request_validation' : 'validate_requests' , 'pyramid_swagger.enable_response_validation' : 'validate_responses' , 'pyramid_swagger.enable_swagger_spec_validation' : 'validate_swagger_spec' , 'pyramid_swagger.use_models' : 'use_models' , 'pyramid_swagger.user_formats' : 'formats' , 'pyramid_swagger.include_missing_properties' : 'include_missing_properties' , } configs = { 'use_models' : False } bravado_core_configs_from_pyramid_swagger_configs = { bravado_core_key : settings [ pyramid_swagger_key ] for pyramid_swagger_key , bravado_core_key in iteritems ( config_keys ) if pyramid_swagger_key in settings } if bravado_core_configs_from_pyramid_swagger_configs : warnings . warn ( message = 'Configs {old_configs} are deprecated, please use {new_configs} instead.' . format ( old_configs = ', ' . join ( k for k , v in sorted ( iteritems ( config_keys ) ) ) , new_configs = ', ' . join ( '{}{}' . format ( BRAVADO_CORE_CONFIG_PREFIX , v ) for k , v in sorted ( iteritems ( config_keys ) ) ) , ) , category = DeprecationWarning , ) configs . update ( bravado_core_configs_from_pyramid_swagger_configs ) configs . update ( { key . replace ( BRAVADO_CORE_CONFIG_PREFIX , '' ) : value for key , value in iteritems ( settings ) if key . startswith ( BRAVADO_CORE_CONFIG_PREFIX ) } ) return configs

1<CODESPLIT>https://github.com/inveniosoftware/invenio-records-rest/blob/e7b63c5f72cef03d06d3f1b4c12c0d37e3a628b9/invenio_records_rest/loaders/marshmallow.py#L68-L78<CODESPLIT>MarshmallowErrors.get_body<CODESPLIT>Get the request body .<CODESPLIT>def get_body ( self , environ = None ) : body = dict ( status = self . code , message = self . get_description ( environ ) , ) if self . errors : body [ 'errors' ] = self . errors return json . dumps ( body )

0<CODESPLIT>https://github.com/hovren/crisp/blob/65cae19e7cfae5a397859096c9ef666e0f4e7f1b/crisp/tracking.py#L71-L110<CODESPLIT>track_points<CODESPLIT>Return optical flow magnitude for the given image sequence The flow magnitude is the mean value of the total ( sparse ) optical flow between two images . Crude outlier detection using the max_diff parameter is used . Parameters ---------------- image_sequence : sequence Sequence of image data ( ndarrays ) to calculate flow magnitude from max_diff : float Distance threshold for outlier rejection gftt_options : dict Keyword arguments to the OpenCV goodFeaturesToTrack function Returns ---------------- flow : ndarray The optical flow magnitude<CODESPLIT>def track_points ( img1 , img2 , initial_points = None , gftt_params = { } ) : params = GFTT_DEFAULTS if gftt_params : params . update ( gftt_params ) if initial_points is None : initial_points = cv2 . goodFeaturesToTrack ( img1 , params [ 'max_corners' ] , params [ 'quality_level' ] , params [ 'min_distance' ] ) [ _points , status , err ] = cv2 . calcOpticalFlowPyrLK ( img1 , img2 , initial_points , np . array ( [ ] ) ) # Filter out valid points only points = _points [ np . nonzero ( status ) ] initial_points = initial_points [ np . nonzero ( status ) ] return ( points , initial_points )

1<CODESPLIT>https://github.com/Capitains/MyCapytain/blob/b11bbf6b6ae141fc02be70471e3fbf6907be6593/MyCapytain/resources/texts/remote/cts.py#L134-L153<CODESPLIT>_SharedMethod.getPassagePlus<CODESPLIT>Retrieve a passage and informations around it and store it in the object<CODESPLIT>def getPassagePlus ( self , reference = None ) : if reference : urn = "{0}:{1}" . format ( self . urn , reference ) else : urn = str ( self . urn ) response = xmlparser ( self . retriever . getPassagePlus ( urn = urn ) ) passage = CtsPassage ( urn = urn , resource = response , retriever = self . retriever ) passage . _parse_request ( response . xpath ( "//ti:reply/ti:label" , namespaces = XPATH_NAMESPACES ) [ 0 ] ) self . citation = passage . citation return passage

1<CODESPLIT>https://github.com/LEMS/pylems/blob/4eeb719d2f23650fe16c38626663b69b5c83818b/lems/parser/LEMS.py#L1539-L1565<CODESPLIT>LEMSFileParser.parse_state_variable<CODESPLIT>Parses <StateVariable ><CODESPLIT>def parse_state_variable ( self , node ) : if 'name' in node . lattrib : name = node . lattrib [ 'name' ] else : self . raise_error ( '<StateVariable> must specify a name' ) if 'dimension' in node . lattrib : dimension = node . lattrib [ 'dimension' ] else : self . raise_error ( "State variable '{0}' must specify a dimension" , name ) if 'exposure' in node . lattrib : exposure = node . lattrib [ 'exposure' ] else : exposure = None self . current_regime . add_state_variable ( StateVariable ( name , dimension , exposure ) )

1<CODESPLIT>https://github.com/openspending/babbage/blob/9e03efe62e0be0cceabafd4de2a09cb8ec794b92/babbage/model/concept.py#L40-L47<CODESPLIT>Concept.match_ref<CODESPLIT>Check if the ref matches one the concept s aliases . If so mark the matched ref so that we use it as the column label .<CODESPLIT>def match_ref ( self , ref ) : if ref in self . refs : self . _matched_ref = ref return True return False

1<CODESPLIT>https://github.com/willkg/socorro-siggen/blob/db7e3233e665a458a961c48da22e93a69b1d08d6/siggen/utils.py#L242-L324<CODESPLIT>drop_prefix_and_return_type<CODESPLIT>Takes the function value from a frame and drops prefix and return type<CODESPLIT>def drop_prefix_and_return_type ( function ) : DELIMITERS = { '(' : ')' , '{' : '}' , '[' : ']' , '<' : '>' , '`' : "'" } OPEN = DELIMITERS . keys ( ) CLOSE = DELIMITERS . values ( ) # The list of tokens accumulated so far tokens = [ ] # Keeps track of open delimiters so we can match and close them levels = [ ] # The current token we're building current = [ ] for i , char in enumerate ( function ) : if char in OPEN : levels . append ( char ) current . append ( char ) elif char in CLOSE : if levels and DELIMITERS [ levels [ - 1 ] ] == char : levels . pop ( ) current . append ( char ) else : # This is an unmatched close. current . append ( char ) elif levels : current . append ( char ) elif char == ' ' : tokens . append ( '' . join ( current ) ) current = [ ] else : current . append ( char ) if current : tokens . append ( '' . join ( current ) ) while len ( tokens ) > 1 and tokens [ - 1 ] . startswith ( ( '(' , '[clone' ) ) : # It's possible for the function signature to have a space between # the function name and the parenthesized arguments or [clone ...] # thing. If that's the case, we join the last two tokens. We keep doing # that until the last token is nice. # # Example: # #     somefunc (int arg1, int arg2) #             ^ #     somefunc(int arg1, int arg2) [clone .cold.111] #                                 ^ #     somefunc(int arg1, int arg2) [clone .cold.111] [clone .cold.222] #                                 ^                 ^ tokens = tokens [ : - 2 ] + [ ' ' . join ( tokens [ - 2 : ] ) ] return tokens [ - 1 ]

0<CODESPLIT>https://github.com/mirukan/pydecensooru/blob/2a2bec93c40ed2d3e359ee203eceabf42ef1755d/pydecensooru/main.py#L35-L38<CODESPLIT>fill_missing_info<CODESPLIT>Decensor a post info dict from Danbooru API if needed .<CODESPLIT>def fill_missing_info ( info : dict , site_url : str = DEFAULT_SITE ) -> dict : try : md5 , ext = find_censored_md5ext ( info [ "id" ] ) except TypeError : # None returned by find_.. return info sample_ext = "jpg" if ext != "zip" else "webm" if info [ "id" ] > 2_800_000 : site_url = site_url . rstrip ( "/" ) file_url = f"{site_url}/data/{md5}.{ext}" sample_url = f"{site_url}/data/sample/sample-{md5}.{sample_ext}" else : server = "raikou2" if info [ "id" ] > 850_000 else "raikou1" url_base = f"https://{server}.donmai.us" file_url = f"{url_base}/{md5[:2]}/{md5[2:4]}/{md5}.{ext}" sample_url = ( f"{url_base}/sample/{md5[:2]}/{md5[2:4]}/" f"sample-{md5}.{sample_ext}" ) if info [ "image_width" ] < 850 : sample_url = file_url return { * * info , * * { "file_ext" : ext , "md5" : md5 , "file_url" : file_url , "large_file_url" : sample_url , "preview_file_url" : ( f"https://raikou4.donmai.us/preview/" f"{md5[:2]}/{md5[2:4]}/{md5}.jpg" ) , } }

0<CODESPLIT>https://github.com/brunobord/meuhdb/blob/2ef2ea0b1065768d88f52bacf1b94b3d3ce3d9eb/meuhdb/core.py#L263-L267<CODESPLIT>MeuhDb.filter<CODESPLIT>Search keys whose values match with the searched values<CODESPLIT>def filter ( self , * * kwargs ) : keys = self . filter_keys ( * * kwargs ) return self . keys_to_values ( keys )

0<CODESPLIT>https://github.com/CalebBell/fluids/blob/57f556752e039f1d3e5a822f408c184783db2828/fluids/geometry.py#L832-L874<CODESPLIT>V_vertical_conical_concave<CODESPLIT>r Calculates volume of a vertical tank with a concave ellipsoidal bottom according to [ 1 ] _ . No provision for the top of the tank is made here .<CODESPLIT>def V_vertical_conical_concave ( D , a , h ) : if h < abs ( a ) : Vf = pi * D ** 2 / 12. * ( 3 * h + a - ( a + h ) ** 3 / a ** 2 ) else : Vf = pi * D ** 2 / 12. * ( 3 * h + a ) return Vf

0<CODESPLIT>https://github.com/limpyd/redis-limpyd/blob/3c745dde1390a0bd09690b77a089dcc08c6c7e43/limpyd/contrib/collection.py#L667-L728<CODESPLIT>ExtendedCollectionManager.from_stored<CODESPLIT>Will call the collection and store the result in Redis and return a new collection based on this stored result . Note that only primary keys are stored ie calls to values / values_list are ignored when storing result . But choices about instances / values_list are transmited to the new collection . If no key is given a new one will be generated . The ttl is the time redis will keep the new key . By default its DEFAULT_STORE_TTL which is 60 secondes . You can pass None if you don t want expiration .<CODESPLIT>def from_stored ( self , key ) : # only one stored key allowed if self . stored_key : raise ValueError ( 'This collection is already based on a stored one' ) # prepare the collection self . stored_key = key self . intersect ( _StoredCollection ( self . cls . get_connection ( ) , key ) ) self . sort ( by = 'nosort' ) # keep stored order # count the number of results to manage empty result (to not behave like # expired key) self . _stored_len = self . cls . get_connection ( ) . llen ( key ) return self

0<CODESPLIT>https://github.com/DataBiosphere/toil/blob/a8252277ff814e7bee0971139c2344f88e44b644/src/toil/job.py#L672-L696<CODESPLIT>Job.checkJobGraphAcylic<CODESPLIT>A checkpoint job is a job that is restarted if either it fails or if any of \ its successors completely fails exhausting their retries .<CODESPLIT>def checkJobGraphAcylic ( self ) : #Get the root jobs roots = self . getRootJobs ( ) if len ( roots ) == 0 : raise JobGraphDeadlockException ( "Graph contains no root jobs due to cycles" ) #Get implied edges extraEdges = self . _getImpliedEdges ( roots ) #Check for directed cycles in the augmented graph visited = set ( ) for root in roots : root . _checkJobGraphAcylicDFS ( [ ] , visited , extraEdges )

1<CODESPLIT>https://github.com/WebarchivCZ/WA-KAT/blob/16d064a3a775dc1d2713debda7847ded52dd2a06/src/wa_kat/templates/static/js/Lib/site-packages/components/keyword_handler.py#L44-L83<CODESPLIT>KeywordListHandler._render<CODESPLIT>Render the HTML code for all the : attr : keywords stored in this class .<CODESPLIT>def _render ( self ) : # hide the list in case that there is no `keyword` to be displayed if self . keywords : self . whole_el . style . display = "block" self . all_lists_el . style . display = "block" else : self . whole_el . style . display = "none" if "<li>" not in self . all_lists_el . html : self . all_lists_el . style . display = "none" # construct the HTML code for each keyword html_lines = ( "<li class='kw_enum'>{0} {1}</li>\n" . format ( keyword , ( self . _remover % cnt ) ) for cnt , keyword in enumerate ( self . keywords ) ) # put the keywords into the HTML code of the page self . el . innerHTML = "<ol>\n%s\n</ol>\n" % "\n" . join ( html_lines ) # this function is used to bind the ✖ to function for removing the # keyword def keyword_remover ( keyword ) : def remover ( ev ) : self . remove_keyword ( keyword ) return remover # go thru all the keywords and bind them to keyword_remover() for cnt , keyword in enumerate ( self . keywords ) : uid = "kw_remover_id_%d" % cnt el = document [ uid ] el . bind ( "click" , keyword_remover ( keyword ) )

0<CODESPLIT>https://github.com/tariqdaouda/rabaDB/blob/42e0d6ee65149ae4f1e4c380cc695a9e7d2d1bbc/rabaDB/Raba.py#L672-L677<CODESPLIT>Raba.set<CODESPLIT>returns a set of the available fields . In order to be able ti securely loop of the fields raba_id and json are not included in the set<CODESPLIT>def set ( self , * * args ) : for k , v in args . items ( ) : setattr ( self , k , v )

1<CODESPLIT>https://github.com/yyuu/botornado/blob/fffb056f5ff2324d1d5c1304014cfb1d899f602e/boto/ec2/autoscale/__init__.py#L381-L394<CODESPLIT>AutoScaleConnection.delete_policy<CODESPLIT>Delete a policy .<CODESPLIT>def delete_policy ( self , policy_name , autoscale_group = None ) : params = { 'PolicyName' : policy_name } if autoscale_group : params [ 'AutoScalingGroupName' ] = autoscale_group return self . get_status ( 'DeletePolicy' , params )

0<CODESPLIT>https://github.com/yyuu/botornado/blob/fffb056f5ff2324d1d5c1304014cfb1d899f602e/boto/s3/lifecycle.py#L90-L99<CODESPLIT>Lifecycle.add_rule<CODESPLIT>Returns a string containing the XML version of the Lifecycle configuration as defined by S3 .<CODESPLIT>def add_rule ( self , id , prefix , status , expiration ) : rule = Rule ( id , prefix , status , expiration ) self . append ( rule )

1<CODESPLIT>https://github.com/Bachmann1234/diff-cover/blob/901cb3fc986982961785e841658085ead453c6c9/diff_cover/diff_reporter.py#L177-L225<CODESPLIT>GitDiffReporter._git_diff<CODESPLIT>Run git diff and returns a dict in which the keys are changed file paths and the values are lists of line numbers .<CODESPLIT>def _git_diff ( self ) : # If we do not have a cached result, execute `git diff` if self . _diff_dict is None : result_dict = dict ( ) for diff_str in self . _get_included_diff_results ( ) : # Parse the output of the diff string diff_dict = self . _parse_diff_str ( diff_str ) for src_path in diff_dict . keys ( ) : if self . _is_path_excluded ( src_path ) : continue # If no _supported_extensions provided, or extension present: process root , extension = os . path . splitext ( src_path ) extension = extension [ 1 : ] . lower ( ) # 'not self._supported_extensions' tests for both None and empty list [] if not self . _supported_extensions or extension in self . _supported_extensions : added_lines , deleted_lines = diff_dict [ src_path ] # Remove any lines from the dict that have been deleted # Include any lines that have been added result_dict [ src_path ] = [ line for line in result_dict . get ( src_path , [ ] ) if not line in deleted_lines ] + added_lines # Eliminate repeats and order line numbers for ( src_path , lines ) in result_dict . items ( ) : result_dict [ src_path ] = self . _unique_ordered_lines ( lines ) # Store the resulting dict self . _diff_dict = result_dict # Return the diff cache return self . _diff_dict

0<CODESPLIT>https://github.com/Gandi/gandi.cli/blob/6ee5b8fc8ec44b0a6c232043ca610606ad8f693d/gandi/cli/modules/cert.py#L169-L177<CODESPLIT>Certificate.usable_ids<CODESPLIT>List possible certificate packages .<CODESPLIT>def usable_ids ( cls , id , accept_multi = True ) : try : qry_id = [ int ( id ) ] except ValueError : try : qry_id = cls . from_cn ( id ) except Exception : qry_id = None if not qry_id or not accept_multi and len ( qry_id ) != 1 : msg = 'unknown identifier %s' % id cls . error ( msg ) return qry_id if accept_multi else qry_id [ 0 ]

0<CODESPLIT>https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/reprfunc.py#L136-L161<CODESPLIT>ordered_repr<CODESPLIT>Convenience function for : func : __repr__ . Here you pass a list of internal attributes and it assumes that the : func : __init__ parameter names have the leading underscore dropped .<CODESPLIT>def ordered_repr ( obj : object , attrlist : Iterable [ str ] , joiner : str = COMMA_SPACE ) -> str : return "<{classname}({kvp})>" . format ( classname = type ( obj ) . __name__ , kvp = joiner . join ( "{}={}" . format ( a , repr ( getattr ( obj , a ) ) ) for a in attrlist ) )

0<CODESPLIT>https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/patched/notpip/_internal/wheel.py#L790-L812<CODESPLIT>should_use_ephemeral_cache<CODESPLIT>Format command information for logging .<CODESPLIT>def should_use_ephemeral_cache ( req , # type: InstallRequirement format_control , # type: FormatControl autobuilding , # type: bool cache_available # type: bool ) : # type: (...) -> Optional[bool] if req . constraint : return None if req . is_wheel : if not autobuilding : logger . info ( 'Skipping %s, due to already being wheel.' , req . name , ) return None if not autobuilding : return False if req . editable or not req . source_dir : return None if req . link and not req . link . is_artifact : # VCS checkout. Build wheel just for this run. return True if "binary" not in format_control . get_allowed_formats ( canonicalize_name ( req . name ) ) : logger . info ( "Skipping bdist_wheel for %s, due to binaries " "being disabled for it." , req . name , ) return None link = req . link base , ext = link . splitext ( ) if cache_available and _contains_egg_info ( base ) : return False # Otherwise, build the wheel just for this run using the ephemeral # cache since we are either in the case of e.g. a local directory, or # no cache directory is available to use. return True

0<CODESPLIT>https://github.com/ojake/django-tracked-model/blob/19bc48874dd2e5fb5defedc6b8c5c3915cce1424/tracked_model/serializer.py#L53-L83<CODESPLIT>restore_model<CODESPLIT>Returns obj as a dict .<CODESPLIT>def restore_model ( cls , data ) : obj = cls ( ) for field in data : setattr ( obj , field , data [ field ] [ Field . VALUE ] ) return obj

0<CODESPLIT>https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hmtk/plotting/mapping.py#L480-L524<CODESPLIT>HMTKBaseMap.add_focal_mechanism<CODESPLIT>Creates a plot of a catalogue showing where particular clusters exist<CODESPLIT>def add_focal_mechanism ( self , catalogue , magnitude = None , overlay = True ) : longitude = catalogue . data [ 'longitude' ] latitude = catalogue . data [ 'latitude' ] strike = catalogue . data [ 'strike1' ] dip = catalogue . data [ 'dip1' ] rake = catalogue . data [ 'rake1' ] if not magnitude or ( magnitude < 0 ) : magnitude = catalogue . data [ 'magnitude' ] for i , mag in enumerate ( magnitude ) : color = self . _select_color_mag ( mag ) focal_mechanism = [ strike [ i ] , dip [ i ] , rake [ i ] ] x , y = self . m ( longitude [ i ] , latitude [ i ] ) self . m . plot ( x , y ) size = mag * 10000 beach = Beach ( focal_mechanism , linewidth = 1 , xy = ( x , y ) , width = size , zorder = size , facecolor = color ) self . ax . add_collection ( beach ) if not overlay : plt . show ( ) else : for i in range ( 0 , catalogue . get_number_tensors ( ) ) : x , y = self . m ( longitude [ i ] , latitude [ i ] ) self . m . plot ( x , y ) focal_mechanism = [ strike [ i ] , dip [ i ] , rake [ i ] ] size = magnitude * 10000. beach = Beach ( focal_mechanism , linewidth = 1 , xy = ( x , y ) , width = size , zorder = size , facecolor = 'r' ) self . ax . add_collection ( beach ) if not overlay : plt . show ( )

0<CODESPLIT>https://github.com/dossier/dossier.web/blob/1cad1cce3c37d3a4e956abc710a2bc1afe16a092/dossier/web/interface.py#L268-L297<CODESPLIT>SearchEngine.results<CODESPLIT>Creates a filter predicate .<CODESPLIT>def results ( self ) : results = self . recommendations ( ) transformed = [ ] for t in results [ 'results' ] : if len ( t ) == 2 : cid , fc = t info = { } elif len ( t ) == 3 : cid , fc , info = t else : bottle . abort ( 500 , 'Invalid search result: "%r"' % t ) result = info result [ 'content_id' ] = cid if not self . params [ 'omit_fc' ] : result [ 'fc' ] = util . fc_to_json ( fc ) transformed . append ( result ) results [ 'results' ] = transformed return results

0<CODESPLIT>https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/server/grr_response_server/gui/api_plugins/flow.py#L160-L173<CODESPLIT>ApiFlowDescriptor._GetArgsDescription<CODESPLIT>Get a description of the calling prototype for this flow class .<CODESPLIT>def _GetArgsDescription ( self , args_type ) : args = { } if args_type : for type_descriptor in args_type . type_infos : if not type_descriptor . hidden : args [ type_descriptor . name ] = { "description" : type_descriptor . description , "default" : type_descriptor . default , "type" : "" , } if type_descriptor . type : args [ type_descriptor . name ] [ "type" ] = type_descriptor . type . __name__ return args

1<CODESPLIT>https://github.com/gwpy/gwpy/blob/7a92b917e7dd2d99b15895293a1fa1d66cdb210a/gwpy/signal/spectral/_ui.py#L246-L263<CODESPLIT>psd<CODESPLIT>Generate a PSD using a method function<CODESPLIT>def psd ( timeseries , method_func , * args , * * kwargs ) : # decorator has translated the arguments for us, so just call psdn() return _psdn ( timeseries , method_func , * args , * * kwargs )

1<CODESPLIT>https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/plugins/utils.py#L13-L28<CODESPLIT>get_plugin_modules<CODESPLIT>Get plugin modules from input strings : param tuple plugins : a tuple of plugin names in str<CODESPLIT>def get_plugin_modules ( plugins ) : if not plugins : raise MissingPluginNames ( "input plugin names are required" ) modules = [ ] for plugin in plugins : short_name = PLUGIN_MAPPING . get ( plugin . lower ( ) , plugin . lower ( ) ) full_path = '%s%s' % ( module_prefix , short_name ) modules . append ( importlib . import_module ( full_path ) ) return tuple ( modules )

0<CODESPLIT>https://github.com/TurboGears/backlash/blob/b8c73a6c8a203843f5a52c43b858ae5907fb2a4f/backlash/tbtools.py#L343-L353<CODESPLIT>Frame.render_source<CODESPLIT>Like the plaintext attribute but returns a generator<CODESPLIT>def render_source ( self ) : return SOURCE_TABLE_HTML % text_ ( '\n' . join ( line . render ( ) for line in self . get_annotated_lines ( ) ) )

0<CODESPLIT>https://github.com/alvinwan/TexSoup/blob/63323ed71510fd2351102b8c36660a3b7703cead/TexSoup/data.py#L228-L247<CODESPLIT>TexNode.children<CODESPLIT>r This is valid if and only if<CODESPLIT>def children ( self ) : for child in self . expr . children : node = TexNode ( child ) node . parent = self yield node

1<CODESPLIT>https://github.com/kgritesh/pip-save/blob/70a1269db5db05bb850c2caa00222ebe40b2f2fd/setup.py#L19-L22<CODESPLIT>get_absolute_path<CODESPLIT>Transform relative pathnames into absolute pathnames .<CODESPLIT>def get_absolute_path ( * args ) : directory = os . path . dirname ( os . path . abspath ( __file__ ) ) return os . path . join ( directory , * args )

0<CODESPLIT>https://github.com/JdeRobot/base/blob/303b18992785b2fe802212f2d758a60873007f1f/src/drivers/MAVLinkServer/MAVProxy/modules/mavproxy_misc.py#L166-L172<CODESPLIT>MiscModule.cmd_rcbind<CODESPLIT>show version<CODESPLIT>def cmd_rcbind ( self , args ) : if len ( args ) < 1 : print ( "Usage: rcbind <dsmmode>" ) return self . master . mav . command_long_send ( self . settings . target_system , self . settings . target_component , mavutil . mavlink . MAV_CMD_START_RX_PAIR , 0 , float ( args [ 0 ] ) , 0 , 0 , 0 , 0 , 0 , 0 )

0<CODESPLIT>https://github.com/mredar/django-dublincore/blob/eabac3fc9225d7961a5d509718c1d3059e3681cb/dublincore/models.py#L145-L174<CODESPLIT>Subscene.search<CODESPLIT>Make sure that the term is valid . If changed create a QualifiedDublinCoreElementHistory object and save it .<CODESPLIT>def search ( self , release_name , lang ) : subtitles = [ ] language = babelfish . Language . fromalpha2 ( lang ) . name payload = { 'q' : release_name , 'r' : 'true' } url = 'http://subscene.com/subtitles/release' response = requests . get ( url , params = payload , headers = self . headers ) . text soup = BS ( response , "lxml" ) positive = soup . find_all ( class_ = 'l r positive-icon' ) neutral = soup . find_all ( class_ = 'l r neutral-icon' ) for node in chain ( positive , neutral ) : suburl = node . parent [ 'href' ] quality = node [ 'class' ] [ 2 ] . split ( '-' ) [ 0 ] name = node . parent . findChildren ( ) [ 1 ] . text . strip ( ) if language . lower ( ) in suburl and 'trailer' not in name . lower ( ) : subtitle = { } subtitle [ 'release' ] = name subtitle [ 'link' ] = urljoin ( self . base_url , suburl ) subtitle [ 'lang' ] = lang subtitle [ 'movie' ] = name + quality subtitle [ 'date' ] = '10/10/2010' subtitles . append ( subtitle ) return subtitles

1<CODESPLIT>https://github.com/senaite/senaite.jsonapi/blob/871959f4b1c9edbb477e9456325527ca78e13ec6/src/senaite/jsonapi/api.py#L1447-L1463<CODESPLIT>validate_object<CODESPLIT>Validate the entire object<CODESPLIT>def validate_object ( brain_or_object , data ) : obj = get_object ( brain_or_object ) # Call the validator of AT Content Types if is_at_content ( obj ) : return obj . validate ( data = data ) return { }

0<CODESPLIT>https://github.com/baguette-io/baguette-messaging/blob/8d1c4707ea7eace8617fed2d97df2fcc9d0cdee1/farine/amqp/consumer.py#L125-L144<CODESPLIT>modelserializer_factory<CODESPLIT>| Launch the consumer . | It can listen forever for messages or just wait for one .<CODESPLIT>def modelserializer_factory ( model , serializer = None , fields = None , exclude = None ) : # default values serializer = serializer or serializers . ModelSerializer attrs = { 'model' : model } if fields == '__all__' : opts = model . _meta . concrete_model . _meta attrs [ 'fields' ] = [ field . name for field in opts . fields if field . serialize ] elif fields is not None : attrs [ 'fields' ] = fields if exclude is not None : attrs [ 'exclude' ] = exclude # create meta class parent = ( object , ) Meta = type ( 'Meta' , parent , attrs ) # Give this new serializer class a reasonable name. class_name = model . __name__ + 'Serializer' # Class attributes for the new serializer class. serializer_class_attrs = { 'Meta' : Meta , } return type ( serializer ) ( class_name , ( serializer , ) , serializer_class_attrs )

0<CODESPLIT>https://github.com/knagra/farnsworth/blob/1b6589f0d9fea154f0a1e2231ed906764ed26d26/base/views.py#L133-L289<CODESPLIT>my_profile_view<CODESPLIT>The view of the homepage .<CODESPLIT>def my_profile_view ( request ) : page_name = "Profile Page" if request . user . username == ANONYMOUS_USERNAME : return red_home ( request , MESSAGES [ 'SPINELESS' ] ) userProfile = UserProfile . objects . get ( user = request . user ) change_password_form = PasswordChangeForm ( request . user , request . POST if "submit_password_form" in request . POST else None , ) update_email_form = UpdateEmailForm ( request . POST if "submit_profile_form" in request . POST else None , instance = request . user , prefix = "user" , ) update_profile_form = UpdateProfileForm ( request . POST if "submit_profile_form" in request . POST else None , instance = userProfile , prefix = "profile" , ) if change_password_form . is_valid ( ) : change_password_form . save ( ) messages . add_message ( request , messages . SUCCESS , "Your password was successfully changed." ) return HttpResponseRedirect ( reverse ( 'my_profile' ) ) if update_email_form . is_valid ( ) and update_profile_form . is_valid ( ) : update_email_form . save ( ) update_profile_form . save ( ) messages . add_message ( request , messages . SUCCESS , "Your profile has been successfully updated." ) return HttpResponseRedirect ( reverse ( 'my_profile' ) ) return render_to_response ( 'my_profile.html' , { 'page_name' : page_name , "update_email_form" : update_email_form , 'update_profile_form' : update_profile_form , 'change_password_form' : change_password_form , } , context_instance = RequestContext ( request ) )

1<CODESPLIT>https://github.com/gmr/infoblox/blob/163dd9cff5f77c08751936c56aa8428acfd2d208/infoblox/cli.py#L41-L49<CODESPLIT>InfobloxHost.delete_old_host<CODESPLIT>Remove all records for the host .<CODESPLIT>def delete_old_host ( self , hostname ) : host = Host ( self . session , name = hostname ) return host . delete ( )

1<CODESPLIT>https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/transform/nystroem_tica.py#L411-L414<CODESPLIT>oASIS_Nystroem._compute_error<CODESPLIT>Evaluate the absolute error of the Nystroem approximation for each column<CODESPLIT>def _compute_error ( self ) : # err_i = sum_j R_{k,ij} A_{k,ji} - d_i self . _err = np . sum ( np . multiply ( self . _R_k , self . _C_k . T ) , axis = 0 ) - self . _d

1<CODESPLIT>https://github.com/PMEAL/OpenPNM/blob/0547b5724ffedc0a593aae48639d36fe10e0baed/openpnm/io/MAT.py#L29-L60<CODESPLIT>MAT.save<CODESPLIT>r Write Network to a Mat file for exporting to Matlab .<CODESPLIT>def save ( cls , network , phases = [ ] , filename = '' ) : project , network , phases = cls . _parse_args ( network = network , phases = phases ) network = network [ 0 ] # Write to file if filename == '' : filename = project . name filename = cls . _parse_filename ( filename = filename , ext = 'mat' ) d = Dict . to_dict ( network = network , phases = phases , interleave = True ) d = FlatDict ( d , delimiter = '|' ) d = sanitize_dict ( d ) new_d = { } for key in list ( d . keys ( ) ) : new_key = key . replace ( '|' , '_' ) . replace ( '.' , '_' ) new_d [ new_key ] = d . pop ( key ) spio . savemat ( file_name = filename , mdict = new_d )

0<CODESPLIT>https://github.com/PMBio/limix-backup/blob/1e201fdb5c694d0d5506f207f3de65d8ef66146c/limix/mtSet/iset_full.py#L48-L53<CODESPLIT>ISet_Full.setXr<CODESPLIT>fit null model<CODESPLIT>def setXr ( self , Xr ) : self . Xr = Xr self . gp_block . covar . G = Xr

0<CODESPLIT>https://github.com/santosjorge/cufflinks/blob/ca1cbf93998dc793d0b1f8ac30fe1f2bd105f63a/cufflinks/datagen.py#L157-L171<CODESPLIT>scatter<CODESPLIT>Returns a DataFrame with the required format for a heatmap plot<CODESPLIT>def scatter ( n_categories = 5 , n = 10 , prefix = 'category' , mode = None ) : categories = [ ] for i in range ( n_categories ) : categories . extend ( [ prefix + str ( i + 1 ) ] * n ) return pd . DataFrame ( { 'x' : np . random . randn ( n * n_categories ) , 'y' : np . random . randn ( n * n_categories ) , 'text' : getName ( n * n_categories , mode = mode ) , 'categories' : categories } )

0<CODESPLIT>https://github.com/productml/blurr/blob/1b688b2c4a9bbbb2139c58bf0682ddc05a6c24fa/blurr/core/evaluation.py#L91-L97<CODESPLIT>EvaluationContext.global_add<CODESPLIT>Merges the provided evaluation context to the current evaluation context . : param evaluation_context : Evaluation context to merge .<CODESPLIT>def global_add ( self , key : str , value : Any ) -> None : self . global_context [ key ] = value

0<CODESPLIT>https://github.com/RomelTorres/alpha_vantage/blob/4e0b5057e520e3e3de69cf947301765817290121/alpha_vantage/techindicators.py#L253-L290<CODESPLIT>TechIndicators.get_stochrsi<CODESPLIT>Return the stochatic oscillator values in two json objects as data and meta_data . It raises ValueError when problems arise<CODESPLIT>def get_stochrsi ( self , symbol , interval = 'daily' , time_period = 20 , series_type = 'close' , fastkperiod = None , fastdperiod = None , fastdmatype = None ) : _FUNCTION_KEY = "STOCHRSI" return _FUNCTION_KEY , 'Technical Analysis: STOCHRSI' , 'Meta Data'

1<CODESPLIT>https://github.com/opendatateam/udata/blob/f016585af94b0ff6bd73738c700324adc8ba7f8f/udata/search/commands.py#L109-L127<CODESPLIT>set_alias<CODESPLIT>Properly end an indexation by creating an alias . Previous alias is deleted if needed .<CODESPLIT>def set_alias ( index_name , delete = True ) : log . info ( 'Creating alias "{0}" on index "{1}"' . format ( es . index_name , index_name ) ) if es . indices . exists_alias ( name = es . index_name ) : alias = es . indices . get_alias ( name = es . index_name ) previous_indices = alias . keys ( ) if index_name not in previous_indices : es . indices . put_alias ( index = index_name , name = es . index_name ) for index in previous_indices : if index != index_name : es . indices . delete_alias ( index = index , name = es . index_name ) if delete : es . indices . delete ( index = index ) else : es . indices . put_alias ( index = index_name , name = es . index_name )

0<CODESPLIT>https://github.com/tanghaibao/jcvi/blob/d2e31a77b6ade7f41f3b321febc2b4744d1cdeca/jcvi/assembly/ca.py#L128-L212<CODESPLIT>augustus<CODESPLIT>%prog prune best . edges<CODESPLIT>def augustus ( args ) : p = OptionParser ( augustus . __doc__ ) p . add_option ( "--autotrain" , default = False , action = "store_true" , help = "Run autoAugTrain.pl to iteratively train AUGUSTUS" ) p . set_home ( "augustus" ) opts , args = p . parse_args ( args ) if len ( args ) != 3 : sys . exit ( not p . print_help ( ) ) species , gffile , fastafile = args mhome = opts . augustus_home augdir = "augustus" cwd = os . getcwd ( ) mkdir ( augdir ) os . chdir ( augdir ) target = "{0}/config/species/{1}" . format ( mhome , species ) if op . exists ( target ) : logging . debug ( "Removing existing target `{0}`" . format ( target ) ) sh ( "rm -rf {0}" . format ( target ) ) sh ( "{0}/scripts/new_species.pl --species={1}" . format ( mhome , species ) ) sh ( "{0}/scripts/gff2gbSmallDNA.pl ../{1} ../{2} 1000 raw.gb" . format ( mhome , gffile , fastafile ) ) sh ( "{0}/bin/etraining --species={1} raw.gb 2> train.err" . format ( mhome , species ) ) sh ( "cat train.err | perl -pe 's/.*in sequence (\S+): .*/$1/' > badgenes.lst" ) sh ( "{0}/scripts/filterGenes.pl badgenes.lst raw.gb > training.gb" . format ( mhome ) ) sh ( "grep -c LOCUS raw.gb training.gb" ) # autoAugTrain failed to execute, disable for now if opts . autotrain : sh ( "rm -rf {0}" . format ( target ) ) sh ( "{0}/scripts/autoAugTrain.pl --trainingset=training.gb --species={1}" . format ( mhome , species ) ) os . chdir ( cwd ) sh ( "cp -r {0} augustus/" . format ( target ) )

0<CODESPLIT>https://github.com/projectshift/shift-boiler/blob/8e6f3a3e4b9493fb6c8bd16bed160ede153bfb0b/boiler/user/models.py#L396-L406<CODESPLIT>cd<CODESPLIT>Roles accessor<CODESPLIT>def cd ( * args ) : if args [ 0 ] == "-" : try : newpwd , os . environ [ "OLDPWD" ] = os . environ [ "OLDPWD" ] , os . getcwd ( ) except KeyError as e : # $OLDPWD initially not set raise e else : os . chdir ( newpwd ) print ( newpwd ) else : os . environ [ "OLDPWD" ] = os . getcwd ( ) os . chdir ( * args )

1<CODESPLIT>https://github.com/maweigert/gputools/blob/6ab26efeb05dceef74cf13aadeeeb9b009b529dd/gputools/fft/fftshift.py#L83-L119<CODESPLIT>_fftshift_single<CODESPLIT>basic fftshift of an OCLArray<CODESPLIT>def _fftshift_single ( d_g , res_g , ax = 0 ) : dtype_kernel_name = { np . float32 : "fftshift_1_f" , np . complex64 : "fftshift_1_c" } N = d_g . shape [ ax ] N1 = 1 if ax == 0 else np . prod ( d_g . shape [ : ax ] ) N2 = 1 if ax == len ( d_g . shape ) - 1 else np . prod ( d_g . shape [ ax + 1 : ] ) dtype = d_g . dtype . type prog = OCLProgram ( abspath ( "kernels/fftshift.cl" ) ) prog . run_kernel ( dtype_kernel_name [ dtype ] , ( N2 , N // 2 , N1 ) , None , d_g . data , res_g . data , np . int32 ( N ) , np . int32 ( N2 ) ) return res_g

0<CODESPLIT>https://github.com/LettError/MutatorMath/blob/10318fc4e7c9cee9df6130826829baea3054a42b/Lib/mutatorMath/ufo/document.py#L501-L576<CODESPLIT>DesignSpaceDocumentReader.readAxes<CODESPLIT>Read the source elements .<CODESPLIT>def readAxes ( self ) : for axisElement in self . root . findall ( ".axes/axis" ) : axis = { } axis [ 'name' ] = name = axisElement . attrib . get ( "name" ) axis [ 'tag' ] = axisElement . attrib . get ( "tag" ) axis [ 'minimum' ] = float ( axisElement . attrib . get ( "minimum" ) ) axis [ 'maximum' ] = float ( axisElement . attrib . get ( "maximum" ) ) axis [ 'default' ] = float ( axisElement . attrib . get ( "default" ) ) # we're not using the map for anything. axis [ 'map' ] = [ ] for warpPoint in axisElement . findall ( ".map" ) : inputValue = float ( warpPoint . attrib . get ( "input" ) ) outputValue = float ( warpPoint . attrib . get ( "output" ) ) axis [ 'map' ] . append ( ( inputValue , outputValue ) ) # there are labelnames in the element # but we don't need them for building the fonts. self . axes [ name ] = axis self . axesOrder . append ( axis [ 'name' ] )

0<CODESPLIT>https://github.com/nanoporetech/ont_fast5_api/blob/352b3903155fcf4f19234c4f429dcefaa6d6bc4a/ont_fast5_api/fast5_file.py#L486-L502<CODESPLIT>Fast5File.add_analysis_attributes<CODESPLIT>Add a new subgroup to an existing analysis group . : param group_name : The name of the analysis group you are adding to . : param subgroup_name : The name of the new subgroup . : param attrs : A dictionary representing the attributes to assign to the subgroup . The new subgroup must not already exist . The subgroup name can be a nested name such as Template / Data . This will create the Template subgroup ( if it does not exist ) and the Data subgroup below it .<CODESPLIT>def add_analysis_attributes ( self , group_name , attrs , clear = False ) : self . assert_writeable ( ) group = 'Analyses/{}' . format ( group_name ) self . _add_attributes ( group , attrs , clear )

0<CODESPLIT>https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/module/bucketing_module.py#L466-L479<CODESPLIT>BucketingModule.get_outputs<CODESPLIT>Updates parameters according to installed optimizer and the gradient computed in the previous forward - backward cycle .<CODESPLIT>def get_outputs ( self , merge_multi_context = True ) : assert self . binded and self . params_initialized return self . _curr_module . get_outputs ( merge_multi_context = merge_multi_context )

1<CODESPLIT>https://github.com/mamrhein/specification/blob/a4c09a0d286cda7a04e8a189f12e23edd97f64ea/specification/_extd_ast_expr.py#L412-L425<CODESPLIT>SourceGenerator.visit_Call<CODESPLIT>Return node s representation as function call .<CODESPLIT>def visit_Call ( self , node : AST , dfltChaining : bool = True ) -> str : args = node . args try : kwds = node . keywords except AttributeError : kwds = [ ] self . compact = True args_src = ( self . visit ( arg ) for arg in args ) kwds_src = ( self . visit ( kwd ) for kwd in kwds ) param_src = ', ' . join ( chain ( args_src , kwds_src ) ) src = f"{self.visit(node.func)}({param_src})" self . compact = False return src

0<CODESPLIT>https://github.com/fake-name/ChromeController/blob/914dd136184e8f1165c7aa6ef30418aaf10c61f0/ChromeController/manager.py#L165-L236<CODESPLIT>ChromeRemoteDebugInterface.set_cookie<CODESPLIT>Retreive the cookies from the remote browser .<CODESPLIT>def set_cookie ( self , cookie ) : # Function path: Network.setCookie # Domain: Network # Method name: setCookie # WARNING: This function is marked 'Experimental'! # Parameters: #         Required arguments: #                 'url' (type: string) -> The request-URI to associate with the setting of the cookie. This value can affect the default domain and path values of the created cookie. #                 'name' (type: string) -> The name of the cookie. #                 'value' (type: string) -> The value of the cookie. #         Optional arguments: #                 'domain' (type: string) -> If omitted, the cookie becomes a host-only cookie. #                 'path' (type: string) -> Defaults to the path portion of the url parameter. #                 'secure' (type: boolean) -> Defaults ot false. #                 'httpOnly' (type: boolean) -> Defaults to false. #                 'sameSite' (type: CookieSameSite) -> Defaults to browser default behavior. #                 'expirationDate' (type: Timestamp) -> If omitted, the cookie becomes a session cookie. # Returns: #         'success' (type: boolean) -> True if successfully set cookie. # Description: Sets a cookie with the given cookie data; may overwrite equivalent cookies if they exist. assert isinstance ( cookie , http . cookiejar . Cookie ) , 'The value passed to `set_cookie` must be an instance of http.cookiejar.Cookie().' + ' Passed: %s ("%s").' % ( type ( cookie ) , cookie ) # Yeah, the cookielib stores this attribute as a string, despite it containing a # boolean value. No idea why. is_http_only = str ( cookie . get_nonstandard_attr ( 'httponly' , 'False' ) ) . lower ( ) == "true" # I'm unclear what the "url" field is actually for. A cookie only needs the domain and # path component to be fully defined. Considering the API apparently allows the domain and # path parameters to be unset, I think it forms a partially redundant, with some # strange interactions with mode-changing between host-only and more general # cookies depending on what's set where. # Anyways, given we need a URL for the API to work properly, we produce a fake # host url by building it out of the relevant cookie properties. fake_url = urllib . parse . urlunsplit ( ( "http" if is_http_only else "https" , # Scheme cookie . domain , # netloc cookie . path , # path '' , # query '' , # fragment ) ) params = { 'url' : fake_url , 'name' : cookie . name , 'value' : cookie . value if cookie . value else "" , 'domain' : cookie . domain , 'path' : cookie . path , 'secure' : cookie . secure , 'expires' : float ( cookie . expires ) if cookie . expires else float ( 2 ** 32 ) , 'httpOnly' : is_http_only , # The "sameSite" flag appears to be a chromium-only extension for controlling # cookie sending in non-first-party contexts. See: # https://bugs.chromium.org/p/chromium/issues/detail?id=459154 # Anyways, we just use the default here, whatever that is. # sameSite       = cookie.xxx } ret = self . Network_setCookie ( * * params ) return ret

1<CODESPLIT>https://github.com/manns/pyspread/blob/0e2fd44c2e0f06605efc3058c20a43a8c1f9e7e0/pyspread/src/gui/_grid.py#L865-L881<CODESPLIT>GridCellEventHandlers.OnCellTextRotation<CODESPLIT>Cell text rotation event handler<CODESPLIT>def OnCellTextRotation ( self , event ) : with undo . group ( _ ( "Rotation" ) ) : self . grid . actions . toggle_attr ( "angle" ) self . grid . ForceRefresh ( ) self . grid . update_attribute_toolbar ( ) if is_gtk ( ) : try : wx . Yield ( ) except : pass event . Skip ( )

1<CODESPLIT>https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/repository/sessions.py#L2933-L2955<CODESPLIT>AssetCompositionDesignSession.move_asset_behind<CODESPLIT>Reorders assets in a composition by moving the specified asset behind of a reference asset .<CODESPLIT>def move_asset_behind ( self , asset_id , composition_id , reference_id ) : if ( not isinstance ( composition_id , ABCId ) and composition_id . get_identifier_namespace ( ) != 'repository.Composition' ) : raise errors . InvalidArgument ( 'the argument is not a valid OSID Id' ) composition_map , collection = self . _get_composition_collection ( composition_id ) composition_map [ 'assetIds' ] = move_id_behind ( asset_id , reference_id , composition_map [ 'assetIds' ] ) collection . save ( composition_map )

0<CODESPLIT>https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/sim_type.py#L1006-L1015<CODESPLIT>define_struct<CODESPLIT>Run a string through the C preprocessor that ships with pycparser but is weirdly inaccessible?<CODESPLIT>def define_struct ( defn ) : struct = parse_type ( defn ) ALL_TYPES [ struct . name ] = struct return struct

1<CODESPLIT>https://github.com/google/python-gflags/blob/4f06c3d0d6cbe9b1fb90ee9fb1c082b3bf9285f6/gflags/__init__.py#L187-L214<CODESPLIT>register_multi_flags_validator<CODESPLIT>Adds a constraint to multiple flags .<CODESPLIT>def register_multi_flags_validator ( flag_names , multi_flags_checker , message = 'Flags validation failed' , flag_values = FLAGS ) : v = gflags_validators . MultiFlagsValidator ( flag_names , multi_flags_checker , message ) _add_validator ( flag_values , v )

0<CODESPLIT>https://github.com/idank/bashlex/blob/800cb7e3c634eaa3c81f8a8648fd7fd4e27050ac/bashlex/parser.py#L64-L115<CODESPLIT>p_redirection_heredoc<CODESPLIT>redirection : GREATER WORD | LESS WORD | NUMBER GREATER WORD | NUMBER LESS WORD | REDIR_WORD GREATER WORD | REDIR_WORD LESS WORD | GREATER_GREATER WORD | NUMBER GREATER_GREATER WORD | REDIR_WORD GREATER_GREATER WORD | GREATER_BAR WORD | NUMBER GREATER_BAR WORD | REDIR_WORD GREATER_BAR WORD | LESS_GREATER WORD | NUMBER LESS_GREATER WORD | REDIR_WORD LESS_GREATER WORD | LESS_LESS_LESS WORD | NUMBER LESS_LESS_LESS WORD | REDIR_WORD LESS_LESS_LESS WORD | LESS_AND NUMBER | NUMBER LESS_AND NUMBER | REDIR_WORD LESS_AND NUMBER | GREATER_AND NUMBER | NUMBER GREATER_AND NUMBER | REDIR_WORD GREATER_AND NUMBER | LESS_AND WORD | NUMBER LESS_AND WORD | REDIR_WORD LESS_AND WORD | GREATER_AND WORD | NUMBER GREATER_AND WORD | REDIR_WORD GREATER_AND WORD | GREATER_AND DASH | NUMBER GREATER_AND DASH | REDIR_WORD GREATER_AND DASH | LESS_AND DASH | NUMBER LESS_AND DASH | REDIR_WORD LESS_AND DASH | AND_GREATER WORD | AND_GREATER_GREATER WORD<CODESPLIT>def p_redirection_heredoc ( p ) : parserobj = p . context assert isinstance ( parserobj , _parser ) output = ast . node ( kind = 'word' , word = p [ len ( p ) - 1 ] , parts = [ ] , pos = p . lexspan ( len ( p ) - 1 ) ) if len ( p ) == 3 : p [ 0 ] = ast . node ( kind = 'redirect' , input = None , type = p [ 1 ] , heredoc = None , output = output , pos = ( p . lexpos ( 1 ) , p . endlexpos ( 2 ) ) ) else : p [ 0 ] = ast . node ( kind = 'redirect' , input = p [ 1 ] , type = p [ 2 ] , heredoc = None , output = output , pos = ( p . lexpos ( 1 ) , p . endlexpos ( 3 ) ) ) if p . slice [ len ( p ) - 2 ] . ttype == tokenizer . tokentype . LESS_LESS : parserobj . redirstack . append ( ( p [ 0 ] , False ) ) else : parserobj . redirstack . append ( ( p [ 0 ] , True ) )

0<CODESPLIT>https://github.com/OpenHumans/open-humans-api/blob/ca2a28cf5d55cfdae13dd222ba58c25565bdb86e/ohapi/projects.py#L66-L91<CODESPLIT>OHProject.download_member_shared<CODESPLIT>Download files to sync a local dir to match OH member project data .<CODESPLIT>def download_member_shared ( cls , member_data , target_member_dir , source = None , max_size = MAX_SIZE_DEFAULT , id_filename = False ) : logging . debug ( 'Download member shared data...' ) sources_shared = member_data [ 'sources_shared' ] file_data = cls . _get_member_file_data ( member_data , id_filename = id_filename ) logging . info ( 'Downloading member data to {}' . format ( target_member_dir ) ) for basename in file_data : # If not in sources shared, it's the project's own data. Skip. if file_data [ basename ] [ 'source' ] not in sources_shared : continue # Filter source if specified. Determine target directory for file. if source : if source == file_data [ basename ] [ 'source' ] : target_filepath = os . path . join ( target_member_dir , basename ) else : continue else : source_data_dir = os . path . join ( target_member_dir , file_data [ basename ] [ 'source' ] ) if not os . path . exists ( source_data_dir ) : os . mkdir ( source_data_dir ) target_filepath = os . path . join ( source_data_dir , basename ) download_file ( download_url = file_data [ basename ] [ 'download_url' ] , target_filepath = target_filepath , max_bytes = parse_size ( max_size ) )

0<CODESPLIT>https://github.com/ajk8/hatchery/blob/e068c9f5366d2c98225babb03d4cde36c710194f/hatchery/executor.py#L88-L101<CODESPLIT>call<CODESPLIT>Call a setup . py command or list of commands<CODESPLIT>def call ( cmd_args , suppress_output = False ) : if not funcy . is_list ( cmd_args ) and not funcy . is_tuple ( cmd_args ) : cmd_args = shlex . split ( cmd_args ) logger . info ( 'executing `{}`' . format ( ' ' . join ( cmd_args ) ) ) call_request = CallRequest ( cmd_args , suppress_output = suppress_output ) call_result = call_request . run ( ) if call_result . exitval : logger . error ( '`{}` returned error code {}' . format ( ' ' . join ( cmd_args ) , call_result . exitval ) ) return call_result

1<CODESPLIT>https://github.com/i3visio/osrframework/blob/83437f4c14c9c08cb80a896bd9834c77f6567871/osrframework/api/twitter_api.py#L669-L695<CODESPLIT>TwitterAPIWrapper.get_friends<CODESPLIT>Method to get the friends of a user .<CODESPLIT>def get_friends ( self , query ) : # Connecting to the API api = self . _connectToAPI ( ) # Verifying the limits of the API self . _rate_limit_status ( api = api , mode = "get_friends" ) # Making the call to the API try : friends_ids = api . friends_ids ( query ) except : return [ ] """res = []         # Extracting the information from each profile         for a in aux:             us= self.getUser(a)             res.append(self._processUser(us))""" return friends_ids

0<CODESPLIT>https://github.com/IntelPython/mkl_fft/blob/54b3271d64666f9af9f11418b4ca43d69054eb94/mkl_fft/_numpy_fft.py#L164-L247<CODESPLIT>rfft<CODESPLIT>Compute the one - dimensional inverse discrete Fourier Transform .<CODESPLIT>def rfft ( a , n = None , axis = - 1 , norm = None ) : unitary = _unitary ( norm ) if unitary and n is None : a = asarray ( a ) n = a . shape [ axis ] output = mkl_fft . rfft_numpy ( a , n = n , axis = axis ) if unitary : output *= 1 / sqrt ( n ) return output

1<CODESPLIT>https://github.com/programa-stic/barf-project/blob/18ed9e5eace55f7bf6015ec57f037c364099021c/barf/analysis/gadgets/classifier.py#L104-L121<CODESPLIT>GadgetClassifier.classify<CODESPLIT>Classify gadgets .<CODESPLIT>def classify ( self , gadget ) : typed_gadgets = [ ] for g_type , g_classifier in self . _classifiers . items ( ) : try : typed_gadgets += self . _classify ( gadget , g_classifier , g_type , self . _emu_iters ) except : import traceback print ( "[-] Error classifying gadgets :" ) print ( gadget ) print ( "" ) print ( traceback . format_exc ( ) ) # Sort and return. return sorted ( typed_gadgets , key = lambda g : str ( g ) )

1<CODESPLIT>https://github.com/jobovy/galpy/blob/9c5b9fe65d58835624dffe432be282060918ee08/galpy/potential/LogarithmicHaloPotential.py#L103-L122<CODESPLIT>LogarithmicHaloPotential._Rforce<CODESPLIT>NAME : _Rforce PURPOSE : evaluate the radial force for this potential INPUT : R - Galactocentric cylindrical radius z - vertical height phi - azimuth t - time OUTPUT : the radial force HISTORY :<CODESPLIT>def _Rforce ( self , R , z , phi = 0. , t = 0. ) : if self . isNonAxi : Rt2 = R ** 2. * ( 1. - self . _1m1overb2 * nu . sin ( phi ) ** 2. ) return - Rt2 / R / ( Rt2 + ( z / self . _q ) ** 2. + self . _core2 ) else : return - R / ( R ** 2. + ( z / self . _q ) ** 2. + self . _core2 )

1<CODESPLIT>https://github.com/Scoppio/RagnarokEngine3/blob/4395d419ccd64fe9327c41f200b72ee0176ad896/RagnarokEngine3/RE3.py#L3016-L3019<CODESPLIT>Camera.Reset<CODESPLIT>Reset the camera back to its defaults .<CODESPLIT>def Reset ( self ) : self . pan = self . world_center self . desired_pan = self . pos

0<CODESPLIT>https://github.com/simonvh/genomepy/blob/abace2366511dbe855fe1430b1f7d9ec4cbf6d29/genomepy/utils.py#L10-L25<CODESPLIT>install<CODESPLIT>Generate a BED file with gap locations .<CODESPLIT>def install ( name , provider , genome_dir , localname , mask , regex , match , annotation ) : genomepy . install_genome ( name , provider , genome_dir = genome_dir , localname = localname , mask = mask , regex = regex , invert_match = not ( match ) , annotation = annotation )

1<CODESPLIT>https://github.com/roclark/sportsreference/blob/ea0bae432be76450e137671d2998eb38f962dffd/sportsreference/nhl/boxscore.py#L307-L357<CODESPLIT>Boxscore._parse_game_date_and_location<CODESPLIT>Retrieve the game s date and location .<CODESPLIT>def _parse_game_date_and_location ( self , boxscore ) : scheme = BOXSCORE_SCHEME [ "game_info" ] items = [ i . text ( ) for i in boxscore ( scheme ) . items ( ) ] game_info = items [ 0 ] . split ( '\n' ) arena = None attendance = None date = None duration = None playoff_round = None time = None if game_info [ 0 ] . count ( ',' ) == 2 : date = ',' . join ( game_info [ 0 ] . split ( ',' ) [ 0 : 2 ] ) . strip ( ) time = game_info [ 0 ] . split ( ',' ) [ - 1 ] . strip ( ) else : date = game_info [ 0 ] for line in game_info : if 'Arena: ' in line : arena = line . replace ( 'Arena: ' , '' ) if 'Attendance: ' in line : attendance = line . replace ( 'Attendance: ' , '' ) . replace ( ',' , '' ) if 'Game Duration: ' in line : duration = line . replace ( 'Game Duration: ' , '' ) if 'eastern first round' in line . lower ( ) or 'western first round' in line . lower ( ) or 'eastern second round' in line . lower ( ) or 'western second round' in line . lower ( ) or 'eastern conference finals' in line . lower ( ) or 'western conference finals' in line . lower ( ) or 'stanley cup final' in line . lower ( ) : playoff_round = line setattr ( self , '_arena' , arena ) setattr ( self , '_attendance' , attendance ) setattr ( self , '_date' , date ) setattr ( self , '_duration' , duration ) setattr ( self , '_playoff_round' , playoff_round ) setattr ( self , '_time' , time )

0<CODESPLIT>https://github.com/sphinx-gallery/sphinx-gallery/blob/b0c1f6701bf3f4cef238757e1105cf3686b5e674/sphinx_gallery/gen_rst.py#L524-L592<CODESPLIT>executable_script<CODESPLIT>Execute and capture output from python script already in block structure<CODESPLIT>def executable_script ( src_file , gallery_conf ) : filename_pattern = gallery_conf . get ( 'filename_pattern' ) execute = re . search ( filename_pattern , src_file ) and gallery_conf [ 'plot_gallery' ] return execute

1<CODESPLIT>https://github.com/raiden-network/raiden/blob/407ba15c72074e9de88771d6b9661ff4dc36bef5/raiden/waiting.py#L153-L210<CODESPLIT>wait_for_channel_in_states<CODESPLIT>Wait until all channels are in target_states .<CODESPLIT>def wait_for_channel_in_states ( raiden : 'RaidenService' , payment_network_id : PaymentNetworkID , token_address : TokenAddress , channel_ids : List [ ChannelID ] , retry_timeout : float , target_states : Sequence [ str ] , ) -> None : chain_state = views . state_from_raiden ( raiden ) token_network = views . get_token_network_by_token_address ( chain_state = chain_state , payment_network_id = payment_network_id , token_address = token_address , ) if token_network is None : raise ValueError ( f'The token {token_address} is not registered on the network {payment_network_id}.' , ) token_network_address = token_network . address list_cannonical_ids = [ CanonicalIdentifier ( chain_identifier = chain_state . chain_id , token_network_address = token_network_address , channel_identifier = channel_identifier , ) for channel_identifier in channel_ids ] while list_cannonical_ids : canonical_id = list_cannonical_ids [ - 1 ] chain_state = views . state_from_raiden ( raiden ) channel_state = views . get_channelstate_by_canonical_identifier ( chain_state = chain_state , canonical_identifier = canonical_id , ) channel_is_settled = ( channel_state is None or channel . get_status ( channel_state ) in target_states ) if channel_is_settled : list_cannonical_ids . pop ( ) else : gevent . sleep ( retry_timeout )

0<CODESPLIT>https://github.com/madedotcom/photon-pump/blob/ff0736c9cacd43c1f783c9668eefb53d03a3a93e/photonpump/connection.py#L601-L663<CODESPLIT>Client.get_event<CODESPLIT>Publish a single event to the EventStore .<CODESPLIT>async def get_event ( self , stream : str , event_number : int , resolve_links = True , require_master = False , correlation_id : uuid . UUID = None , ) -> msg . Event : correlation_id = correlation_id or uuid . uuid4 ( ) cmd = convo . ReadEvent ( stream , event_number , resolve_links , require_master , conversation_id = correlation_id , ) result = await self . dispatcher . start_conversation ( cmd ) return await result

0<CODESPLIT>https://github.com/zsimic/runez/blob/14363b719a1aae1528859a501a22d075ce0abfcc/src/runez/program.py#L163-L190<CODESPLIT>DeptRequest.get_dept_name<CODESPLIT>: param dict|None env_vars : Env vars to customize : param dict env : Original env vars<CODESPLIT>def get_dept_name ( self ) : self . logger . info ( "%s\t%s" % ( self . request_method , self . request_url ) ) return self . json_response . get ( "name" , None )

0<CODESPLIT>https://github.com/sternoru/goscalecms/blob/7eee50357c47ebdfe3e573a8b4be3b67892d229e/goscale/models.py#L66-L86<CODESPLIT>Post.json<CODESPLIT>Returns dictionary of post fields and attributes<CODESPLIT>def json ( self , dict = None , indent = None ) : if not dict : dict = self . dict ( ) for key , value in dict . iteritems ( ) : if type ( value ) == datetime . datetime : dict [ key ] = value . strftime ( conf . GOSCALE_ATOM_DATETIME_FORMAT ) return simplejson . dumps ( dict , indent = indent )

0<CODESPLIT>https://github.com/tanghaibao/jcvi/blob/d2e31a77b6ade7f41f3b321febc2b4744d1cdeca/jcvi/formats/agp.py#L1831-L1871<CODESPLIT>build<CODESPLIT>%prog validate agpfile componentfasta targetfasta<CODESPLIT>def build ( args ) : p = OptionParser ( build . __doc__ ) p . add_option ( "--newagp" , dest = "newagp" , default = False , action = "store_true" , help = "Check components to trim dangling N's [default: %default]" ) p . add_option ( "--novalidate" , dest = "novalidate" , default = False , action = "store_true" , help = "Don't validate the agpfile [default: %default]" ) opts , args = p . parse_args ( args ) if len ( args ) != 3 : sys . exit ( not p . print_help ( ) ) agpfile , componentfasta , targetfasta = args validate = not opts . novalidate if opts . newagp : assert agpfile . endswith ( ".agp" ) newagpfile = agpfile . replace ( ".agp" , ".trimmed.agp" ) newagp = open ( newagpfile , "w" ) else : newagpfile = None newagp = None agp = AGP ( agpfile , validate = validate , sorted = True ) agp . build_all ( componentfasta = componentfasta , targetfasta = targetfasta , newagp = newagp ) logging . debug ( "Target fasta written to `{0}`." . format ( targetfasta ) ) return newagpfile

0<CODESPLIT>https://github.com/jfilter/split-folders/blob/5047964ce86507283b769e5e826dde909e8687da/split_folders/split.py#L89-L99<CODESPLIT>list_files<CODESPLIT>Returns shuffled files<CODESPLIT>def list_files ( directory ) : return [ f for f in pathlib . Path ( directory ) . iterdir ( ) if f . is_file ( ) and not f . name . startswith ( '.' ) ]

0<CODESPLIT>https://github.com/dicaso/leopard/blob/ee9f45251aaacd1e453b135b419f4f0b50fb036e/leopard/__init__.py#L424-L459<CODESPLIT>Report.outputPDF<CODESPLIT>Output report to word docx<CODESPLIT>def outputPDF ( self , * * kwargs ) : import pylatex as pl geometry_options = { "tmargin" : "2cm" , "lmargin" : "2cm" } doc = pl . Document ( geometry_options = geometry_options ) #Following option avoids float error when to many unplaced figs or tabs # (to force placing floats also \clearpage can be used after a section for example) doc . append ( pl . utils . NoEscape ( r'\extrafloats{100}' ) ) doc . append ( pl . utils . NoEscape ( r'\title{' + self . title + '}' ) ) if self . addTime : from time import localtime , strftime doc . append ( pl . utils . NoEscape ( r'\date{' + strftime ( "%Y-%m-%d %H:%M:%S" , localtime ( ) ) + r'}' ) ) else : doc . append ( pl . utils . NoEscape ( r'\date{\today}' ) ) if self . author : doc . append ( pl . utils . NoEscape ( r'\author{' + self . author + '}' ) ) doc . append ( pl . utils . NoEscape ( r'\maketitle' ) ) # Append introduction if self . p : with doc . create ( pl . Section ( 'Introduction' ) ) : doc . append ( renewliner ( self . p ) ) # Sections c = count ( 1 ) for section in self . sections : section . sectionsPDF ( walkTrace = ( next ( c ) , ) , doc = doc ) # Append conclusion if self . conclusion : with doc . create ( pl . Section ( 'Conclusion' ) ) : doc . append ( renewliner ( self . conclusion ) ) # Generate pdf doc . generate_pdf ( self . outfile , * * kwargs )

0<CODESPLIT>https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/formatdb.py#L45-L58<CODESPLIT>FormatDb._get_result_paths<CODESPLIT>Set the input path and log path based on data ( a fasta filepath )<CODESPLIT>def _get_result_paths ( self , data ) : # access data through self.Parameters so we know it's been cast # to a FilePath wd = self . WorkingDir db_name = self . Parameters [ '-n' ] . Value log_name = self . Parameters [ '-l' ] . Value result = { } result [ 'log' ] = ResultPath ( Path = wd + log_name , IsWritten = True ) if self . Parameters [ '-p' ] . Value == 'F' : extensions = [ 'nhr' , 'nin' , 'nsq' , 'nsd' , 'nsi' ] else : extensions = [ 'phr' , 'pin' , 'psq' , 'psd' , 'psi' ] for extension in extensions : for file_path in glob ( wd + ( db_name + '*' + extension ) ) : # this will match e.g. nr.01.psd and nr.psd key = file_path . split ( db_name + '.' ) [ 1 ] result_path = ResultPath ( Path = file_path , IsWritten = True ) result [ key ] = result_path return result

0<CODESPLIT>https://github.com/a1ezzz/wasp-general/blob/1029839d33eb663f8dec76c1c46754d53c1de4a9/wasp_general/network/web/cookies.py#L254-L263<CODESPLIT>WHTTPCookieJar.ro<CODESPLIT>Remove cookie by its name<CODESPLIT>def ro ( self ) : ro_jar = WHTTPCookieJar ( ) for cookie in self . __cookies . values ( ) : ro_jar . add_cookie ( cookie . ro ( ) ) ro_jar . __ro_flag = True return ro_jar

0<CODESPLIT>https://github.com/HewlettPackard/python-hpOneView/blob/3c6219723ef25e6e0c83d44a89007f89bc325b89/hpOneView/resources/servers/server_profiles.py#L323-L331<CODESPLIT>EnclosureGroups.update_script<CODESPLIT>Retrieves the profile template for a given server profile .<CODESPLIT>def update_script ( self , script_body ) : uri = "{}/script" . format ( self . data [ 'uri' ] ) return self . _helper . update ( script_body , uri = uri )

0<CODESPLIT>https://github.com/databio/pypiper/blob/00e6c2b94033c4187d47ff14c5580bbfc2ff097f/pypiper/manager.py#L1780-L1829<CODESPLIT>PipelineManager._cleanup<CODESPLIT>Add files ( or regexs ) to a cleanup list to delete when this pipeline completes successfully . When making a call with run that produces intermediate files that should be deleted after the pipeline completes you flag these files for deletion with this command . Files added with clean_add will only be deleted upon success of the pipeline .<CODESPLIT>def _cleanup ( self , dry_run = False ) : if dry_run : # Move all unconditional cleans into the conditional list if len ( self . cleanup_list ) > 0 : combined_list = self . cleanup_list_conditional + self . cleanup_list self . cleanup_list_conditional = combined_list self . cleanup_list = [ ] if len ( self . cleanup_list ) > 0 : print ( "\nCleaning up flagged intermediate files. . ." ) for expr in self . cleanup_list : print ( "\nRemoving glob: " + expr ) try : # Expand regular expression files = glob . glob ( expr ) # Remove entry from cleanup list while files in self . cleanup_list : self . cleanup_list . remove ( files ) # and delete the files for file in files : if os . path . isfile ( file ) : print ( "`rm " + file + "`" ) os . remove ( os . path . join ( file ) ) elif os . path . isdir ( file ) : print ( "`rmdir " + file + "`" ) os . rmdir ( os . path . join ( file ) ) except : pass if len ( self . cleanup_list_conditional ) > 0 : run_flag = flag_name ( RUN_FLAG ) flag_files = [ fn for fn in glob . glob ( self . outfolder + flag_name ( "*" ) ) if COMPLETE_FLAG not in os . path . basename ( fn ) and not "{}_{}" . format ( self . name , run_flag ) == os . path . basename ( fn ) ] if len ( flag_files ) == 0 and not dry_run : print ( "\nCleaning up conditional list. . ." ) for expr in self . cleanup_list_conditional : print ( "\nRemoving glob: " + expr ) try : files = glob . glob ( expr ) while files in self . cleanup_list_conditional : self . cleanup_list_conditional . remove ( files ) for file in files : if os . path . isfile ( file ) : print ( "`rm " + file + "`" ) os . remove ( os . path . join ( file ) ) elif os . path . isdir ( file ) : print ( "`rmdir " + file + "`" ) os . rmdir ( os . path . join ( file ) ) except : pass else : print ( "\nConditional flag found: " + str ( [ os . path . basename ( i ) for i in flag_files ] ) ) print ( "\nThese conditional files were left in place:\n\n- " + "\n- " . join ( self . cleanup_list_conditional ) ) # Produce a cleanup script. no_cleanup_script = [ ] for cleandir in self . cleanup_list_conditional : try : items_to_clean = glob . glob ( cleandir ) for clean_item in items_to_clean : with open ( self . cleanup_file , "a" ) as clean_script : if os . path . isfile ( clean_item ) : clean_script . write ( "rm " + clean_item + "\n" ) elif os . path . isdir ( clean_item ) : clean_script . write ( "rmdir " + clean_item + "\n" ) except Exception as e : no_cleanup_script . append ( cleandir ) if no_cleanup_script : print ( '\n\nCould not produce cleanup script for item(s):\n\n- ' + '\n- ' . join ( no_cleanup_script ) )

1<CODESPLIT>https://github.com/glomex/gcdt/blob/cd67cf416371337b83cb9ca3f696277125703339/gcdt/event_source/s3.py#L51-L80<CODESPLIT>S3EventSource._get_notification_spec<CODESPLIT>if key_filters in self . _config : filters_spec = { Key : { FilterRules : [] } } # I do not think this is a useful structure : for filter in self . _config [ key_filters ] : if type in filter and value in filter and filter [ type ] in ( prefix suffix ) : rule = { Name : filter [ type ] . capitalize () Value : filter [ value ] } filters_spec [ Key ] [ FilterRules ] . append ( rule )<CODESPLIT>def _get_notification_spec ( self , lambda_arn ) : lambda_name = base . get_lambda_name ( lambda_arn ) notification_spec = { 'Id' : self . _make_notification_id ( lambda_name ) , 'Events' : [ e for e in self . _config [ 'events' ] ] , 'LambdaFunctionArn' : lambda_arn } # Add S3 key filters filter_rules = [ ] # look for filter rules for filter_type in [ 'prefix' , 'suffix' ] : if filter_type in self . _config : rule = { 'Name' : filter_type . capitalize ( ) , 'Value' : self . _config [ filter_type ] } filter_rules . append ( rule ) if filter_rules : notification_spec [ 'Filter' ] = { 'Key' : { 'FilterRules' : filter_rules } } return notification_spec

1<CODESPLIT>https://github.com/twisted/mantissa/blob/53e5502aba23ce99be78b27f923a276593033fe8/xmantissa/interstore.py#L909-L921<CODESPLIT>_AMPErrorExposer.expose<CODESPLIT>Expose a function for processing a given AMP error .<CODESPLIT>def expose ( self , commandObject , exceptionType ) : thunk = super ( _AMPErrorExposer , self ) . expose ( ( commandObject . commandName , commandObject . errors . get ( exceptionType ) ) ) def thunkplus ( function ) : result = thunk ( function ) result . command = commandObject result . exception = exceptionType return result return thunkplus

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/composer.py#L63-L209<CODESPLIT>_run_composer<CODESPLIT>Run PHP s composer with a specific action .<CODESPLIT>def _run_composer ( action , directory = None , composer = None , php = None , runas = None , prefer_source = None , prefer_dist = None , no_scripts = None , no_plugins = None , optimize = None , no_dev = None , quiet = False , composer_home = '/root' , extra_flags = None , env = None ) : if composer is not None : if php is None : php = 'php' else : composer = 'composer' # Validate Composer is there if not _valid_composer ( composer ) : raise CommandNotFoundError ( '\'composer.{0}\' is not available. Couldn\'t find \'{1}\'.' . format ( action , composer ) ) if action is None : raise SaltInvocationError ( 'The \'action\' argument is required' ) # Don't need a dir for the 'selfupdate' action; all other actions do need a dir if directory is None and action != 'selfupdate' : raise SaltInvocationError ( 'The \'directory\' argument is required for composer.{0}' . format ( action ) ) # Base Settings cmd = [ composer , action , '--no-interaction' , '--no-ansi' ] if extra_flags is not None : cmd . extend ( salt . utils . args . shlex_split ( extra_flags ) ) # If php is set, prepend it if php is not None : cmd = [ php ] + cmd # Add Working Dir if directory is not None : cmd . extend ( [ '--working-dir' , directory ] ) # Other Settings if quiet is True : cmd . append ( '--quiet' ) if no_dev is True : cmd . append ( '--no-dev' ) if prefer_source is True : cmd . append ( '--prefer-source' ) if prefer_dist is True : cmd . append ( '--prefer-dist' ) if no_scripts is True : cmd . append ( '--no-scripts' ) if no_plugins is True : cmd . append ( '--no-plugins' ) if optimize is True : cmd . append ( '--optimize-autoloader' ) if env is not None : env = salt . utils . data . repack_dictlist ( env ) env [ 'COMPOSER_HOME' ] = composer_home else : env = { 'COMPOSER_HOME' : composer_home } result = __salt__ [ 'cmd.run_all' ] ( cmd , runas = runas , env = env , python_shell = False ) if result [ 'retcode' ] != 0 : raise CommandExecutionError ( result [ 'stderr' ] ) if quiet is True : return True return result

0<CODESPLIT>https://github.com/studionow/pybrightcove/blob/19c946b689a80156e070fe9bc35589c4b768e614/pybrightcove/http_core.py#L138-L145<CODESPLIT>Uri._get_relative_path<CODESPLIT>Creates a deep copy of this request .<CODESPLIT>def _get_relative_path ( self ) : param_string = self . _get_query_string ( ) if self . path is None : path = '/' else : path = self . path if param_string : return '?' . join ( [ path , param_string ] ) else : return path

0<CODESPLIT>https://github.com/rackerlabs/rackspace-python-neutronclient/blob/5a5009a8fe078e3aa1d582176669f1b28ab26bef/neutronclient/v2_0/client.py#L968-L971<CODESPLIT>Client.retrieve_loadbalancer_stats<CODESPLIT>Updates a load balancer .<CODESPLIT>def retrieve_loadbalancer_stats ( self , loadbalancer , * * _params ) : return self . get ( self . lbaas_loadbalancer_path_stats % ( loadbalancer ) , params = _params )

1<CODESPLIT>https://github.com/eevee/dictproxyhack/blob/79962f974d6be6578461bad37834e6a4a651ce02/dictproxyhack.py#L71-L111<CODESPLIT>_get_from_c_api<CODESPLIT>dictproxy does exist in previous versions but the Python constructor refuses to create new objects so we must be underhanded and sneaky with ctypes .<CODESPLIT>def _get_from_c_api ( ) : from ctypes import pythonapi , py_object PyDictProxy_New = pythonapi . PyDictProxy_New PyDictProxy_New . argtypes = ( py_object , ) PyDictProxy_New . restype = py_object # To actually create new dictproxy instances, we need a class that calls # the above C API functions, but a subclass would also bring with it some # indirection baggage.  Let's skip all that and have the subclass's __new__ # return an object of the real dictproxy type. # In other words, the `dictproxy` class being created here is never # instantiated and never does anything.  It's a glorified function. # (That's why it just inherits from `object`, too.) class dictproxy ( object ) : """Read-only proxy for a dict, using the same mechanism Python uses for         the __dict__ attribute on objects.          Create with dictproxy(some_dict).  The original dict is still         read/write, and changes to the original dict are reflected in real time         via the proxy, but the proxy cannot be edited in any way.         """ def __new__ ( cls , d ) : if not isinstance ( d , dict ) : # I suspect bad things would happen if this were not true. raise TypeError ( "dictproxy can only proxy to a real dict" ) return PyDictProxy_New ( d ) # Try this once to make sure it actually works.  Because PyPy has all the # parts and then crashes when trying to actually use them. # See also: https://bugs.pypy.org/issue1233 dictproxy ( dict ( ) ) # And slap on a metaclass that fools isinstance() while we're at it. return _add_isinstance_tomfoolery ( dictproxy )

1<CODESPLIT>https://github.com/minhhoit/yacms/blob/2921b706b7107c6e8c5f2bbf790ff11f85a2167f/yacms/generic/views.py#L91-L119<CODESPLIT>comment<CODESPLIT>Handle a ThreadedCommentForm submission and redirect back to its related object .<CODESPLIT>def comment ( request , template = "generic/comments.html" , extra_context = None ) : response = initial_validation ( request , "comment" ) if isinstance ( response , HttpResponse ) : return response obj , post_data = response form_class = import_dotted_path ( settings . COMMENT_FORM_CLASS ) form = form_class ( request , obj , post_data ) if form . is_valid ( ) : url = obj . get_absolute_url ( ) if is_spam ( request , form , url ) : return redirect ( url ) comment = form . save ( request ) response = redirect ( add_cache_bypass ( comment . get_absolute_url ( ) ) ) # Store commenter's details in a cookie for 90 days. for field in ThreadedCommentForm . cookie_fields : cookie_name = ThreadedCommentForm . cookie_prefix + field cookie_value = post_data . get ( field , "" ) set_cookie ( response , cookie_name , cookie_value ) return response elif request . is_ajax ( ) and form . errors : return HttpResponse ( dumps ( { "errors" : form . errors } ) ) # Show errors with stand-alone comment form. context = { "obj" : obj , "posted_comment_form" : form } context . update ( extra_context or { } ) return TemplateResponse ( request , template , context )

0<CODESPLIT>https://github.com/20tab/twentytab-tree/blob/f2c1ced33e6c211bb52a25a7d48155e39fbdc088/tree/models.py#L97-L117<CODESPLIT>Node.slugable<CODESPLIT>It returns node s slug<CODESPLIT>def slugable ( self ) : if self . page : if self . is_leaf_node ( ) : return True if not self . is_leaf_node ( ) and not self . page . regex : return True if not self . is_leaf_node ( ) and self . page . regex and not self . page . show_regex : return True if not self . is_leaf_node ( ) and self . page . regex and self . page . show_regex and self . value_regex : return True elif not self . is_leaf_node ( ) and not self . hide_in_url : return True return False

0<CODESPLIT>https://github.com/Jaymon/prom/blob/b7ad2c259eca198da03e1e4bc7d95014c168c361/prom/cli/dump.py#L44-L63<CODESPLIT>get_modules<CODESPLIT>given a module return all the parent_class subclasses that are found in that module and any submodules .<CODESPLIT>def get_modules ( modulepath ) : m = importlib . import_module ( modulepath ) mpath = m . __file__ ret = set ( [ m ] ) if "__init__." in mpath . lower ( ) : mpath = os . path . dirname ( mpath ) # https://docs.python.org/2/library/pkgutil.html#pkgutil.iter_modules for module_info in pkgutil . iter_modules ( [ mpath ] ) : submodulepath = "." . join ( [ modulepath , module_info [ 1 ] ] ) if module_info [ 2 ] : # module is a package submodules = get_modules ( submodulepath ) ret . update ( submodules ) else : ret . add ( importlib . import_module ( submodulepath ) ) return ret

0<CODESPLIT>https://github.com/political-memory/django-representatives/blob/811c90d0250149e913e6196f0ab11c97d396be39/representatives/migrations/0021_update_fr_committees.py#L25-L59<CODESPLIT>EBUtils.build_api_object<CODESPLIT>Migrate to new FR committee abbreviations<CODESPLIT>def build_api_object ( uo = None , api_key = None , uo_id = None , uo_type = None ) : if uo is not None : api_key = uo . resolve_api_key ( ) if uo . resolve_api_key ( ) is not None else api_key uo_id = uo . uo_id if uo . uo_id is not None else uo_id uo_type = uo . uo_type if uo . uo_type is not None else uo_type if uo_type is None or uo_type == EBConsts . INVALID_KEY_TYPE : uo_type = 0 return "%s%010x%010x" % ( api_key , uo_id , uo_type )

1<CODESPLIT>https://github.com/klen/graphite-beacon/blob/c1f071e9f557693bc90f6acbc314994985dc3b77/graphite_beacon/alerts.py#L318-L337<CODESPLIT>URLAlert.load<CODESPLIT>Load URL .<CODESPLIT>def load ( self ) : LOGGER . debug ( '%s: start checking: %s' , self . name , self . query ) if self . waiting : self . notify ( 'warning' , 'Process takes too much time' , target = 'waiting' , ntype = 'common' ) else : self . waiting = True try : response = yield self . client . fetch ( self . query , method = self . options . get ( 'method' , 'GET' ) , request_timeout = self . request_timeout , connect_timeout = self . connect_timeout , validate_cert = self . options . get ( 'validate_cert' , True ) ) self . check ( [ ( self . get_data ( response ) , self . query ) ] ) self . notify ( 'normal' , 'Metrics are loaded' , target = 'loading' , ntype = 'common' ) except Exception as e : self . notify ( 'critical' , str ( e ) , target = 'loading' , ntype = 'common' ) self . waiting = False

1<CODESPLIT>https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/notebook/callback.py#L130-L142<CODESPLIT>PandasLogger.append_metrics<CODESPLIT>Append new metrics to selected dataframes .<CODESPLIT>def append_metrics ( self , metrics , df_name ) : dataframe = self . _dataframes [ df_name ] _add_new_columns ( dataframe , metrics ) dataframe . loc [ len ( dataframe ) ] = metrics

0<CODESPLIT>https://github.com/SylvanasSun/FishFishJump/blob/696212d242d8d572f3f1b43925f3d8ab8acc6a2d/fish_dashboard/scrapyd/scrapyd_agent.py#L267-L279<CODESPLIT>lspci<CODESPLIT>Get urls that scrapyd logs file by project name and spider name : param project_name : the project name : param spider_name : the spider name : return : two list of the logs file name and logs file url<CODESPLIT>def lspci ( vendor = None , device = None ) : lspci_cmd = [ 'lspci' , '-xxxx' , '-v' ] lspci_results = subprocess . run ( lspci_cmd , stdout = subprocess . PIPE ) lspci_stdout = lspci_results . stdout . decode ( 'utf-8' ) config = [ parse_lspci_output ( stdout ) for stdout in lspci_stdout . split ( '\n\n' ) if stdout != '' ] if vendor is not None : config = [ c for c in config if c . vendor_id == vendor ] pass if device is not None : config = [ c for c in config if c . device_id == device ] pass return config

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/virt.py#L2997-L3021<CODESPLIT>define_xml_str<CODESPLIT>Define a persistent domain based on the XML passed to the function<CODESPLIT>def define_xml_str ( xml , * * kwargs ) : # pylint: disable=redefined-outer-name conn = __get_conn ( * * kwargs ) ret = conn . defineXML ( xml ) is not None conn . close ( ) return ret

0<CODESPLIT>https://github.com/peterbe/premailer/blob/4d74656fb12e8e44683fa787ae71c0735282376b/premailer/premailer.py#L53-L59<CODESPLIT>get_or_create_head<CODESPLIT>makes every property in a string !important .<CODESPLIT>def get_or_create_head ( root ) : head = _create_cssselector ( "head" ) ( root ) if not head : head = etree . Element ( "head" ) body = _create_cssselector ( "body" ) ( root ) [ 0 ] body . getparent ( ) . insert ( 0 , head ) return head else : return head [ 0 ]

1<CODESPLIT>https://github.com/PyCQA/astroid/blob/e0a298df55b15abcb77c2a93253f5ab7be52d0fb/astroid/rebuilder.py#L489-L497<CODESPLIT>TreeRebuilder.visit_dictcomp<CODESPLIT>visit a DictComp node by returning a fresh instance of it<CODESPLIT>def visit_dictcomp ( self , node , parent ) : newnode = nodes . DictComp ( node . lineno , node . col_offset , parent ) newnode . postinit ( self . visit ( node . key , newnode ) , self . visit ( node . value , newnode ) , [ self . visit ( child , newnode ) for child in node . generators ] , ) return newnode

1<CODESPLIT>https://github.com/tensorflow/datasets/blob/46ceb0cf7b4690f38ecbbc689e4d659a903d08dc/tensorflow_datasets/text/cnn_dailymail.py#L163-L207<CODESPLIT>_get_art_abs<CODESPLIT>Get abstract ( highlights ) and article from a story file path .<CODESPLIT>def _get_art_abs ( story_file ) : # Based on https://github.com/abisee/cnn-dailymail/blob/master/ #     make_datafiles.py lines = _read_text_file ( story_file ) # Lowercase everything lines = [ line . lower ( ) for line in lines ] # Put periods on the ends of lines that are missing them # (this is a problem in the dataset because many image captions don't end in # periods; consequently they end up in the body of the article as run-on # sentences) def fix_missing_period ( line ) : """Adds a period to a line that is missing a period.""" if '@highlight' in line : return line if not line : return line if line [ - 1 ] in END_TOKENS : return line return line + ' .' lines = [ fix_missing_period ( line ) for line in lines ] # Separate out article and abstract sentences article_lines = [ ] highlights = [ ] next_is_highlight = False for line in lines : if not line : continue # empty line elif line . startswith ( '@highlight' ) : next_is_highlight = True elif next_is_highlight : highlights . append ( line ) else : article_lines . append ( line ) # Make article into a single string article = ' ' . join ( article_lines ) # Make abstract into a single string, putting <s> and </s> tags around # the sentences. abstract = ' ' . join ( [ '%s %s %s' % ( SENTENCE_START , sent , SENTENCE_END ) for sent in highlights ] ) return article , abstract

1<CODESPLIT>https://github.com/theonion/django-bulbs/blob/0c0e6e3127a7dc487b96677fab95cacd2b3806da/bulbs/sections/models.py#L57-L75<CODESPLIT>Section._save_percolator<CODESPLIT>saves the query field as an elasticsearch percolator<CODESPLIT>def _save_percolator ( self ) : index = Content . search_objects . mapping . index query_filter = self . get_content ( ) . to_dict ( ) q = { } if "query" in query_filter : q = { "query" : query_filter . get ( "query" , { } ) } else : return es . index ( index = index , doc_type = ".percolator" , body = q , id = self . es_id )

0<CODESPLIT>https://github.com/brendonh/pyth/blob/f2a06fc8dc9b1cfc439ea14252d39b9845a7fa4b/pyth/plugins/latex/writer.py#L42-L60<CODESPLIT>LatexWriter.write<CODESPLIT>Return the style sheet that will ultimately be inserted into the latex document .<CODESPLIT>def write ( klass , document , target = None , stylesheet = "" ) : writer = LatexWriter ( document , target , stylesheet ) return writer . go ( )

0<CODESPLIT>https://github.com/pierre-rouanet/aupyom/blob/90f819b2d0b6cf7467b1945af029317a61e52e56/aupyom/sound.py#L50-L53<CODESPLIT>Sound.chunks<CODESPLIT>Loads an audiofile uses sr = 22050 by default .<CODESPLIT>def chunks ( self ) : if not hasattr ( self , '_it' ) : class ChunkIterator ( object ) : def __iter__ ( iter ) : return iter def __next__ ( iter ) : try : chunk = self . _next_chunk ( ) except StopIteration : if self . loop : self . _init_stretching ( ) return iter . __next__ ( ) raise return chunk next = __next__ self . _it = ChunkIterator ( ) return self . _it

0<CODESPLIT>https://github.com/johnnoone/json-spec/blob/f91981724cea0c366bd42a6670eb07bbe31c0e0c/src/jsonspec/validators/draft03.py#L301-L346<CODESPLIT>Draft03Validator.validate_format<CODESPLIT>Validate object against validator<CODESPLIT>def validate_format ( self , obj , pointer = None ) : if 'format' in self . attrs : substituted = { 'color' : 'css.color' , 'date-time' : 'utc.datetime' , 'date' : 'utc.date' , 'time' : 'utc.time' , 'utc-millisec' : 'utc.millisec' , 'regex' : 'regex' , 'style' : 'css.style' , 'phone' : 'phone' , 'uri' : 'uri' , 'email' : 'email' , 'ip-address' : 'ipv4' , 'ipv6' : 'ipv6' , 'host-name' : 'hostname' , } . get ( self . attrs [ 'format' ] , self . attrs [ 'format' ] ) logger . debug ( 'use %s' , substituted ) return self . formats [ substituted ] ( obj ) return obj

0<CODESPLIT>https://github.com/peterbrittain/asciimatics/blob/f471427d7786ce2d5f1eeb2dae0e67d19e46e085/asciimatics/widgets.py#L1464-L1481<CODESPLIT>Widget.register_frame<CODESPLIT>Set the size and position of the Widget .<CODESPLIT>def register_frame ( self , frame ) : self . _frame = frame self . string_len = wcswidth if self . _frame . canvas . unicode_aware else len

1<CODESPLIT>https://github.com/JukeboxPipeline/jukebox-core/blob/bac2280ca49940355270e4b69400ce9976ab2e6f/src/jukeboxcore/addons/guerilla/guerillamgmt.py#L2133-L2145<CODESPLIT>GuerillaMGMTWin.asset_create_task<CODESPLIT>Create a new task<CODESPLIT>def asset_create_task ( self , * args , * * kwargs ) : if not self . cur_asset : return task = self . create_task ( element = self . cur_asset ) if task : taskdata = djitemdata . TaskItemData ( task ) treemodel . TreeItem ( taskdata , self . asset_task_model . root )

0<CODESPLIT>https://github.com/NLeSC/scriptcwl/blob/33bb847a875379da3a5702c7a98dfa585306b960/scriptcwl/library.py#L134-L140<CODESPLIT>load_steps<CODESPLIT>Return object in yaml file .<CODESPLIT>def load_steps ( working_dir = None , steps_dir = None , step_file = None , step_list = None ) : if steps_dir is not None : step_files = glob . glob ( os . path . join ( steps_dir , '*.cwl' ) ) elif step_file is not None : step_files = [ step_file ] elif step_list is not None : step_files = [ ] for path in step_list : if os . path . isdir ( path ) : step_files += glob . glob ( os . path . join ( path , '*.cwl' ) ) else : step_files . append ( path ) else : step_files = [ ] if working_dir is not None : step_files = sort_loading_order ( step_files ) steps = { } for f in step_files : if working_dir is not None : # Copy file to working_dir if not working_dir == os . path . dirname ( f ) and not is_url ( f ) : copied_file = os . path . join ( working_dir , os . path . basename ( f ) ) shutil . copy2 ( f , copied_file ) f = copied_file # Create steps try : s = Step ( f ) steps [ s . name ] = s except ( NotImplementedError , ValidationException , PackedWorkflowException ) as e : logger . warning ( e ) return steps

0<CODESPLIT>https://github.com/wong2/pick/blob/bde1809387b17a0dd0b3250f03039e9123ecd9c7/pick/__init__.py#L69-L79<CODESPLIT>pick<CODESPLIT>return the current selected option as a tuple : ( option index ) or as a list of tuples ( in case multi_select == True )<CODESPLIT>def pick ( options , title = None , indicator = '*' , default_index = 0 , multi_select = False , min_selection_count = 0 , options_map_func = None ) : picker = Picker ( options , title , indicator , default_index , multi_select , min_selection_count , options_map_func ) return picker . start ( )

1<CODESPLIT>https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/boto_vpc.py#L1641-L1671<CODESPLIT>associate_dhcp_options_to_vpc<CODESPLIT>Given valid DHCP options id and a valid VPC id associate the DHCP options record with the VPC .<CODESPLIT>def associate_dhcp_options_to_vpc ( dhcp_options_id , vpc_id = None , vpc_name = None , region = None , key = None , keyid = None , profile = None ) : try : vpc_id = check_vpc ( vpc_id , vpc_name , region , key , keyid , profile ) if not vpc_id : return { 'associated' : False , 'error' : { 'message' : 'VPC {0} does not exist.' . format ( vpc_name or vpc_id ) } } conn = _get_conn ( region = region , key = key , keyid = keyid , profile = profile ) if conn . associate_dhcp_options ( dhcp_options_id , vpc_id ) : log . info ( 'DHCP options with id %s were associated with VPC %s' , dhcp_options_id , vpc_id ) return { 'associated' : True } else : log . warning ( 'DHCP options with id %s were not associated with VPC %s' , dhcp_options_id , vpc_id ) return { 'associated' : False , 'error' : { 'message' : 'DHCP options could not be associated.' } } except BotoServerError as e : return { 'associated' : False , 'error' : __utils__ [ 'boto.get_error' ] ( e ) }

1<CODESPLIT>https://github.com/hyperledger/indy-plenum/blob/dcd144e238af7f17a869ffc9412f13dc488b7020/plenum/server/node.py#L2569-L2610<CODESPLIT>Node.processPropagate<CODESPLIT>Process one propagateRequest sent to this node asynchronously<CODESPLIT>def processPropagate ( self , msg : Propagate , frm ) : logger . debug ( "{} received propagated request: {}" . format ( self . name , msg ) ) request = TxnUtilConfig . client_request_class ( * * msg . request ) clientName = msg . senderClient if not self . isProcessingReq ( request . key ) : ledger_id , seq_no = self . seqNoDB . get_by_payload_digest ( request . payload_digest ) if ledger_id is not None and seq_no is not None : self . _clean_req_from_verified ( request ) logger . debug ( "{} ignoring propagated request {} " "since it has been already ordered" . format ( self . name , msg ) ) return self . startedProcessingReq ( request . key , clientName ) # forced request should be processed before consensus self . handle_request_if_forced ( request ) else : if clientName is not None and not self . is_sender_known_for_req ( request . key ) : # Since some propagates might not include the client name self . set_sender_for_req ( request . key , clientName ) self . requests . add_propagate ( request , frm ) self . propagate ( request , clientName ) self . tryForwarding ( request )

1<CODESPLIT>https://github.com/oleiade/durations/blob/62c176dfa7d36d5c59bf93bdebfdc80ab53757bd/durations/parser.py#L43-L93<CODESPLIT>extract_tokens<CODESPLIT>Extracts durations tokens from a duration representation .<CODESPLIT>def extract_tokens ( representation , separators = SEPARATOR_CHARACTERS ) : buff = "" elements = [ ] last_index = 0 last_token = None for index , c in enumerate ( representation ) : # if separator character is found, push # the content of the buffer in the elements list if c in separators : if buff : # If the last found token is invalid, # raise and InvalidTokenError if not valid_token ( buff ) : raise InvalidTokenError ( "Duration representation {0} contains " "an invalid token: {1}" . format ( representation , buff ) ) # If buffer content is a separator word, for example # "and", just ignore it if not buff . strip ( ) in SEPARATOR_TOKENS : elements . append ( buff ) # Anyway, reset buffer and last token marker # to their zero value buff = "" last_token = None else : token = compute_char_token ( c ) if ( token is not None and last_token is not None and token != last_token ) : elements . append ( buff ) buff = c else : buff += c last_token = token # push the content left in representation # in the elements list elements . append ( buff ) return list ( zip ( elements [ : : 2 ] , elements [ 1 : : 2 ] ) )

0<CODESPLIT>https://github.com/mozilla-releng/scriptworker/blob/8e97bbd83b9b578565ec57904c966dd6ae4ef0ae/scriptworker/task.py#L219-L235<CODESPLIT>get_and_check_project<CODESPLIT>Get what Github pull request created the graph .<CODESPLIT>def get_and_check_project ( valid_vcs_rules , source_url ) : project_path = match_url_regex ( valid_vcs_rules , source_url , match_url_path_callback ) if project_path is None : raise ValueError ( "Unknown repo for source url {}!" . format ( source_url ) ) project = project_path . split ( '/' ) [ - 1 ] return project

0<CODESPLIT>https://github.com/ARMmbed/icetea/blob/b2b97ac607429830cf7d62dae2e3903692c7c778/icetea_lib/tools/GenericProcess.py#L503-L548<CODESPLIT>GenericProcess.start_process<CODESPLIT>Stop the process .<CODESPLIT>def start_process ( self , cmd = None , path = "" , processing_callback = None ) : self . cmd = self . cmd if not cmd else cmd self . path = self . path if not path else path if self . path : self . path = os . path . abspath ( self . path ) self . cmd_arr = [ ] # set stdbuf in/out/err to zero size = no buffers in use if self . nobuf : self . cmd_arr . extend ( [ 'stdbuf' , '-i0' , '-o0' , '-e0' ] ) # check if user want to debug this process if self . gdb : # add gdb parameters, run program immediately self . cmd_arr . extend ( [ 'gdb' , '-ex=run' , '--args' ] ) elif self . gdbs : # add gdbserver parameters, run program immediately self . cmd_arr . extend ( [ 'gdbserver' , 'localhost:' + str ( self . gdbs_port ) ] ) elif self . vgdb : # add valgrind vgdb parameters, run program but wait for remote gdb connection self . cmd_arr . extend ( [ 'valgrind' , '--vgdb=yes' , '--vgdb-error=0' ] ) if self . valgrind : self . cmd_arr . extend ( self . __get_valgrind_params ( ) ) self . cmd_arr . extend ( self . cmd ) prefn = None if not platform . system ( ) == "Windows" : prefn = os . setsid self . logger . debug ( "Instantiating process " "%s at %s with command %s" % ( self . name , self . path , " " . join ( self . cmd_arr ) ) , extra = { "type" : "   " } ) self . proc = subprocess . Popen ( self . cmd_arr , cwd = self . path , stdout = subprocess . PIPE , stdin = subprocess . PIPE , preexec_fn = prefn ) if UNIXPLATFORM : import fcntl file_descr = self . proc . stdout . fileno ( ) fcntl_var = fcntl . fcntl ( file_descr , fcntl . F_GETFL ) fcntl . fcntl ( file_descr , fcntl . F_SETFL , fcntl_var | os . O_NONBLOCK ) if self . proc . pid : # Start stream reader thread self . read_thread = NonBlockingStreamReader ( self . proc . stdout , processing_callback ) self . read_thread . start ( ) self . logger . info ( "Process '%s' running with pid: %i" % ( ' ' . join ( self . cmd_arr ) , self . proc . pid ) , extra = { 'type' : '<->' } ) else : self . logger . warning ( "Process start fails" , extra = { 'type' : '<->' } ) raise NameError ( 'Connection Fails' )

1<CODESPLIT>https://github.com/blackecho/Deep-Learning-TensorFlow/blob/ddeb1f2848da7b7bee166ad2152b4afc46bb2086/yadlt/models/boltzmann/rbm.py#L207-L218<CODESPLIT>RBM.sample_hidden_from_visible<CODESPLIT>Sample the hidden units from the visible units .<CODESPLIT>def sample_hidden_from_visible ( self , visible ) : hprobs = tf . nn . sigmoid ( tf . add ( tf . matmul ( visible , self . W ) , self . bh_ ) ) hstates = utilities . sample_prob ( hprobs , self . hrand ) return hprobs , hstates

1<CODESPLIT>https://github.com/smarie/python-parsyfiles/blob/344b37e1151e8d4e7c2ee49ae09d6568715ae64e/parsyfiles/plugins_base/support_for_objects.py#L243-L319<CODESPLIT>__is_valid_for_dict_to_object_conversion<CODESPLIT>Returns true if the provided types are valid for dict_to_object conversion<CODESPLIT>def __is_valid_for_dict_to_object_conversion ( strict_mode : bool , from_type : Type , to_type : Type ) -> bool : # right now we're stuck with the default logger.. logr = default_logger if to_type is None or is_any_type ( to_type ) : # explicitly handle the 'None' (joker) or 'any' type return True elif is_collection ( to_type , strict = True ) : # if the destination type is 'strictly a collection' (not a subclass of a collection) we know that we can't # handle it here, the constructor is not pep484-typed return False else : # (1) Try the type itself try : # can we find enough pep-484 information in the constructor to be able to understand what is required ? get_constructor_attributes_types ( to_type ) return True except TypeInformationRequiredError as main_e : # # failed: we cant guess the required types of constructor arguments # if strict_mode: #     # Warning and return NO #     if should_display_warnings_for(to_type): #         logr.warn('Object constructor signature for type {} does not allow parsyfiles to ' #                    'automatically create instances from dict content. Caught {}: {}' #                    ''.format(get_pretty_type_str(to_type), type(main_e).__name__, main_e)) #     return False # # # non-strict mode: (2) Check if any subclasses exist # subclasses = get_all_subclasses(to_type) # if len(subclasses) > GLOBAL_CONFIG.dict_to_object_subclass_limit: #     logr.warn('WARNING: Type {} has {} subclasses, only {} will be tried by parsyfiles when attempting to ' #               'create it from a subclass. You can raise this limit by setting the appropriate option with ' #               '`parsyfiles_global_config()`' #               ''.format(to_type, len(subclasses), GLOBAL_CONFIG.dict_to_object_subclass_limit)) # # # Then for each subclass also try (with a configurable limit in nb of subclasses) # for subclass in subclasses[0:GLOBAL_CONFIG.dict_to_object_subclass_limit]: #     try: #         get_constructor_attributes_types(subclass) #         # OK, but issue warning for the root type still #         if should_display_warnings_for(to_type): #             logr.warn('WARNING: Object constructor signature for type {} does not allow parsyfiles to ' #                       'automatically create instances from dict content, but it can for at least one of ' #                       'its subclasses ({}) so it might be ok for you. Caught {}: {}' #                       ''.format(get_pretty_type_str(to_type), get_pretty_type_str(subclass), #                                  type(main_e).__name__, main_e)) #         return True #     except TypeInformationRequiredError as e: #         # failed: we cant guess the required types of constructor arguments #         if should_display_warnings_for(to_type): #             logr.warn('WARNING: Object constructor signature for type {} does not allow parsyfiles to ' #                       'automatically create instances from dict content. Caught {}: {}' #                       ''.format(subclass, type(e).__name__, e)) # # # Nothing succeeded if should_display_warnings_for ( to_type ) : logr . warn ( 'WARNING: Object constructor signature for type {} does not allow parsyfiles to ' 'automatically create instances from dict content. Caught {}: {}' '' . format ( get_pretty_type_str ( to_type ) , type ( main_e ) . __name__ , main_e ) ) return False

0<CODESPLIT>https://github.com/prompt-toolkit/pymux/blob/3f66e62b9de4b2251c7f9afad6c516dc5a30ec67/pymux/main.py#L487-L498<CODESPLIT>Pymux.add_process<CODESPLIT>Create a new : class : pymux . arrangement . Window in the arrangement .<CODESPLIT>def add_process ( self , command = None , vsplit = False , start_directory = None ) : assert command is None or isinstance ( command , six . text_type ) assert start_directory is None or isinstance ( start_directory , six . text_type ) window = self . arrangement . get_active_window ( ) pane = self . _create_pane ( window , command , start_directory = start_directory ) window . add_pane ( pane , vsplit = vsplit ) pane . focus ( ) self . invalidate ( )

0<CODESPLIT>https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/bootstrap.py#L54-L59<CODESPLIT>f<CODESPLIT>Christophe s suggestion for residuals G [ i ] = Sqrt ( Sum_j ( x [ j ] - a [ i j ] ) ^2 ) - C [ i ]<CODESPLIT>def f ( x , a , c ) : v = g ( x , a , c ) return v . dot ( v )

0<CODESPLIT>https://github.com/vcs-python/libvcs/blob/f7dc055250199bac6be7439b1d2240583f0bb354/libvcs/git.py#L109-L134<CODESPLIT>GitRepo.remotes_get<CODESPLIT>Retrieve the repository clone if doesn t exist .<CODESPLIT>def remotes_get ( self ) : remotes = { } cmd = self . run ( [ 'remote' ] ) ret = filter ( None , cmd . split ( '\n' ) ) for remote_name in ret : remotes [ remote_name ] = self . remote_get ( remote_name ) return remotes

1<CODESPLIT>https://github.com/vadimk2016/v-vk-api/blob/ef5656e09944b5319a1f573cfb7b022f3d31c0cf/v_vk_api/session.py#L93-L106<CODESPLIT>APISession.login<CODESPLIT>Authorizes a user and returns a bool value of the result<CODESPLIT>def login ( self ) -> bool : response = self . get ( self . LOGIN_URL ) login_url = get_base_url ( response . text ) login_data = { 'email' : self . _login , 'pass' : self . _password } login_response = self . post ( login_url , login_data ) url_params = get_url_params ( login_response . url ) self . check_for_additional_actions ( url_params , login_response . text , login_data ) if 'remixsid' in self . cookies or 'remixsid6' in self . cookies : return True

0<CODESPLIT>https://github.com/hozn/stravalib/blob/5500ebc39e0bf4706bb1ca4c27b25e56becaaa5f/stravalib/client.py#L470-L490<CODESPLIT>Client.get_club_members<CODESPLIT>Gets the activities associated with specified club .<CODESPLIT>def get_club_members ( self , club_id , limit = None ) : result_fetcher = functools . partial ( self . protocol . get , '/clubs/{id}/members' , id = club_id ) return BatchedResultsIterator ( entity = model . Athlete , bind_client = self , result_fetcher = result_fetcher , limit = limit )

0<CODESPLIT>https://github.com/photo/openphoto-python/blob/209a1da27c8d8c88dbcf4ea6c6f57031ea1bc44b/trovebox/objects/tag.py#L10-L20<CODESPLIT>Tag.update<CODESPLIT>Endpoint : / tag / <id > / delete . json<CODESPLIT>def update ( self , * * kwds ) : result = self . _client . tag . update ( self , * * kwds ) self . _replace_fields ( result . get_fields ( ) )

1<CODESPLIT>https://github.com/cstatz/maui/blob/db99986e93699ee20c5cffdd5b4ee446f8607c5d/maui/field/view.py#L296-L302<CODESPLIT>View.sub<CODESPLIT>Function to sub 3D View with vector or 2D array ( type = numpy . ndarray or 2D Field or 2D View ) or 2D View with vector ( type = numpy . ndarray ) : param x : array ( 1D 2D ) or field ( 2D ) or View ( 2D ) : param axis : specifies axis eg . axis = ( 1 2 ) plane lies in yz - plane axis = 0 vector along x axis : return : dict with result of operation ( same form as view . d )<CODESPLIT>def sub ( self , x , axis ) : return self . __array_op ( operator . sub , x , axis )

1<CODESPLIT>https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/attr/_make.py#L70-L219<CODESPLIT>attrib<CODESPLIT>Create a new attribute on a class .<CODESPLIT>def attrib ( default = NOTHING , validator = None , repr = True , cmp = True , hash = None , init = True , convert = None , metadata = None , type = None , converter = None , factory = None , kw_only = False , ) : if hash is not None and hash is not True and hash is not False : raise TypeError ( "Invalid value for hash.  Must be True, False, or None." ) if convert is not None : if converter is not None : raise RuntimeError ( "Can't pass both `convert` and `converter`.  " "Please use `converter` only." ) warnings . warn ( "The `convert` argument is deprecated in favor of `converter`.  " "It will be removed after 2019/01." , DeprecationWarning , stacklevel = 2 , ) converter = convert if factory is not None : if default is not NOTHING : raise ValueError ( "The `default` and `factory` arguments are mutually " "exclusive." ) if not callable ( factory ) : raise ValueError ( "The `factory` argument must be a callable." ) default = Factory ( factory ) if metadata is None : metadata = { } return _CountingAttr ( default = default , validator = validator , repr = repr , cmp = cmp , hash = hash , init = init , converter = converter , metadata = metadata , type = type , kw_only = kw_only , )

1<CODESPLIT>https://github.com/trustar/trustar-python/blob/707d51adc58d68aed7de12a4ca37949cb75cf122/trustar/api_client.py#L97-L121<CODESPLIT>ApiClient._refresh_token<CODESPLIT>Retrieves the OAuth2 token generated by the user s API key and API secret . Sets the instance property token to this new token . If the current token is still live the server will simply return that .<CODESPLIT>def _refresh_token ( self ) : # use basic auth with API key and secret client_auth = requests . auth . HTTPBasicAuth ( self . api_key , self . api_secret ) # make request post_data = { "grant_type" : "client_credentials" } response = requests . post ( self . auth , auth = client_auth , data = post_data , proxies = self . proxies ) self . last_response = response # raise exception if status code indicates an error if 400 <= response . status_code < 600 : message = "{} {} Error (Trace-Id: {}): {}" . format ( response . status_code , "Client" if response . status_code < 500 else "Server" , self . _get_trace_id ( response ) , "unable to get token" ) raise HTTPError ( message , response = response ) # set token property to the received token self . token = response . json ( ) [ "access_token" ]

1<CODESPLIT>https://github.com/tBuLi/symfit/blob/759dd3d1d4270510d651f40b23dd26b1b10eee83/symfit/contrib/interactive_guess/interactive_guess.py#L241-L247<CODESPLIT>Strategy2D.plot_data<CODESPLIT>Creates and plots a scatter plot of the original data .<CODESPLIT>def plot_data ( self , proj , ax ) : x , y = proj ax . scatter ( self . ig . independent_data [ x ] , self . ig . dependent_data [ y ] , c = 'b' )

1<CODESPLIT>https://github.com/modin-project/modin/blob/5b77d242596560c646b8405340c9ce64acb183cb/modin/backends/pandas/query_compiler.py#L468-L486<CODESPLIT>PandasQueryCompiler.to_pandas<CODESPLIT>Converts Modin DataFrame to Pandas DataFrame .<CODESPLIT>def to_pandas ( self ) : df = self . data . to_pandas ( is_transposed = self . _is_transposed ) if df . empty : if len ( self . columns ) != 0 : df = pandas . DataFrame ( columns = self . columns ) . astype ( self . dtypes ) else : df = pandas . DataFrame ( columns = self . columns , index = self . index ) else : ErrorMessage . catch_bugs_and_request_email ( len ( df . index ) != len ( self . index ) or len ( df . columns ) != len ( self . columns ) ) df . index = self . index df . columns = self . columns return df

1<CODESPLIT>https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/core/grr_response_core/lib/config_lib.py#L665-L686<CODESPLIT>StringInterpolator.ExpandArg<CODESPLIT>Expand the args as a section . parameter from the config .<CODESPLIT>def ExpandArg ( self , * * _ ) : # This function is called when we see close ) and the stack depth has to # exactly match the number of (. if len ( self . stack ) <= 1 : raise lexer . ParseError ( "Unbalanced parenthesis: Can not expand '%s'" % self . processed_buffer ) # This is the full parameter name: e.g. Logging.path parameter_name = self . stack . pop ( - 1 ) if "." not in parameter_name : parameter_name = "%s.%s" % ( self . default_section , parameter_name ) final_value = self . config . Get ( parameter_name , context = self . context ) if final_value is None : final_value = "" type_info_obj = ( self . config . FindTypeInfo ( parameter_name ) or type_info . String ( ) ) # Encode the interpolated string according to its type. self . stack [ - 1 ] += type_info_obj . ToString ( final_value )

0<CODESPLIT>https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/heapq3.py#L574-L593<CODESPLIT>_siftdown_max<CODESPLIT>Maxheap variant of _siftup<CODESPLIT>def _siftdown_max ( heap , startpos , pos ) : newitem = heap [ pos ] # Follow the path to the root, moving parents down until finding a place # newitem fits. while pos > startpos : parentpos = ( pos - 1 ) >> 1 parent = heap [ parentpos ] if parent < newitem : heap [ pos ] = parent pos = parentpos continue break heap [ pos ] = newitem

1<CODESPLIT>https://github.com/Capitains/MyCapytain/blob/b11bbf6b6ae141fc02be70471e3fbf6907be6593/MyCapytain/resources/collections/dts/_resolver.py#L187-L218<CODESPLIT>HttpResolverDtsCollection.parse_member<CODESPLIT>Parse the member value of a Collection response and returns the list of object while setting the graph relationship based on direction<CODESPLIT>def parse_member ( cls , obj : dict , collection : "HttpResolverDtsCollection" , direction : str , * * additional_parameters ) : members = [ ] # Start pagination check here hydra_members = obj . get ( str ( _hyd . member ) , [ ] ) if hydra_members : for member in hydra_members : subcollection = cls . parse ( member , metadata_parsed = False , * * additional_parameters ) if direction == "children" : subcollection . parents . set ( { collection } ) members . append ( subcollection ) if "https://www.w3.org/ns/hydra/core#view" not in obj or ( direction == "children" and collection . size == 0 ) : collection . _parsed [ direction ] = True return members

1<CODESPLIT>https://github.com/rhayes777/PyAutoFit/blob/a9e6144abb08edfc6a6906c4030d7119bf8d3e14/autofit/aggregator.py#L107-L121<CODESPLIT>Aggregator.optimizers_with<CODESPLIT>Load a list of optimizers for phases in the directory with zero or more filters applied .<CODESPLIT>def optimizers_with ( self , * * kwargs ) -> [ non_linear . NonLinearOptimizer ] : return [ phase . optimizer for phase in self . phases_with ( * * kwargs ) ]

1<CODESPLIT>https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/toolbox/sql_tools.py#L72-L85<CODESPLIT>create_CTL<CODESPLIT>create_CTL ( fname_control_file tbl_name src_file cols TRUNCATE )<CODESPLIT>def create_CTL ( fname , tbl_name , col_list , TRUNC_OR_APPEND , delim = ',' ) : with open ( fname , 'w' ) as ct : ct . write ( 'LOAD DATA\n' ) ct . write ( TRUNC_OR_APPEND + '\n' ) ct . write ( 'into table ' + tbl_name + '\n' ) ct . write ( "fields terminated by '" + delim + "'\n" ) ct . write ( 'optionally Enclosed  by \'"\'\n' ) ct . write ( 'TRAILING NULLCOLS\n' ) ct . write ( '(\n' ) ct . write ( ',\n' . join ( c for c in col_list ) ) ct . write ( ')\n' )

0<CODESPLIT>https://github.com/pyecore/pyecoregen/blob/8c7a792f46d7d94e5d13e00e2967dd237351a4cf/pyecoregen/ecore.py#L87-L102<CODESPLIT>EcorePackageModuleTask.classes<CODESPLIT>Determines which classifiers have to be imported into given module .<CODESPLIT>def classes ( p : ecore . EPackage ) : classes = ( c for c in p . eClassifiers if isinstance ( c , ecore . EClass ) ) return sorted ( classes , key = lambda c : len ( set ( c . eAllSuperTypes ( ) ) ) )

0<CODESPLIT>https://github.com/theiviaxx/Frog/blob/a9475463a8eed1323fe3ef5d51f9751fb1dc9edd/frog/views/tag.py#L63-L93<CODESPLIT>post<CODESPLIT>Lists all tags<CODESPLIT>def post ( request ) : res = Result ( ) data = request . POST or json . loads ( request . body ) [ 'body' ] name = data . get ( 'name' , None ) if not name : res . isError = True res . message = "No name given" return JsonResponse ( res . asDict ( ) ) tag = Tag . objects . get_or_create ( name = name . lower ( ) ) [ 0 ] res . append ( tag . json ( ) ) return JsonResponse ( res . asDict ( ) )

1<CODESPLIT>https://github.com/Yubico/yubikey-manager/blob/3ac27bc59ae76a59db9d09a530494add2edbbabf/ykman/cli/otp.py#L182-L202<CODESPLIT>ndef<CODESPLIT>Select slot configuration to use for NDEF .<CODESPLIT>def ndef ( ctx , slot , prefix ) : dev = ctx . obj [ 'dev' ] controller = ctx . obj [ 'controller' ] if not dev . config . nfc_supported : ctx . fail ( 'NFC interface not available.' ) if not controller . slot_status [ slot - 1 ] : ctx . fail ( 'Slot {} is empty.' . format ( slot ) ) try : if prefix : controller . configure_ndef_slot ( slot , prefix ) else : controller . configure_ndef_slot ( slot ) except YkpersError as e : _failed_to_write_msg ( ctx , e )

1<CODESPLIT>https://github.com/pallets/werkzeug/blob/a220671d66755a94630a212378754bb432811158/src/werkzeug/datastructures.py#L1914-L1927<CODESPLIT>_CacheControl._get_cache_value<CODESPLIT>Used internally by the accessor properties .<CODESPLIT>def _get_cache_value ( self , key , empty , type ) : if type is bool : return key in self if key in self : value = self [ key ] if value is None : return empty elif type is not None : try : value = type ( value ) except ValueError : pass return value

1<CODESPLIT>https://github.com/wiheto/teneto/blob/80d7a83a9adc1714589b020627c45bd5b66248ab/teneto/networkmeasures/temporal_closeness_centrality.py#L9-L52<CODESPLIT>temporal_closeness_centrality<CODESPLIT>Returns temporal closeness centrality per node .<CODESPLIT>def temporal_closeness_centrality ( tnet = None , paths = None ) : if tnet is not None and paths is not None : raise ValueError ( 'Only network or path input allowed.' ) if tnet is None and paths is None : raise ValueError ( 'No input.' ) # if shortest paths are not calculated, calculate them if tnet is not None : paths = shortest_temporal_path ( tnet ) pathmat = np . zeros ( [ paths [ [ 'from' , 'to' ] ] . max ( ) . max ( ) + 1 , paths [ [ 'from' , 'to' ] ] . max ( ) . max ( ) + 1 , paths [ [ 't_start' ] ] . max ( ) . max ( ) + 1 ] ) * np . nan pathmat [ paths [ 'from' ] . values , paths [ 'to' ] . values , paths [ 't_start' ] . values ] = paths [ 'temporal-distance' ] closeness = np . nansum ( 1 / np . nanmean ( pathmat , axis = 2 ) , axis = 1 ) / ( pathmat . shape [ 1 ] - 1 ) return closeness

1<CODESPLIT>https://github.com/skulumani/kinematics/blob/e8cb45efb40539982025ed0f85d6561f9f10fef0/kinematics/attitude.py#L262-L271<CODESPLIT>vee_map<CODESPLIT>Return the vee map of a vector<CODESPLIT>def vee_map ( skew ) : vec = 1 / 2 * np . array ( [ skew [ 2 , 1 ] - skew [ 1 , 2 ] , skew [ 0 , 2 ] - skew [ 2 , 0 ] , skew [ 1 , 0 ] - skew [ 0 , 1 ] ] ) return vec

1<CODESPLIT>https://github.com/rsgalloway/grit/blob/e6434ad8a1f4ac5d0903ebad630c81f8a5164d78/grit/repo/version.py#L63-L76<CODESPLIT>ItemsMixin.items<CODESPLIT>Returns set of items .<CODESPLIT>def items ( self , path = None ) : items = list ( self . iteritems ( ) ) if path is not None : path += '$' regex = re . compile ( path ) items = [ i for i in items if regex . match ( i . path ) ] return items

0<CODESPLIT>https://github.com/ankeshanand/py-gfycat/blob/74759a88fc59db1c5b270331b24442bc5f4c38e9/gfycat/client.py#L118-L137<CODESPLIT>BaseMetadata.dict<CODESPLIT>Gets the authorization token<CODESPLIT>def dict ( self ) : metadata = { } properties = { } for name , prop in list ( self . properties . items ( ) ) : properties [ name ] = prop . dict metadata [ 'properties' ] = properties return metadata

0<CODESPLIT>https://github.com/ArchiveTeam/wpull/blob/ddf051aa3322479325ba20aa778cb2cb97606bf5/wpull/protocol/ftp/command.py#L32-L54<CODESPLIT>Resolver._get_ipv6_info<CODESPLIT>Raise FTPServerError if not expected reply code .<CODESPLIT>def _get_ipv6_info ( cls , ip_address : str ) -> tuple : results = socket . getaddrinfo ( ip_address , 0 , proto = socket . IPPROTO_TCP , flags = socket . AI_NUMERICHOST ) flow_info = results [ 0 ] [ 4 ] [ 2 ] control_id = results [ 0 ] [ 4 ] [ 3 ] return flow_info , control_id

0<CODESPLIT>https://github.com/erijo/tellcore-py/blob/7a1eb53e12ef039a2350933e502633df7560f6a8/tellcore/telldus.py#L196-L213<CODESPLIT>TelldusCore.sensors<CODESPLIT>Return all known controllers .<CODESPLIT>def sensors ( self ) : sensors = [ ] try : while True : sensor = self . lib . tdSensor ( ) sensors . append ( Sensor ( lib = self . lib , * * sensor ) ) except TelldusError as e : if e . error != const . TELLSTICK_ERROR_DEVICE_NOT_FOUND : raise return sensors

0<CODESPLIT>https://github.com/Ceasar/easywatch/blob/1dd464d2acca5932473759b187dec4eb63dab2d9/easywatch/easywatch.py#L13-L36<CODESPLIT>WarpActor.rotate<CODESPLIT>Watch a directory for events . - path should be the directory to watch - handler should a function which takes an event_type and src_path and does something interesting . event_type will be one of created deleted modified or moved . src_path will be the absolute path to the file that triggered the event .<CODESPLIT>def rotate ( self , shift ) : self . child_corners . values [ : ] = np . roll ( self . child_corners . values , shift , axis = 0 ) self . update_transform ( )

0<CODESPLIT>https://github.com/coinkite/connectrum/blob/99948f92cc5c3ecb1a8a70146294014e608e50fc/connectrum/client.py#L68-L159<CODESPLIT>IrcListener.got_who_reply<CODESPLIT>Start connection process . Destination must be specified in a ServerInfo () record ( first arg ) .<CODESPLIT>async def got_who_reply ( self , nick = None , real_name = None , * * kws ) : #logger.debug('who reply: %r' % kws) nick = nick [ 2 : ] if nick [ 0 : 2 ] == 'E_' else nick host , ports = real_name . split ( ' ' , 1 ) self . servers . remove ( nick ) logger . debug ( "Found: '%s' at %s with port list: %s" , nick , host , ports ) self . results [ host . lower ( ) ] = ServerInfo ( nick , host , ports ) if not self . servers : self . all_done . set ( )

0<CODESPLIT>https://github.com/Jaymon/endpoints/blob/2f1c4ae2c69a168e69447d3d8395ada7becaa5fb/endpoints/http.py#L495-L506<CODESPLIT>Url.controller<CODESPLIT>Just a shortcut to change the current url equivalent to Url ( self ** kwargs )<CODESPLIT>def controller ( self , * paths , * * query_kwargs ) : kwargs = self . _normalize_params ( * paths , * * query_kwargs ) if self . controller_path : if "path" in kwargs : paths = self . normalize_paths ( self . controller_path , kwargs [ "path" ] ) kwargs [ "path" ] = "/" . join ( paths ) else : kwargs [ "path" ] = self . controller_path return self . create ( self . root , * * kwargs )

0<CODESPLIT>https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/third_party/dnspython/dns/rcode.py#L93-L106<CODESPLIT>to_text<CODESPLIT>Return a ( flags ednsflags ) tuple which encodes the rcode .<CODESPLIT>def to_text ( value ) : text = _by_value . get ( value ) if text is None : text = str ( value ) return text

1<CODESPLIT>https://github.com/codeinn/vcs/blob/e6cd94188e9c36d273411bf3adc0584ac6ab92a0/vcs/utils/diffs.py#L112-L119<CODESPLIT>DiffProcessor.copy_iterator<CODESPLIT>make a fresh copy of generator we should not iterate thru an original as it s needed for repeating operations on this instance of DiffProcessor<CODESPLIT>def copy_iterator ( self ) : self . __udiff , iterator_copy = itertools . tee ( self . __udiff ) return iterator_copy

0<CODESPLIT>https://github.com/Microsoft/azure-devops-python-api/blob/4777ffda2f5052fabbaddb2abe9cb434e0cf1aa8/azure-devops/azure/devops/v5_1/task_agent/task_agent_client.py#L109-L123<CODESPLIT>TaskAgentClient.get_agent<CODESPLIT>DeleteAgent . [ Preview API ] Delete an agent . You probably don t want to call this endpoint directly . Instead [ use the agent configuration script ] ( https : // docs . microsoft . com / azure / devops / pipelines / agents / agents ) to remove an agent from your organization . : param int pool_id : The pool ID to remove the agent from : param int agent_id : The agent ID to remove<CODESPLIT>def get_agent ( self , pool_id , agent_id , include_capabilities = None , include_assigned_request = None , include_last_completed_request = None , property_filters = None ) : route_values = { } if pool_id is not None : route_values [ 'poolId' ] = self . _serialize . url ( 'pool_id' , pool_id , 'int' ) if agent_id is not None : route_values [ 'agentId' ] = self . _serialize . url ( 'agent_id' , agent_id , 'int' ) query_parameters = { } if include_capabilities is not None : query_parameters [ 'includeCapabilities' ] = self . _serialize . query ( 'include_capabilities' , include_capabilities , 'bool' ) if include_assigned_request is not None : query_parameters [ 'includeAssignedRequest' ] = self . _serialize . query ( 'include_assigned_request' , include_assigned_request , 'bool' ) if include_last_completed_request is not None : query_parameters [ 'includeLastCompletedRequest' ] = self . _serialize . query ( 'include_last_completed_request' , include_last_completed_request , 'bool' ) if property_filters is not None : property_filters = "," . join ( property_filters ) query_parameters [ 'propertyFilters' ] = self . _serialize . query ( 'property_filters' , property_filters , 'str' ) response = self . _send ( http_method = 'GET' , location_id = 'e298ef32-5878-4cab-993c-043836571f42' , version = '5.1-preview.1' , route_values = route_values , query_parameters = query_parameters ) return self . _deserialize ( 'TaskAgent' , response )

1<CODESPLIT>https://github.com/Rapptz/discord.py/blob/05d4f7f9620ef33635d6ac965b26528e09cdaf5b/discord/ext/commands/core.py#L744-L766<CODESPLIT>Command.error<CODESPLIT>A decorator that registers a coroutine as a local error handler .<CODESPLIT>def error ( self , coro ) : if not asyncio . iscoroutinefunction ( coro ) : raise TypeError ( 'The error handler must be a coroutine.' ) self . on_error = coro return coro

0<CODESPLIT>https://github.com/rdireen/spherepy/blob/241521401d4d76851d4a1a564a365cfab8e98496/spherepy/pysphi.py#L205-L225<CODESPLIT>hkm_fc<CODESPLIT>index to the first n value for a give m within the spherical coefficients vector . Used by sc_to_fc<CODESPLIT>def hkm_fc ( fdata , Nmax , m , s ) : f = fdata [ : , m ] L1 = f . size MM = int ( L1 / 2 ) Q = s . size ff = np . zeros ( Q , dtype = np . complex128 ) for n in xrange ( MM , L1 ) : ff [ n ] = f [ n - MM ] for n in xrange ( 0 , MM ) : ff [ n ] = f [ n + MM ] # For larger problems, this speeds things up pretty good.  F = np . fft . fft ( ff ) S = np . fft . fft ( s ) out = 4 * np . pi * np . fft . ifft ( F * S ) return out [ 0 : Nmax + 1 ]

0<CODESPLIT>https://github.com/KE-works/pykechain/blob/b0296cf34328fd41660bf6f0b9114fd0167c40c4/pykechain/client.py#L1131-L1167<CODESPLIT>Client.create_proxy_model<CODESPLIT>Create a new Part clone under the Parent .<CODESPLIT>def create_proxy_model ( self , model , parent , name , multiplicity = 'ZERO_MANY' , * * kwargs ) : if model . category != Category . MODEL : raise IllegalArgumentError ( "The model should be of category MODEL" ) if parent . category != Category . MODEL : raise IllegalArgumentError ( "The parent should be of category MODEL" ) data = { "name" : name , "model" : model . id , "parent" : parent . id , "multiplicity" : multiplicity } return self . _create_part ( action = 'create_proxy_model' , data = data , * * kwargs )

0<CODESPLIT>https://github.com/Azure/msrest-for-python/blob/0732bc90bdb290e5f58c675ffdd7dbfa9acefc93/msrest/serialization.py#L1496-L1539<CODESPLIT>Deserializer.deserialize_basic<CODESPLIT>Deserialize a generic object . This will be handled as a dictionary .<CODESPLIT>def deserialize_basic ( self , attr , data_type ) : # If we're here, data is supposed to be a basic type. # If it's still an XML node, take the text if isinstance ( attr , ET . Element ) : attr = attr . text if not attr : if data_type == "str" : # None or '', node <a/> is empty string. return '' else : # None or '', node <a/> with a strong type is None. # Don't try to model "empty bool" or "empty int" return None if data_type == 'bool' : if attr in [ True , False , 1 , 0 ] : return bool ( attr ) elif isinstance ( attr , basestring ) : if attr . lower ( ) in [ 'true' , '1' ] : return True elif attr . lower ( ) in [ 'false' , '0' ] : return False raise TypeError ( "Invalid boolean value: {}" . format ( attr ) ) if data_type == 'str' : return self . deserialize_unicode ( attr ) return eval ( data_type ) ( attr )

1<CODESPLIT>https://github.com/gitpython-developers/GitPython/blob/1f66e25c25cde2423917ee18c4704fff83b837d1/git/repo/base.py#L1042-L1050<CODESPLIT>Repo.has_separate_working_tree<CODESPLIT>: return : True if our git_dir is not at the root of our working_tree_dir but a . git file with a platform agnositic symbolic link . Our git_dir will be wherever the . git file points to : note : bare repositories will always return False here<CODESPLIT>def has_separate_working_tree ( self ) : if self . bare : return False return osp . isfile ( osp . join ( self . working_tree_dir , '.git' ) )

1<CODESPLIT>https://github.com/simpleai-team/simpleai/blob/2836befa7e970013f62e0ee75562652aacac6f65/samples/search/eight_puzzle.py#L81-L91<CODESPLIT>EigthPuzzleProblem.result<CODESPLIT>Return the resulting state after moving a piece to the empty space . ( the action parameter contains the piece to move )<CODESPLIT>def result ( self , state , action ) : rows = string_to_list ( state ) row_e , col_e = find_location ( rows , 'e' ) row_n , col_n = find_location ( rows , action ) rows [ row_e ] [ col_e ] , rows [ row_n ] [ col_n ] = rows [ row_n ] [ col_n ] , rows [ row_e ] [ col_e ] return list_to_string ( rows )

1<CODESPLIT>https://github.com/mbr/simplekv/blob/fc46ee0b8ca9b071d6699f3f0f18a8e599a5a2d6/simplekv/cache.py#L89-L111<CODESPLIT>CacheDecorator.open<CODESPLIT>Implementation of : meth : ~simplekv . KeyValueStore . open .<CODESPLIT>def open ( self , key ) : try : return self . cache . open ( key ) except KeyError : # cache miss, load into cache fp = self . _dstore . open ( key ) self . cache . put_file ( key , fp ) return self . cache . open ( key ) except IOError : # cache error, ignore completely and return from backend return self . _dstore . open ( key )

1<CODESPLIT>https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/core/formatting.py#L122-L136<CODESPLIT>format_timedelta<CODESPLIT>Cast given object to a Timestamp and return a nicely formatted string<CODESPLIT>def format_timedelta ( t , timedelta_format = None ) : timedelta_str = str ( pd . Timedelta ( t ) ) try : days_str , time_str = timedelta_str . split ( ' days ' ) except ValueError : # catch NaT and others that don't split nicely return timedelta_str else : if timedelta_format == 'date' : return days_str + ' days' elif timedelta_format == 'time' : return time_str else : return timedelta_str

0<CODESPLIT>https://github.com/GoogleCloudPlatform/cloud-debug-python/blob/89ce3782c98b814838a3ecb5479ed3882368cbee/src/googleclouddebugger/gcp_hub_client.py#L303-L340<CODESPLIT>GcpHubClient._TransmissionThreadProc<CODESPLIT>Single attempt to register the debuggee .<CODESPLIT>def _TransmissionThreadProc ( self ) : reconnect = True while not self . _shutdown : self . _new_updates . clear ( ) if reconnect : service = self . _BuildService ( ) reconnect = False reconnect , delay = self . _TransmitBreakpointUpdates ( service ) self . _new_updates . wait ( delay )

0<CODESPLIT>https://github.com/PMEAL/OpenPNM/blob/0547b5724ffedc0a593aae48639d36fe10e0baed/openpnm/materials/VoronoiFibers.py#L872-L953<CODESPLIT>DelaunayGeometry.plot_throat<CODESPLIT>r Plot all throats around a given pore or list of pores in 3D<CODESPLIT>def plot_throat ( self , throats , fig = None ) : throat_list = [ ] for throat in throats : if throat in range ( self . num_throats ( ) ) : throat_list . append ( throat ) else : logger . warn ( 'Throat: ' + str ( throat ) + ' not part of geometry' ) if len ( throat_list ) > 0 : verts = self [ 'throat.vertices' ] [ throat_list ] offsets = self [ 'throat.offset_vertices' ] [ throat_list ] normals = self [ 'throat.normal' ] [ throat_list ] coms = self [ 'throat.centroid' ] [ throat_list ] incentre = self [ 'throat.incenter' ] [ throat_list ] inradius = 0.5 * self [ 'throat.indiameter' ] [ throat_list ] row_col = np . ceil ( np . sqrt ( len ( throat_list ) ) ) for i in range ( len ( throat_list ) ) : if fig is None : fig = plt . figure ( ) ax = fig . add_subplot ( row_col , row_col , i + 1 ) vert_2D = self . _rotate_and_chop ( verts [ i ] , normals [ i ] , [ 0 , 0 , 1 ] ) hull = ConvexHull ( vert_2D , qhull_options = 'QJ Pp' ) for simplex in hull . simplices : plt . plot ( vert_2D [ simplex , 0 ] , vert_2D [ simplex , 1 ] , 'k-' , linewidth = 2 ) plt . scatter ( vert_2D [ : , 0 ] , vert_2D [ : , 1 ] ) offset_2D = self . _rotate_and_chop ( offsets [ i ] , normals [ i ] , [ 0 , 0 , 1 ] ) offset_hull = ConvexHull ( offset_2D , qhull_options = 'QJ Pp' ) for simplex in offset_hull . simplices : plt . plot ( offset_2D [ simplex , 0 ] , offset_2D [ simplex , 1 ] , 'g-' , linewidth = 2 ) plt . scatter ( offset_2D [ : , 0 ] , offset_2D [ : , 1 ] ) # Make sure the plot looks nice by finding the greatest # range of points and setting the plot to look square xmax = vert_2D [ : , 0 ] . max ( ) xmin = vert_2D [ : , 0 ] . min ( ) ymax = vert_2D [ : , 1 ] . max ( ) ymin = vert_2D [ : , 1 ] . min ( ) x_range = xmax - xmin y_range = ymax - ymin if ( x_range > y_range ) : my_range = x_range else : my_range = y_range lower_bound_x = xmin - my_range * 0.5 upper_bound_x = xmin + my_range * 1.5 lower_bound_y = ymin - my_range * 0.5 upper_bound_y = ymin + my_range * 1.5 plt . axis ( ( lower_bound_x , upper_bound_x , lower_bound_y , upper_bound_y ) ) plt . grid ( b = True , which = 'major' , color = 'b' , linestyle = '-' ) centroid = self . _rotate_and_chop ( coms [ i ] , normals [ i ] , [ 0 , 0 , 1 ] ) incent = self . _rotate_and_chop ( incentre [ i ] , normals [ i ] , [ 0 , 0 , 1 ] ) plt . scatter ( centroid [ 0 ] [ 0 ] , centroid [ 0 ] [ 1 ] ) plt . scatter ( incent [ 0 ] [ 0 ] , incent [ 0 ] [ 1 ] , c = 'r' ) # Plot incircle t = np . linspace ( 0 , 2 * np . pi , 200 ) u = inradius [ i ] * np . cos ( t ) + incent [ 0 ] [ 0 ] v = inradius [ i ] * np . sin ( t ) + incent [ 0 ] [ 1 ] plt . plot ( u , v , 'r-' ) ax . ticklabel_format ( style = 'sci' , scilimits = ( 0 , 0 ) ) else : logger . error ( "Please provide throat indices" ) return fig

1<CODESPLIT>https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/spanner/google/cloud/spanner_v1/database.py#L680-L741<CODESPLIT>BatchSnapshot.generate_query_batches<CODESPLIT>Start a partitioned query operation .<CODESPLIT>def generate_query_batches ( self , sql , params = None , param_types = None , partition_size_bytes = None , max_partitions = None , ) : partitions = self . _get_snapshot ( ) . partition_query ( sql = sql , params = params , param_types = param_types , partition_size_bytes = partition_size_bytes , max_partitions = max_partitions , ) query_info = { "sql" : sql } if params : query_info [ "params" ] = params query_info [ "param_types" ] = param_types for partition in partitions : yield { "partition" : partition , "query" : query_info }

0<CODESPLIT>https://github.com/JasonKessler/scattertext/blob/cacf1f687d218ee8cae3fc05cc901db824bb1b81/scattertext/TermDocMatrix.py#L512-L523<CODESPLIT>TermDocMatrix.get_scaled_f_scores<CODESPLIT>scaler = self . _get_scaler_function ( scaler_algo ) p_word_given_category = cat_word_counts . astype ( np . float64 ) / cat_word_counts . sum () p_category_given_word = cat_word_counts . astype ( np . float64 ) / ( cat_word_counts + not_cat_word_counts ) scores \ = self . _computer_harmoic_mean_of_probabilities_over_non_zero_in_category_count_terms ( cat_word_counts p_category_given_word p_word_given_category scaler )<CODESPLIT>def get_scaled_f_scores ( self , category , scaler_algo = DEFAULT_SCALER_ALGO , beta = DEFAULT_BETA ) : assert beta > 0 cat_word_counts , not_cat_word_counts = self . _get_catetgory_and_non_category_word_counts ( category ) scores = self . _get_scaled_f_score_from_counts ( cat_word_counts , not_cat_word_counts , scaler_algo , beta ) return np . array ( scores )

0<CODESPLIT>https://github.com/rosenbrockc/fortpy/blob/1ed0757c52d549e41d9d44bdea68cb89529293a5/fortpy/scripts/analyze.py#L315-L324<CODESPLIT>FortpyShell.do_filter<CODESPLIT>Sets the value of the argument with the specified id using the argument passed in from the shell session .<CODESPLIT>def do_filter ( self , arg ) : if arg == "list" : msg . info ( "TEST CASE FILTERS" ) for f in self . curargs [ "tfilter" ] : if f == "*" : msg . info ( "  * (default, matches all)" ) else : msg . info ( "  " + f ) elif arg not in self . curargs [ "tfilter" ] : self . curargs [ "tfilter" ] . append ( arg ) self . do_filter ( "list" )

0<CODESPLIT>https://github.com/erikrose/nose-progressive/blob/42853f11290cfaac8aa3d204714b71e27cc4ec07/noseprogressive/utils.py#L121-L132<CODESPLIT>OneTrackMind.know<CODESPLIT>Return the most human - readable representation of the given path .<CODESPLIT>def know ( self , what , confidence ) : if confidence > self . confidence : self . best = what self . confidence = confidence return self

1<CODESPLIT>https://github.com/nicodv/kmodes/blob/cdb19fe5448aba1bf501626694bb52e68eafab45/kmodes/kmodes.py#L21-L50<CODESPLIT>init_huang<CODESPLIT>Initialize centroids according to method by Huang [ 1997 ] .<CODESPLIT>def init_huang ( X , n_clusters , dissim , random_state ) : n_attrs = X . shape [ 1 ] centroids = np . empty ( ( n_clusters , n_attrs ) , dtype = 'object' ) # determine frequencies of attributes for iattr in range ( n_attrs ) : freq = defaultdict ( int ) for curattr in X [ : , iattr ] : freq [ curattr ] += 1 # Sample centroids using the probabilities of attributes. # (I assume that's what's meant in the Huang [1998] paper; it works, # at least) # Note: sampling using population in static list with as many choices # as frequency counts. Since the counts are small integers, # memory consumption is low. choices = [ chc for chc , wght in freq . items ( ) for _ in range ( wght ) ] # So that we are consistent between Python versions, # each with different dict ordering. choices = sorted ( choices ) centroids [ : , iattr ] = random_state . choice ( choices , n_clusters ) # The previously chosen centroids could result in empty clusters, # so set centroid to closest point in X. for ik in range ( n_clusters ) : ndx = np . argsort ( dissim ( X , centroids [ ik ] ) ) # We want the centroid to be unique, if possible. while np . all ( X [ ndx [ 0 ] ] == centroids , axis = 1 ) . any ( ) and ndx . shape [ 0 ] > 1 : ndx = np . delete ( ndx , 0 ) centroids [ ik ] = X [ ndx [ 0 ] ] return centroids

1<CODESPLIT>https://github.com/vijayvarma392/surfinBH/blob/9f2d25d00f894ee2ce9ffbb02f4e4a41fa7989eb/surfinBH/surfinBH.py#L149-L159<CODESPLIT>SurFinBH._check_unused_kwargs<CODESPLIT>Call this at the end of call module to check if all the kwargs have been used . Assumes kwargs were extracted using pop .<CODESPLIT>def _check_unused_kwargs ( self , kwargs ) : if len ( kwargs . keys ( ) ) != 0 : unused = "" for k in kwargs . keys ( ) : unused += "'%s', " % k if unused [ - 2 : ] == ", " : # get rid of trailing comma unused = unused [ : - 2 ] raise Exception ( 'Unused keys in kwargs: %s' % unused )

1<CODESPLIT>https://github.com/firecat53/urlscan/blob/2d10807d01167873733da3b478c784f8fa21bbc0/urlscan/urlchoose.py#L397-L407<CODESPLIT>URLChooser._shorten<CODESPLIT>s<CODESPLIT>def _shorten ( self ) : # Toggle shortened URL for selected item fpo = self . top . body . focus_position url_idx = len ( [ i for i in self . items [ : fpo + 1 ] if isinstance ( i , urwid . Columns ) ] ) - 1 if self . compact is False and fpo <= 1 : return url = self . urls [ url_idx ] short = not "..." in self . items [ fpo ] [ 1 ] . label self . items [ fpo ] [ 1 ] . set_label ( shorten_url ( url , self . size [ 0 ] , short ) )

0<CODESPLIT>https://github.com/biocommons/biocommons.seqrepo/blob/fb6d88682cb73ee6971cfa47d4dcd90a9c649167/biocommons/seqrepo/seqaliasdb/seqaliasdb.py#L123-L157<CODESPLIT>SeqAliasDB.find_aliases<CODESPLIT>associate a namespaced alias with a sequence<CODESPLIT>def find_aliases ( self , seq_id = None , namespace = None , alias = None , current_only = True , translate_ncbi_namespace = None ) : clauses = [ ] params = [ ] def eq_or_like ( s ) : return "like" if "%" in s else "=" if translate_ncbi_namespace is None : translate_ncbi_namespace = self . translate_ncbi_namespace if alias is not None : clauses += [ "alias {} ?" . format ( eq_or_like ( alias ) ) ] params += [ alias ] if namespace is not None : # Switch to using RefSeq for RefSeq accessions # issue #38: translate "RefSeq" to "NCBI" to enable RefSeq lookups # issue #31: later breaking change, translate database if namespace == "RefSeq" : namespace = "NCBI" clauses += [ "namespace {} ?" . format ( eq_or_like ( namespace ) ) ] params += [ namespace ] if seq_id is not None : clauses += [ "seq_id {} ?" . format ( eq_or_like ( seq_id ) ) ] params += [ seq_id ] if current_only : clauses += [ "is_current = 1" ] cols = [ "seqalias_id" , "seq_id" , "alias" , "added" , "is_current" ] if translate_ncbi_namespace : cols += [ "case namespace when 'NCBI' then 'RefSeq' else namespace end as namespace" ] else : cols += [ "namespace" ] sql = "select {cols} from seqalias" . format ( cols = ", " . join ( cols ) ) if clauses : sql += " where " + " and " . join ( "(" + c + ")" for c in clauses ) sql += " order by seq_id, namespace, alias" _logger . debug ( "Executing: " + sql ) return self . _db . execute ( sql , params )

1<CODESPLIT>https://github.com/lsbardel/python-stdnet/blob/78db5320bdedc3f28c5e4f38cda13a4469e35db7/stdnet/odm/base.py#L517-L524<CODESPLIT>Model.backend<CODESPLIT>The : class : stdnet . BackendDatServer for this instance . It can be None .<CODESPLIT>def backend ( self , client = None ) : session = self . session if session : return session . model ( self ) . backend

1<CODESPLIT>https://github.com/realestate-com-au/dashmat/blob/433886e52698f0ddb9956f087b76041966c3bcd1/dashmat/server/server.py#L202-L320<CODESPLIT>generate_dashboard_js<CODESPLIT>Given a dashboard make the bundle javascript for it<CODESPLIT>def generate_dashboard_js ( dashboard , react_server , compiled_static_folder , compiled_static_prep , modules , dynamic_dashboard_js = True ) : javascript = dashboard . make_dashboard_module ( modules ) dashboard_folder = os . path . join ( compiled_static_folder , "dashboards" ) if not os . path . exists ( dashboard_folder ) : os . makedirs ( dashboard_folder ) filename = dashboard . path . replace ( "_" , "__" ) . replace ( "/" , "_" ) final_location = "{0}.js" . format ( os . path . join ( dashboard_folder , filename ) ) if not dynamic_dashboard_js : return final_location js_location = os . path . join ( dashboard_folder , "{0}.js" . format ( filename ) ) raw_location = os . path . join ( dashboard_folder , "{0}.raw" . format ( filename ) ) if os . path . exists ( raw_location ) : with open ( raw_location ) as fle : if fle . read ( ) != javascript : with open ( raw_location , 'w' ) as write_fle : write_fle . write ( javascript ) else : with open ( raw_location , 'w' ) as write_fle : write_fle . write ( javascript ) do_change = False js_mtime = - 1 if not os . path . exists ( js_location ) else os . stat ( js_location ) . st_mtime if not os . path . exists ( js_location ) or os . stat ( raw_location ) . st_mtime > js_mtime : do_change = True folders = [ ( "dashmat.server" , os . path . join ( here , "static" , "react" ) ) ] for name , module in modules . items ( ) : react_folder = pkg_resources . resource_filename ( module . relative_to , "static/react" ) if ( module . relative_to , react_folder ) not in folders : if os . path . exists ( react_folder ) : if not do_change : for root , dirs , files in os . walk ( react_folder , followlinks = True ) : for fle in files : location = os . path . join ( root , fle ) if os . stat ( location ) . st_mtime > js_mtime : do_change = True break folders . append ( ( module . relative_to , react_folder ) ) if do_change : directory = None try : directory = tempfile . mkdtemp ( dir = compiled_static_prep ) shutil . copy ( raw_location , os . path . join ( directory , "{0}.js" . format ( filename ) ) ) for module_path , react_folder in folders : dest = os . path . join ( directory , module_path ) shutil . copytree ( react_folder , dest ) with open ( os . path . join ( directory , "webpack.config.js" ) , 'w' ) as fle : fle . write ( dedent ( """                     var webpack = require("webpack");                      module.exports = {{                       entry: [ "/modules/{0}.js" ],                       output: {{                         filename: "/compiled/dashboards/{0}.js",                         library: "Dashboard"                       }},                       module: {{                         loaders: [                           {{                             exclude: /node_modules/,                             loader: "babel",                             test: /\.jsx?$/,                             query: {{                                 presets: ["react", "es2015"],                                 plugins: ["transform-object-rest-spread"]                             }}                           }},                           {{                             test: /\.css$/,                             loader: "style!css?modules"                           }}                         ]                       }},                       plugins: [                         new webpack.NoErrorsPlugin(),                         new webpack.ProvidePlugin({{                           Promise: 'imports?this=>global!exports?global.Promise!es6-promise',                           fetch: 'imports?this=>global!exports?global.fetch!whatwg-fetch'                         }})                       ]                     }};                 """ . format ( filename ) ) ) react_server . build_webpack ( directory ) finally : if directory and os . path . exists ( directory ) : shutil . rmtree ( directory ) return final_location

